{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663a56aa-5ceb-4021-aab7-76311cb9d4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 188ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 193ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 195ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 191ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 190ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 191ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 197ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 194ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 204ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 182ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Test Accuracy: 0.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set dataset path\n",
    "data_path = r\"C:\\Users\\yogen\\Downloads\\SOCOFing\"\n",
    "\n",
    "# Image processing parameters\n",
    "img_size = (128, 128)\n",
    "\n",
    "def load_images(data_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder in os.listdir(data_path):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, file)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, img_size)  # Resize\n",
    "                    images.append(img)\n",
    "                    labels.append(folder)  # Using folder name as label\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load dataset\n",
    "images, labels = load_images(data_path)\n",
    "\n",
    "# Normalize pixel values\n",
    "images = images / 255.0\n",
    "\n",
    "# Reshape to add channel dimension\n",
    "images = images.reshape(-1, img_size[0], img_size[1], 1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_size[0], img_size[1], 1)),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(set(labels)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b2661d-5e3c-42ba-bb87-5dc9df123dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (25.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec972e9-a082-4e92-b989-0f7d82fcd4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.2.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.62.1)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.14.1-cp312-cp312-win_amd64.whl.metadata (50 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "   ---------------------------------------- 0.0/376.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/376.0 MB 8.5 MB/s eta 0:00:45\n",
      "   ---------------------------------------- 1.3/376.0 MB 3.7 MB/s eta 0:01:41\n",
      "   ---------------------------------------- 2.9/376.0 MB 4.9 MB/s eta 0:01:16\n",
      "    --------------------------------------- 5.0/376.0 MB 6.4 MB/s eta 0:00:58\n",
      "    --------------------------------------- 8.1/376.0 MB 8.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 10.7/376.0 MB 9.5 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 13.4/376.0 MB 9.4 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 14.9/376.0 MB 9.2 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 17.0/376.0 MB 9.1 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 18.1/376.0 MB 8.8 MB/s eta 0:00:41\n",
      "   -- ------------------------------------- 19.1/376.0 MB 8.6 MB/s eta 0:00:42\n",
      "   -- ------------------------------------- 20.7/376.0 MB 8.4 MB/s eta 0:00:43\n",
      "   -- ------------------------------------- 22.0/376.0 MB 8.2 MB/s eta 0:00:44\n",
      "   -- ------------------------------------- 23.1/376.0 MB 8.2 MB/s eta 0:00:44\n",
      "   -- ------------------------------------- 23.6/376.0 MB 7.9 MB/s eta 0:00:45\n",
      "   -- ------------------------------------- 25.7/376.0 MB 7.8 MB/s eta 0:00:46\n",
      "   -- ------------------------------------- 27.8/376.0 MB 7.9 MB/s eta 0:00:45\n",
      "   --- ------------------------------------ 29.4/376.0 MB 7.9 MB/s eta 0:00:44\n",
      "   --- ------------------------------------ 30.9/376.0 MB 7.9 MB/s eta 0:00:44\n",
      "   --- ------------------------------------ 32.2/376.0 MB 7.9 MB/s eta 0:00:44\n",
      "   --- ------------------------------------ 34.3/376.0 MB 7.8 MB/s eta 0:00:44\n",
      "   --- ------------------------------------ 36.4/376.0 MB 7.9 MB/s eta 0:00:43\n",
      "   ---- ----------------------------------- 38.8/376.0 MB 8.1 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 40.4/376.0 MB 8.1 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 42.5/376.0 MB 8.1 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 43.5/376.0 MB 8.0 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 44.8/376.0 MB 7.9 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 46.1/376.0 MB 7.9 MB/s eta 0:00:42\n",
      "   ----- ---------------------------------- 47.4/376.0 MB 7.8 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 48.8/376.0 MB 7.8 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 50.1/376.0 MB 7.7 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 50.9/376.0 MB 7.6 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 52.2/376.0 MB 7.5 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 54.0/376.0 MB 7.6 MB/s eta 0:00:43\n",
      "   ----- ---------------------------------- 55.6/376.0 MB 7.6 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 56.9/376.0 MB 7.5 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 58.5/376.0 MB 7.5 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 59.2/376.0 MB 7.4 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 60.3/376.0 MB 7.3 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 62.1/376.0 MB 7.4 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 63.4/376.0 MB 7.4 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 64.2/376.0 MB 7.3 MB/s eta 0:00:43\n",
      "   ------ --------------------------------- 65.3/376.0 MB 7.2 MB/s eta 0:00:43\n",
      "   ------- -------------------------------- 66.3/376.0 MB 7.2 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 67.1/376.0 MB 7.1 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 68.4/376.0 MB 7.1 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 69.7/376.0 MB 7.0 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 71.6/376.0 MB 7.1 MB/s eta 0:00:43\n",
      "   ------- -------------------------------- 73.1/376.0 MB 7.1 MB/s eta 0:00:43\n",
      "   ------- -------------------------------- 74.7/376.0 MB 7.1 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 76.5/376.0 MB 7.1 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 78.1/376.0 MB 7.1 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 79.4/376.0 MB 7.1 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 81.0/376.0 MB 7.1 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 82.3/376.0 MB 7.1 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 83.9/376.0 MB 7.1 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 85.2/376.0 MB 7.1 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 86.2/376.0 MB 7.0 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 87.3/376.0 MB 7.0 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 88.3/376.0 MB 7.0 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 89.4/376.0 MB 6.9 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 89.9/376.0 MB 6.9 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 91.5/376.0 MB 6.9 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 93.1/376.0 MB 6.9 MB/s eta 0:00:42\n",
      "   ---------- ----------------------------- 94.4/376.0 MB 6.8 MB/s eta 0:00:42\n",
      "   ---------- ----------------------------- 94.9/376.0 MB 6.8 MB/s eta 0:00:42\n",
      "   ---------- ----------------------------- 96.5/376.0 MB 6.8 MB/s eta 0:00:42\n",
      "   ---------- ----------------------------- 98.6/376.0 MB 6.8 MB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 99.9/376.0 MB 6.8 MB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 100.9/376.0 MB 6.8 MB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 101.7/376.0 MB 6.7 MB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 102.8/376.0 MB 6.7 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 103.8/376.0 MB 6.7 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 105.1/376.0 MB 6.7 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 105.9/376.0 MB 6.6 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 107.7/376.0 MB 6.7 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 108.8/376.0 MB 6.6 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 110.1/376.0 MB 6.6 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 111.9/376.0 MB 6.7 MB/s eta 0:00:40\n",
      "   ------------ --------------------------- 113.5/376.0 MB 6.7 MB/s eta 0:00:40\n",
      "   ------------ --------------------------- 114.6/376.0 MB 6.7 MB/s eta 0:00:40\n",
      "   ------------ --------------------------- 115.9/376.0 MB 6.6 MB/s eta 0:00:40\n",
      "   ------------ --------------------------- 116.9/376.0 MB 6.6 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 118.0/376.0 MB 6.6 MB/s eta 0:00:40\n",
      "   ------------ --------------------------- 119.0/376.0 MB 6.6 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 119.8/376.0 MB 6.6 MB/s eta 0:00:40\n",
      "   ------------ --------------------------- 121.6/376.0 MB 6.6 MB/s eta 0:00:39\n",
      "   ------------- -------------------------- 123.5/376.0 MB 6.6 MB/s eta 0:00:39\n",
      "   ------------- -------------------------- 125.3/376.0 MB 6.6 MB/s eta 0:00:38\n",
      "   ------------- -------------------------- 127.9/376.0 MB 6.7 MB/s eta 0:00:38\n",
      "   ------------- -------------------------- 129.5/376.0 MB 6.7 MB/s eta 0:00:37\n",
      "   ------------- -------------------------- 130.8/376.0 MB 6.7 MB/s eta 0:00:37\n",
      "   -------------- ------------------------- 132.6/376.0 MB 6.7 MB/s eta 0:00:37\n",
      "   -------------- ------------------------- 134.5/376.0 MB 6.7 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 135.8/376.0 MB 6.7 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 137.1/376.0 MB 6.7 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 139.2/376.0 MB 6.7 MB/s eta 0:00:36\n",
      "   --------------- ------------------------ 141.0/376.0 MB 6.8 MB/s eta 0:00:35\n",
      "   --------------- ------------------------ 143.1/376.0 MB 6.8 MB/s eta 0:00:35\n",
      "   --------------- ------------------------ 144.4/376.0 MB 6.8 MB/s eta 0:00:35\n",
      "   --------------- ------------------------ 146.0/376.0 MB 6.8 MB/s eta 0:00:34\n",
      "   --------------- ------------------------ 148.6/376.0 MB 6.8 MB/s eta 0:00:34\n",
      "   ---------------- ----------------------- 150.7/376.0 MB 6.9 MB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 153.6/376.0 MB 6.9 MB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 155.7/376.0 MB 7.0 MB/s eta 0:00:32\n",
      "   ---------------- ----------------------- 158.1/376.0 MB 7.0 MB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 160.2/376.0 MB 7.0 MB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 162.5/376.0 MB 7.1 MB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 164.6/376.0 MB 7.1 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 166.7/376.0 MB 7.1 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 168.0/376.0 MB 7.1 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 169.6/376.0 MB 7.1 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 172.0/376.0 MB 7.1 MB/s eta 0:00:29\n",
      "   ------------------ --------------------- 173.5/376.0 MB 7.1 MB/s eta 0:00:29\n",
      "   ------------------ --------------------- 174.9/376.0 MB 7.1 MB/s eta 0:00:29\n",
      "   ------------------ --------------------- 176.7/376.0 MB 7.1 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 178.8/376.0 MB 7.2 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 180.4/376.0 MB 7.2 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 181.9/376.0 MB 7.2 MB/s eta 0:00:28\n",
      "   ------------------- -------------------- 183.8/376.0 MB 7.2 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 185.3/376.0 MB 7.2 MB/s eta 0:00:27\n",
      "   ------------------- -------------------- 187.4/376.0 MB 7.2 MB/s eta 0:00:27\n",
      "   -------------------- ------------------- 189.5/376.0 MB 7.2 MB/s eta 0:00:26\n",
      "   -------------------- ------------------- 191.9/376.0 MB 7.2 MB/s eta 0:00:26\n",
      "   -------------------- ------------------- 194.2/376.0 MB 7.3 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 196.1/376.0 MB 7.3 MB/s eta 0:00:25\n",
      "   --------------------- ------------------ 198.7/376.0 MB 7.3 MB/s eta 0:00:25\n",
      "   --------------------- ------------------ 200.5/376.0 MB 7.3 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 202.9/376.0 MB 7.4 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 203.4/376.0 MB 7.4 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 205.0/376.0 MB 7.3 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 206.8/376.0 MB 7.3 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 209.7/376.0 MB 7.4 MB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 212.3/376.0 MB 7.4 MB/s eta 0:00:23\n",
      "   ---------------------- ----------------- 215.7/376.0 MB 7.5 MB/s eta 0:00:22\n",
      "   ----------------------- ---------------- 218.6/376.0 MB 7.5 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 221.5/376.0 MB 7.6 MB/s eta 0:00:21\n",
      "   ----------------------- ---------------- 223.9/376.0 MB 7.6 MB/s eta 0:00:21\n",
      "   ------------------------ --------------- 226.8/376.0 MB 7.6 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 229.4/376.0 MB 7.7 MB/s eta 0:00:20\n",
      "   ------------------------ --------------- 232.0/376.0 MB 7.7 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 234.1/376.0 MB 7.7 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 236.2/376.0 MB 7.8 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 238.6/376.0 MB 7.8 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 238.6/376.0 MB 7.8 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 240.4/376.0 MB 7.7 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 242.0/376.0 MB 7.6 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 243.5/376.0 MB 7.6 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 245.1/376.0 MB 7.6 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 246.7/376.0 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 248.8/376.0 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 249.8/376.0 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 250.6/376.0 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 251.7/376.0 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 252.7/376.0 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 253.8/376.0 MB 7.6 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 255.6/376.0 MB 7.6 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 257.2/376.0 MB 7.6 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 258.7/376.0 MB 7.6 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 259.5/376.0 MB 7.6 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 260.3/376.0 MB 7.5 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 261.9/376.0 MB 7.5 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 263.2/376.0 MB 7.5 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 264.5/376.0 MB 7.5 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 266.1/376.0 MB 7.4 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 267.1/376.0 MB 7.5 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 267.9/376.0 MB 7.4 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 268.7/376.0 MB 7.4 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 269.2/376.0 MB 7.4 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 269.7/376.0 MB 7.4 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 271.1/376.0 MB 7.4 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 272.1/376.0 MB 7.4 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 273.4/376.0 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 275.0/376.0 MB 7.3 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 276.0/376.0 MB 7.3 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 277.3/376.0 MB 7.3 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 278.9/376.0 MB 7.4 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 280.8/376.0 MB 7.4 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 282.1/376.0 MB 7.4 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 283.9/376.0 MB 7.4 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 285.7/376.0 MB 7.4 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 287.6/376.0 MB 7.4 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 289.1/376.0 MB 7.4 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 291.2/376.0 MB 7.5 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 293.1/376.0 MB 7.5 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 294.1/376.0 MB 7.5 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 295.4/376.0 MB 7.5 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 296.2/376.0 MB 7.5 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 297.0/376.0 MB 7.5 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 298.3/376.0 MB 7.4 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 300.4/376.0 MB 7.5 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 302.5/376.0 MB 7.5 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 303.8/376.0 MB 7.5 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 304.9/376.0 MB 7.5 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 305.4/376.0 MB 7.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 306.7/376.0 MB 7.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 308.0/376.0 MB 7.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 309.3/376.0 MB 7.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 310.4/376.0 MB 7.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 312.0/376.0 MB 7.4 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 313.8/376.0 MB 7.5 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 315.6/376.0 MB 7.5 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 317.5/376.0 MB 7.5 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 319.3/376.0 MB 7.5 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 320.9/376.0 MB 7.5 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 323.0/376.0 MB 7.6 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 325.1/376.0 MB 7.6 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 326.4/376.0 MB 7.6 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 328.2/376.0 MB 7.6 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 330.3/376.0 MB 7.7 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 332.7/376.0 MB 7.7 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 335.5/376.0 MB 7.8 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 337.4/376.0 MB 7.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 339.7/376.0 MB 7.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 341.8/376.0 MB 7.9 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 343.1/376.0 MB 7.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 345.5/376.0 MB 7.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 348.4/376.0 MB 7.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 350.5/376.0 MB 8.0 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 351.8/376.0 MB 7.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 353.9/376.0 MB 8.0 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 356.0/376.0 MB 8.0 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 357.3/376.0 MB 8.0 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 358.9/376.0 MB 8.0 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 360.7/376.0 MB 8.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 363.6/376.0 MB 8.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 365.7/376.0 MB 8.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  367.3/376.0 MB 8.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  368.6/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  370.4/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  371.7/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  374.1/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.7/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.9/376.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 376.0/376.0 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.2.0-py3-none-any.whl (276 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 8.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.0/1.3 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 5.8 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/26.4 MB 6.3 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.0/26.4 MB 6.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.6/26.4 MB 4.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 4.5/26.4 MB 5.5 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.8/26.4 MB 5.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.3/26.4 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 8.4/26.4 MB 6.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 10.5/26.4 MB 6.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 12.1/26.4 MB 6.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 13.4/26.4 MB 6.5 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.7/26.4 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.8/26.4 MB 6.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 18.1/26.4 MB 6.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 19.7/26.4 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.8/26.4 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 23.1/26.4 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.7/26.4 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.3/5.5 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.9/5.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp312-cp312-win_amd64.whl (306 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wheel, werkzeug, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, google-pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.2.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 h5py-3.13.0 keras-3.9.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.1 rich-13.9.4 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-2.5.0 werkzeug-3.1.3 wheel-0.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd2ce45-4f5f-4e69-8e95-f34ce2c5be67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 178ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 177ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 173ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 173ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 169ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 171ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 178ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 168ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 168ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 169ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Test Accuracy: 0.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set dataset path\n",
    "data_path = r\"C:\\Users\\yogen\\Downloads\\SOCOFing\"\n",
    "\n",
    "# Image processing parameters\n",
    "img_size = (128, 128)\n",
    "\n",
    "def load_images(data_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder in os.listdir(data_path):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, file)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, img_size)  # Resize\n",
    "                    images.append(img)\n",
    "                    labels.append(folder)  # Using folder name as label\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load dataset\n",
    "images, labels = load_images(data_path)\n",
    "\n",
    "# Normalize pixel values\n",
    "images = images / 255.0\n",
    "\n",
    "# Reshape to add channel dimension\n",
    "images = images.reshape(-1, img_size[0], img_size[1], 1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_size[0], img_size[1], 1)),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(set(labels)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa4375-0547-4d6c-a4f6-d727dba312c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "333c5770-79a4-4b98-bc77-6fce4d15295d",
   "metadata": {},
   "source": [
    "## TRAIL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283a742f-2d4f-4a57-af0c-6fe927ce09c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No images loaded. Check your file paths and folder structure.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images), np\u001b[38;5;241m.\u001b[39marray(labels)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Normalize pixel values\u001b[39;00m\n\u001b[0;32m     44\u001b[0m images \u001b[38;5;241m=\u001b[39m images \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m, in \u001b[0;36mload_images\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m                     labels\u001b[38;5;241m.\u001b[39mappend(folder)  \u001b[38;5;66;03m# Use folder name as label\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(images) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images loaded. Check your file paths and folder structure.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(labels))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unique labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images), np\u001b[38;5;241m.\u001b[39marray(labels)\n",
      "\u001b[1;31mValueError\u001b[0m: No images loaded. Check your file paths and folder structure."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set data path\n",
    "data_path = r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\"  # Ensure this path exists\n",
    "\n",
    "# Function to load images and labels\n",
    "def load_images(data_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Path does not exist: {data_path}\")\n",
    "\n",
    "    for folder in os.listdir(data_path):\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        if os.path.isdir(folder_path):  # Check if it's a folder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".BMP\"):  # Ensure only image files are loaded\n",
    "                    img_path = os.path.join(folder_path, filename)\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is not None:\n",
    "                        img = cv2.resize(img, (128, 128))  # Resize for consistency\n",
    "                        images.append(img)\n",
    "                        labels.append(folder)  # Use folder name as label\n",
    "\n",
    "    if len(images) == 0:\n",
    "        raise ValueError(\"No images loaded. Check your file paths and folder structure.\")\n",
    "\n",
    "    print(f\"Loaded {len(images)} images with {len(set(labels))} unique labels\")\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load dataset\n",
    "images, labels = load_images(data_path)\n",
    "\n",
    "# Normalize pixel values\n",
    "images = images / 255.0\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Debugging: Check label distribution\n",
    "print(f\"Label distribution: {np.bincount(labels)}\")\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = X_train.reshape(-1, 128, 128, 1)\n",
    "X_test = X_test.reshape(-1, 128, 128, 1)\n",
    "\n",
    "# Model Architecture\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(128, 128, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(set(labels)), activation=\"softmax\" if len(set(labels)) > 2 else \"sigmoid\")  # Auto fix activation\n",
    "])\n",
    "\n",
    "# Choose loss function based on label count\n",
    "loss_fn = \"sparse_categorical_crossentropy\" if len(set(labels)) > 2 else \"binary_crossentropy\"\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss=loss_fn,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Plot Training Progress\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ef2262-e82b-4d70-a3e3-c0fa964ccb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Path exists: C:\\Users\\yogen\\Downloads\\SOCOFing\\real\n",
      "['100__M_Left_index_finger.BMP', '100__M_Left_little_finger.BMP', '100__M_Left_middle_finger.BMP', '100__M_Left_ring_finger.BMP', '100__M_Left_thumb_finger.BMP', '100__M_Right_index_finger.BMP', '100__M_Right_little_finger.BMP', '100__M_Right_middle_finger.BMP', '100__M_Right_ring_finger.BMP', '100__M_Right_thumb_finger.BMP', '101__M_Left_index_finger.BMP', '101__M_Left_little_finger.BMP', '101__M_Left_middle_finger.BMP', '101__M_Left_ring_finger.BMP', '101__M_Left_thumb_finger.BMP', '101__M_Right_index_finger.BMP', '101__M_Right_little_finger.BMP', '101__M_Right_middle_finger.BMP', '101__M_Right_ring_finger.BMP', '101__M_Right_thumb_finger.BMP', '102__M_Left_index_finger.BMP', '102__M_Left_little_finger.BMP', '102__M_Left_middle_finger.BMP', '102__M_Left_ring_finger.BMP', '102__M_Left_thumb_finger.BMP', '102__M_Right_index_finger.BMP', '102__M_Right_little_finger.BMP', '102__M_Right_middle_finger.BMP', '102__M_Right_ring_finger.BMP', '102__M_Right_thumb_finger.BMP', '103__F_Left_index_finger.BMP', '103__F_Left_little_finger.BMP', '103__F_Left_middle_finger.BMP', '103__F_Left_ring_finger.BMP', '103__F_Left_thumb_finger.BMP', '103__F_Right_index_finger.BMP', '103__F_Right_little_finger.BMP', '103__F_Right_middle_finger.BMP', '103__F_Right_ring_finger.BMP', '103__F_Right_thumb_finger.BMP', '104__M_Left_index_finger.BMP', '104__M_Left_little_finger.BMP', '104__M_Left_middle_finger.BMP', '104__M_Left_ring_finger.BMP', '104__M_Left_thumb_finger.BMP', '104__M_Right_index_finger.BMP', '104__M_Right_little_finger.BMP', '104__M_Right_middle_finger.BMP', '104__M_Right_ring_finger.BMP', '104__M_Right_thumb_finger.BMP', '105__M_Left_index_finger.BMP', '105__M_Left_little_finger.BMP', '105__M_Left_middle_finger.BMP', '105__M_Left_ring_finger.BMP', '105__M_Left_thumb_finger.BMP', '105__M_Right_index_finger.BMP', '105__M_Right_little_finger.BMP', '105__M_Right_middle_finger.BMP', '105__M_Right_ring_finger.BMP', '105__M_Right_thumb_finger.BMP', '106__M_Left_index_finger.BMP', '106__M_Left_little_finger.BMP', '106__M_Left_middle_finger.BMP', '106__M_Left_ring_finger.BMP', '106__M_Left_thumb_finger.BMP', '106__M_Right_index_finger.BMP', '106__M_Right_little_finger.BMP', '106__M_Right_middle_finger.BMP', '106__M_Right_ring_finger.BMP', '106__M_Right_thumb_finger.BMP', '107__M_Left_index_finger.BMP', '107__M_Left_little_finger.BMP', '107__M_Left_middle_finger.BMP', '107__M_Left_ring_finger.BMP', '107__M_Left_thumb_finger.BMP', '107__M_Right_index_finger.BMP', '107__M_Right_little_finger.BMP', '107__M_Right_middle_finger.BMP', '107__M_Right_ring_finger.BMP', '107__M_Right_thumb_finger.BMP', '108__M_Left_index_finger.BMP', '108__M_Left_little_finger.BMP', '108__M_Left_middle_finger.BMP', '108__M_Left_ring_finger.BMP', '108__M_Left_thumb_finger.BMP', '108__M_Right_index_finger.BMP', '108__M_Right_little_finger.BMP', '108__M_Right_middle_finger.BMP', '108__M_Right_ring_finger.BMP', '108__M_Right_thumb_finger.BMP', '109__F_Left_index_finger.BMP', '109__F_Left_little_finger.BMP', '109__F_Left_middle_finger.BMP', '109__F_Left_ring_finger.BMP', '109__F_Left_thumb_finger.BMP', '109__F_Right_index_finger.BMP', '109__F_Right_little_finger.BMP', '109__F_Right_middle_finger.BMP', '109__F_Right_ring_finger.BMP', '109__F_Right_thumb_finger.BMP', '10__M_Left_index_finger.BMP', '10__M_Left_little_finger.BMP', '10__M_Left_middle_finger.BMP', '10__M_Left_ring_finger.BMP', '10__M_Left_thumb_finger.BMP', '10__M_Right_index_finger.BMP', '10__M_Right_little_finger.BMP', '10__M_Right_middle_finger.BMP', '10__M_Right_ring_finger.BMP', '10__M_Right_thumb_finger.BMP', '110__F_Left_index_finger.BMP', '110__F_Left_little_finger.BMP', '110__F_Left_middle_finger.BMP', '110__F_Left_ring_finger.BMP', '110__F_Left_thumb_finger.BMP', '110__F_Right_index_finger.BMP', '110__F_Right_little_finger.BMP', '110__F_Right_middle_finger.BMP', '110__F_Right_ring_finger.BMP', '110__F_Right_thumb_finger.BMP', '111__M_Left_index_finger.BMP', '111__M_Left_little_finger.BMP', '111__M_Left_middle_finger.BMP', '111__M_Left_ring_finger.BMP', '111__M_Left_thumb_finger.BMP', '111__M_Right_index_finger.BMP', '111__M_Right_little_finger.BMP', '111__M_Right_middle_finger.BMP', '111__M_Right_ring_finger.BMP', '111__M_Right_thumb_finger.BMP', '112__M_Left_index_finger.BMP', '112__M_Left_little_finger.BMP', '112__M_Left_middle_finger.BMP', '112__M_Left_ring_finger.BMP', '112__M_Left_thumb_finger.BMP', '112__M_Right_index_finger.BMP', '112__M_Right_little_finger.BMP', '112__M_Right_middle_finger.BMP', '112__M_Right_ring_finger.BMP', '112__M_Right_thumb_finger.BMP', '113__M_Left_index_finger.BMP', '113__M_Left_little_finger.BMP', '113__M_Left_middle_finger.BMP', '113__M_Left_ring_finger.BMP', '113__M_Left_thumb_finger.BMP', '113__M_Right_index_finger.BMP', '113__M_Right_little_finger.BMP', '113__M_Right_middle_finger.BMP', '113__M_Right_ring_finger.BMP', '113__M_Right_thumb_finger.BMP', '114__F_Left_index_finger.BMP', '114__F_Left_little_finger.BMP', '114__F_Left_middle_finger.BMP', '114__F_Left_ring_finger.BMP', '114__F_Left_thumb_finger.BMP', '114__F_Right_index_finger.BMP', '114__F_Right_little_finger.BMP', '114__F_Right_middle_finger.BMP', '114__F_Right_ring_finger.BMP', '114__F_Right_thumb_finger.BMP', '115__F_Left_index_finger.BMP', '115__F_Left_little_finger.BMP', '115__F_Left_middle_finger.BMP', '115__F_Left_ring_finger.BMP', '115__F_Left_thumb_finger.BMP', '115__F_Right_index_finger.BMP', '115__F_Right_little_finger.BMP', '115__F_Right_middle_finger.BMP', '115__F_Right_ring_finger.BMP', '115__F_Right_thumb_finger.BMP', '116__M_Left_index_finger.BMP', '116__M_Left_little_finger.BMP', '116__M_Left_middle_finger.BMP', '116__M_Left_ring_finger.BMP', '116__M_Left_thumb_finger.BMP', '116__M_Right_index_finger.BMP', '116__M_Right_little_finger.BMP', '116__M_Right_middle_finger.BMP', '116__M_Right_ring_finger.BMP', '116__M_Right_thumb_finger.BMP', '117__F_Left_index_finger.BMP', '117__F_Left_little_finger.BMP', '117__F_Left_middle_finger.BMP', '117__F_Left_ring_finger.BMP', '117__F_Left_thumb_finger.BMP', '117__F_Right_index_finger.BMP', '117__F_Right_little_finger.BMP', '117__F_Right_middle_finger.BMP', '117__F_Right_ring_finger.BMP', '117__F_Right_thumb_finger.BMP', '118__F_Left_index_finger.BMP', '118__F_Left_little_finger.BMP', '118__F_Left_middle_finger.BMP', '118__F_Left_ring_finger.BMP', '118__F_Left_thumb_finger.BMP', '118__F_Right_index_finger.BMP', '118__F_Right_little_finger.BMP', '118__F_Right_middle_finger.BMP', '118__F_Right_ring_finger.BMP', '118__F_Right_thumb_finger.BMP', '119__F_Left_index_finger.BMP', '119__F_Left_little_finger.BMP', '119__F_Left_middle_finger.BMP', '119__F_Left_ring_finger.BMP', '119__F_Left_thumb_finger.BMP', '119__F_Right_index_finger.BMP', '119__F_Right_little_finger.BMP', '119__F_Right_middle_finger.BMP', '119__F_Right_ring_finger.BMP', '119__F_Right_thumb_finger.BMP', '11__M_Left_index_finger.BMP', '11__M_Left_little_finger.BMP', '11__M_Left_middle_finger.BMP', '11__M_Left_ring_finger.BMP', '11__M_Left_thumb_finger.BMP', '11__M_Right_index_finger.BMP', '11__M_Right_little_finger.BMP', '11__M_Right_middle_finger.BMP', '11__M_Right_ring_finger.BMP', '11__M_Right_thumb_finger.BMP', '120__M_Left_index_finger.BMP', '120__M_Left_little_finger.BMP', '120__M_Left_middle_finger.BMP', '120__M_Left_ring_finger.BMP', '120__M_Left_thumb_finger.BMP', '120__M_Right_index_finger.BMP', '120__M_Right_little_finger.BMP', '120__M_Right_middle_finger.BMP', '120__M_Right_ring_finger.BMP', '120__M_Right_thumb_finger.BMP', '121__F_Left_index_finger.BMP', '121__F_Left_little_finger.BMP', '121__F_Left_middle_finger.BMP', '121__F_Left_ring_finger.BMP', '121__F_Left_thumb_finger.BMP', '121__F_Right_index_finger.BMP', '121__F_Right_little_finger.BMP', '121__F_Right_middle_finger.BMP', '121__F_Right_ring_finger.BMP', '121__F_Right_thumb_finger.BMP', '122__M_Left_index_finger.BMP', '122__M_Left_little_finger.BMP', '122__M_Left_middle_finger.BMP', '122__M_Left_ring_finger.BMP', '122__M_Left_thumb_finger.BMP', '122__M_Right_index_finger.BMP', '122__M_Right_little_finger.BMP', '122__M_Right_middle_finger.BMP', '122__M_Right_ring_finger.BMP', '122__M_Right_thumb_finger.BMP', '123__M_Left_index_finger.BMP', '123__M_Left_little_finger.BMP', '123__M_Left_middle_finger.BMP', '123__M_Left_ring_finger.BMP', '123__M_Left_thumb_finger.BMP', '123__M_Right_index_finger.BMP', '123__M_Right_little_finger.BMP', '123__M_Right_middle_finger.BMP', '123__M_Right_ring_finger.BMP', '123__M_Right_thumb_finger.BMP', '124__M_Left_index_finger.BMP', '124__M_Left_little_finger.BMP', '124__M_Left_middle_finger.BMP', '124__M_Left_ring_finger.BMP', '124__M_Left_thumb_finger.BMP', '124__M_Right_index_finger.BMP', '124__M_Right_little_finger.BMP', '124__M_Right_middle_finger.BMP', '124__M_Right_ring_finger.BMP', '124__M_Right_thumb_finger.BMP', '125__M_Left_index_finger.BMP', '125__M_Left_little_finger.BMP', '125__M_Left_middle_finger.BMP', '125__M_Left_ring_finger.BMP', '125__M_Left_thumb_finger.BMP', '125__M_Right_index_finger.BMP', '125__M_Right_little_finger.BMP', '125__M_Right_middle_finger.BMP', '125__M_Right_ring_finger.BMP', '125__M_Right_thumb_finger.BMP', '126__F_Left_index_finger.BMP', '126__F_Left_little_finger.BMP', '126__F_Left_middle_finger.BMP', '126__F_Left_ring_finger.BMP', '126__F_Left_thumb_finger.BMP', '126__F_Right_index_finger.BMP', '126__F_Right_little_finger.BMP', '126__F_Right_middle_finger.BMP', '126__F_Right_ring_finger.BMP', '126__F_Right_thumb_finger.BMP', '127__F_Left_index_finger.BMP', '127__F_Left_little_finger.BMP', '127__F_Left_middle_finger.BMP', '127__F_Left_ring_finger.BMP', '127__F_Left_thumb_finger.BMP', '127__F_Right_index_finger.BMP', '127__F_Right_little_finger.BMP', '127__F_Right_middle_finger.BMP', '127__F_Right_ring_finger.BMP', '127__F_Right_thumb_finger.BMP', '128__M_Left_index_finger.BMP', '128__M_Left_little_finger.BMP', '128__M_Left_middle_finger.BMP', '128__M_Left_ring_finger.BMP', '128__M_Left_thumb_finger.BMP', '128__M_Right_index_finger.BMP', '128__M_Right_little_finger.BMP', '128__M_Right_middle_finger.BMP', '128__M_Right_ring_finger.BMP', '128__M_Right_thumb_finger.BMP', '129__M_Left_index_finger.BMP', '129__M_Left_little_finger.BMP', '129__M_Left_middle_finger.BMP', '129__M_Left_ring_finger.BMP', '129__M_Left_thumb_finger.BMP', '129__M_Right_index_finger.BMP', '129__M_Right_little_finger.BMP', '129__M_Right_middle_finger.BMP', '129__M_Right_ring_finger.BMP', '129__M_Right_thumb_finger.BMP', '12__M_Left_index_finger.BMP', '12__M_Left_little_finger.BMP', '12__M_Left_middle_finger.BMP', '12__M_Left_ring_finger.BMP', '12__M_Left_thumb_finger.BMP', '12__M_Right_index_finger.BMP', '12__M_Right_little_finger.BMP', '12__M_Right_middle_finger.BMP', '12__M_Right_ring_finger.BMP', '12__M_Right_thumb_finger.BMP', '130__F_Left_index_finger.BMP', '130__F_Left_little_finger.BMP', '130__F_Left_middle_finger.BMP', '130__F_Left_ring_finger.BMP', '130__F_Left_thumb_finger.BMP', '130__F_Right_index_finger.BMP', '130__F_Right_little_finger.BMP', '130__F_Right_middle_finger.BMP', '130__F_Right_ring_finger.BMP', '130__F_Right_thumb_finger.BMP', '131__M_Left_index_finger.BMP', '131__M_Left_little_finger.BMP', '131__M_Left_middle_finger.BMP', '131__M_Left_ring_finger.BMP', '131__M_Left_thumb_finger.BMP', '131__M_Right_index_finger.BMP', '131__M_Right_little_finger.BMP', '131__M_Right_middle_finger.BMP', '131__M_Right_ring_finger.BMP', '131__M_Right_thumb_finger.BMP', '132__M_Left_index_finger.BMP', '132__M_Left_little_finger.BMP', '132__M_Left_middle_finger.BMP', '132__M_Left_ring_finger.BMP', '132__M_Left_thumb_finger.BMP', '132__M_Right_index_finger.BMP', '132__M_Right_little_finger.BMP', '132__M_Right_middle_finger.BMP', '132__M_Right_ring_finger.BMP', '132__M_Right_thumb_finger.BMP', '133__M_Left_index_finger.BMP', '133__M_Left_little_finger.BMP', '133__M_Left_middle_finger.BMP', '133__M_Left_ring_finger.BMP', '133__M_Left_thumb_finger.BMP', '133__M_Right_index_finger.BMP', '133__M_Right_little_finger.BMP', '133__M_Right_middle_finger.BMP', '133__M_Right_ring_finger.BMP', '133__M_Right_thumb_finger.BMP', '134__M_Left_index_finger.BMP', '134__M_Left_little_finger.BMP', '134__M_Left_middle_finger.BMP', '134__M_Left_ring_finger.BMP', '134__M_Left_thumb_finger.BMP', '134__M_Right_index_finger.BMP', '134__M_Right_little_finger.BMP', '134__M_Right_middle_finger.BMP', '134__M_Right_ring_finger.BMP', '134__M_Right_thumb_finger.BMP', '135__F_Left_index_finger.BMP', '135__F_Left_little_finger.BMP', '135__F_Left_middle_finger.BMP', '135__F_Left_ring_finger.BMP', '135__F_Left_thumb_finger.BMP', '135__F_Right_index_finger.BMP', '135__F_Right_little_finger.BMP', '135__F_Right_middle_finger.BMP', '135__F_Right_ring_finger.BMP', '135__F_Right_thumb_finger.BMP', '136__F_Left_index_finger.BMP', '136__F_Left_little_finger.BMP', '136__F_Left_middle_finger.BMP', '136__F_Left_ring_finger.BMP', '136__F_Left_thumb_finger.BMP', '136__F_Right_index_finger.BMP', '136__F_Right_little_finger.BMP', '136__F_Right_middle_finger.BMP', '136__F_Right_ring_finger.BMP', '136__F_Right_thumb_finger.BMP', '137__M_Left_index_finger.BMP', '137__M_Left_little_finger.BMP', '137__M_Left_middle_finger.BMP', '137__M_Left_ring_finger.BMP', '137__M_Left_thumb_finger.BMP', '137__M_Right_index_finger.BMP', '137__M_Right_little_finger.BMP', '137__M_Right_middle_finger.BMP', '137__M_Right_ring_finger.BMP', '137__M_Right_thumb_finger.BMP', '138__M_Left_index_finger.BMP', '138__M_Left_little_finger.BMP', '138__M_Left_middle_finger.BMP', '138__M_Left_ring_finger.BMP', '138__M_Left_thumb_finger.BMP', '138__M_Right_index_finger.BMP', '138__M_Right_little_finger.BMP', '138__M_Right_middle_finger.BMP', '138__M_Right_ring_finger.BMP', '138__M_Right_thumb_finger.BMP', '139__M_Left_index_finger.BMP', '139__M_Left_little_finger.BMP', '139__M_Left_middle_finger.BMP', '139__M_Left_ring_finger.BMP', '139__M_Left_thumb_finger.BMP', '139__M_Right_index_finger.BMP', '139__M_Right_little_finger.BMP', '139__M_Right_middle_finger.BMP', '139__M_Right_ring_finger.BMP', '139__M_Right_thumb_finger.BMP', '13__F_Left_index_finger.BMP', '13__F_Left_little_finger.BMP', '13__F_Left_middle_finger.BMP', '13__F_Left_ring_finger.BMP', '13__F_Left_thumb_finger.BMP', '13__F_Right_index_finger.BMP', '13__F_Right_little_finger.BMP', '13__F_Right_middle_finger.BMP', '13__F_Right_ring_finger.BMP', '13__F_Right_thumb_finger.BMP', '140__F_Left_index_finger.BMP', '140__F_Left_little_finger.BMP', '140__F_Left_middle_finger.BMP', '140__F_Left_ring_finger.BMP', '140__F_Left_thumb_finger.BMP', '140__F_Right_index_finger.BMP', '140__F_Right_little_finger.BMP', '140__F_Right_middle_finger.BMP', '140__F_Right_ring_finger.BMP', '140__F_Right_thumb_finger.BMP', '141__F_Left_index_finger.BMP', '141__F_Left_little_finger.BMP', '141__F_Left_middle_finger.BMP', '141__F_Left_ring_finger.BMP', '141__F_Left_thumb_finger.BMP', '141__F_Right_index_finger.BMP', '141__F_Right_little_finger.BMP', '141__F_Right_middle_finger.BMP', '141__F_Right_ring_finger.BMP', '141__F_Right_thumb_finger.BMP', '142__F_Left_index_finger.BMP', '142__F_Left_little_finger.BMP', '142__F_Left_middle_finger.BMP', '142__F_Left_ring_finger.BMP', '142__F_Left_thumb_finger.BMP', '142__F_Right_index_finger.BMP', '142__F_Right_little_finger.BMP', '142__F_Right_middle_finger.BMP', '142__F_Right_ring_finger.BMP', '142__F_Right_thumb_finger.BMP', '143__M_Left_index_finger.BMP', '143__M_Left_little_finger.BMP', '143__M_Left_middle_finger.BMP', '143__M_Left_ring_finger.BMP', '143__M_Left_thumb_finger.BMP', '143__M_Right_index_finger.BMP', '143__M_Right_little_finger.BMP', '143__M_Right_middle_finger.BMP', '143__M_Right_ring_finger.BMP', '143__M_Right_thumb_finger.BMP', '144__M_Left_index_finger.BMP', '144__M_Left_little_finger.BMP', '144__M_Left_middle_finger.BMP', '144__M_Left_ring_finger.BMP', '144__M_Left_thumb_finger.BMP', '144__M_Right_index_finger.BMP', '144__M_Right_little_finger.BMP', '144__M_Right_middle_finger.BMP', '144__M_Right_ring_finger.BMP', '144__M_Right_thumb_finger.BMP', '145__M_Left_index_finger.BMP', '145__M_Left_little_finger.BMP', '145__M_Left_middle_finger.BMP', '145__M_Left_ring_finger.BMP', '145__M_Left_thumb_finger.BMP', '145__M_Right_index_finger.BMP', '145__M_Right_little_finger.BMP', '145__M_Right_middle_finger.BMP', '145__M_Right_ring_finger.BMP', '145__M_Right_thumb_finger.BMP', '146__M_Left_index_finger.BMP', '146__M_Left_little_finger.BMP', '146__M_Left_middle_finger.BMP', '146__M_Left_ring_finger.BMP', '146__M_Left_thumb_finger.BMP', '146__M_Right_index_finger.BMP', '146__M_Right_little_finger.BMP', '146__M_Right_middle_finger.BMP', '146__M_Right_ring_finger.BMP', '146__M_Right_thumb_finger.BMP', '147__M_Left_index_finger.BMP', '147__M_Left_little_finger.BMP', '147__M_Left_middle_finger.BMP', '147__M_Left_ring_finger.BMP', '147__M_Left_thumb_finger.BMP', '147__M_Right_index_finger.BMP', '147__M_Right_little_finger.BMP', '147__M_Right_middle_finger.BMP', '147__M_Right_ring_finger.BMP', '147__M_Right_thumb_finger.BMP', '148__M_Left_index_finger.BMP', '148__M_Left_little_finger.BMP', '148__M_Left_middle_finger.BMP', '148__M_Left_ring_finger.BMP', '148__M_Left_thumb_finger.BMP', '148__M_Right_index_finger.BMP', '148__M_Right_little_finger.BMP', '148__M_Right_middle_finger.BMP', '148__M_Right_ring_finger.BMP', '148__M_Right_thumb_finger.BMP', '149__F_Left_index_finger.BMP', '149__F_Left_little_finger.BMP', '149__F_Left_middle_finger.BMP', '149__F_Left_ring_finger.BMP', '149__F_Left_thumb_finger.BMP', '149__F_Right_index_finger.BMP', '149__F_Right_little_finger.BMP', '149__F_Right_middle_finger.BMP', '149__F_Right_ring_finger.BMP', '149__F_Right_thumb_finger.BMP', '14__M_Left_index_finger.BMP', '14__M_Left_little_finger.BMP', '14__M_Left_middle_finger.BMP', '14__M_Left_ring_finger.BMP', '14__M_Left_thumb_finger.BMP', '14__M_Right_index_finger.BMP', '14__M_Right_little_finger.BMP', '14__M_Right_middle_finger.BMP', '14__M_Right_ring_finger.BMP', '14__M_Right_thumb_finger.BMP', '150__M_Left_index_finger.BMP', '150__M_Left_little_finger.BMP', '150__M_Left_middle_finger.BMP', '150__M_Left_ring_finger.BMP', '150__M_Left_thumb_finger.BMP', '150__M_Right_index_finger.BMP', '150__M_Right_little_finger.BMP', '150__M_Right_middle_finger.BMP', '150__M_Right_ring_finger.BMP', '150__M_Right_thumb_finger.BMP', '151__M_Left_index_finger.BMP', '151__M_Left_little_finger.BMP', '151__M_Left_middle_finger.BMP', '151__M_Left_ring_finger.BMP', '151__M_Left_thumb_finger.BMP', '151__M_Right_index_finger.BMP', '151__M_Right_little_finger.BMP', '151__M_Right_middle_finger.BMP', '151__M_Right_ring_finger.BMP', '151__M_Right_thumb_finger.BMP', '152__M_Left_index_finger.BMP', '152__M_Left_little_finger.BMP', '152__M_Left_middle_finger.BMP', '152__M_Left_ring_finger.BMP', '152__M_Left_thumb_finger.BMP', '152__M_Right_index_finger.BMP', '152__M_Right_little_finger.BMP', '152__M_Right_middle_finger.BMP', '152__M_Right_ring_finger.BMP', '152__M_Right_thumb_finger.BMP', '153__M_Left_index_finger.BMP', '153__M_Left_little_finger.BMP', '153__M_Left_middle_finger.BMP', '153__M_Left_ring_finger.BMP', '153__M_Left_thumb_finger.BMP', '153__M_Right_index_finger.BMP', '153__M_Right_little_finger.BMP', '153__M_Right_middle_finger.BMP', '153__M_Right_ring_finger.BMP', '153__M_Right_thumb_finger.BMP', '154__F_Left_index_finger.BMP', '154__F_Left_little_finger.BMP', '154__F_Left_middle_finger.BMP', '154__F_Left_ring_finger.BMP', '154__F_Left_thumb_finger.BMP', '154__F_Right_index_finger.BMP', '154__F_Right_little_finger.BMP', '154__F_Right_middle_finger.BMP', '154__F_Right_ring_finger.BMP', '154__F_Right_thumb_finger.BMP', '155__M_Left_index_finger.BMP', '155__M_Left_little_finger.BMP', '155__M_Left_middle_finger.BMP', '155__M_Left_ring_finger.BMP', '155__M_Left_thumb_finger.BMP', '155__M_Right_index_finger.BMP', '155__M_Right_little_finger.BMP', '155__M_Right_middle_finger.BMP', '155__M_Right_ring_finger.BMP', '155__M_Right_thumb_finger.BMP', '156__F_Left_index_finger.BMP', '156__F_Left_little_finger.BMP', '156__F_Left_middle_finger.BMP', '156__F_Left_ring_finger.BMP', '156__F_Left_thumb_finger.BMP', '156__F_Right_index_finger.BMP', '156__F_Right_little_finger.BMP', '156__F_Right_middle_finger.BMP', '156__F_Right_ring_finger.BMP', '156__F_Right_thumb_finger.BMP', '157__M_Left_index_finger.BMP', '157__M_Left_little_finger.BMP', '157__M_Left_middle_finger.BMP', '157__M_Left_ring_finger.BMP', '157__M_Left_thumb_finger.BMP', '157__M_Right_index_finger.BMP', '157__M_Right_little_finger.BMP', '157__M_Right_middle_finger.BMP', '157__M_Right_ring_finger.BMP', '157__M_Right_thumb_finger.BMP', '158__M_Left_index_finger.BMP', '158__M_Left_little_finger.BMP', '158__M_Left_middle_finger.BMP', '158__M_Left_ring_finger.BMP', '158__M_Left_thumb_finger.BMP', '158__M_Right_index_finger.BMP', '158__M_Right_little_finger.BMP', '158__M_Right_middle_finger.BMP', '158__M_Right_ring_finger.BMP', '158__M_Right_thumb_finger.BMP', '159__M_Left_index_finger.BMP', '159__M_Left_little_finger.BMP', '159__M_Left_middle_finger.BMP', '159__M_Left_ring_finger.BMP', '159__M_Left_thumb_finger.BMP', '159__M_Right_index_finger.BMP', '159__M_Right_little_finger.BMP', '159__M_Right_middle_finger.BMP', '159__M_Right_ring_finger.BMP', '159__M_Right_thumb_finger.BMP', '15__F_Left_index_finger.BMP', '15__F_Left_little_finger.BMP', '15__F_Left_middle_finger.BMP', '15__F_Left_ring_finger.BMP', '15__F_Left_thumb_finger.BMP', '15__F_Right_index_finger.BMP', '15__F_Right_little_finger.BMP', '15__F_Right_middle_finger.BMP', '15__F_Right_ring_finger.BMP', '15__F_Right_thumb_finger.BMP', '160__M_Left_index_finger.BMP', '160__M_Left_little_finger.BMP', '160__M_Left_middle_finger.BMP', '160__M_Left_ring_finger.BMP', '160__M_Left_thumb_finger.BMP', '160__M_Right_index_finger.BMP', '160__M_Right_little_finger.BMP', '160__M_Right_middle_finger.BMP', '160__M_Right_ring_finger.BMP', '160__M_Right_thumb_finger.BMP', '161__F_Left_index_finger.BMP', '161__F_Left_little_finger.BMP', '161__F_Left_middle_finger.BMP', '161__F_Left_ring_finger.BMP', '161__F_Left_thumb_finger.BMP', '161__F_Right_index_finger.BMP', '161__F_Right_little_finger.BMP', '161__F_Right_middle_finger.BMP', '161__F_Right_ring_finger.BMP', '161__F_Right_thumb_finger.BMP', '162__M_Left_index_finger.BMP', '162__M_Left_little_finger.BMP', '162__M_Left_middle_finger.BMP', '162__M_Left_ring_finger.BMP', '162__M_Left_thumb_finger.BMP', '162__M_Right_index_finger.BMP', '162__M_Right_little_finger.BMP', '162__M_Right_middle_finger.BMP', '162__M_Right_ring_finger.BMP', '162__M_Right_thumb_finger.BMP', '163__M_Left_index_finger.BMP', '163__M_Left_little_finger.BMP', '163__M_Left_middle_finger.BMP', '163__M_Left_ring_finger.BMP', '163__M_Left_thumb_finger.BMP', '163__M_Right_index_finger.BMP', '163__M_Right_little_finger.BMP', '163__M_Right_middle_finger.BMP', '163__M_Right_ring_finger.BMP', '163__M_Right_thumb_finger.BMP', '164__M_Left_index_finger.BMP', '164__M_Left_little_finger.BMP', '164__M_Left_middle_finger.BMP', '164__M_Left_ring_finger.BMP', '164__M_Left_thumb_finger.BMP', '164__M_Right_index_finger.BMP', '164__M_Right_little_finger.BMP', '164__M_Right_middle_finger.BMP', '164__M_Right_ring_finger.BMP', '164__M_Right_thumb_finger.BMP', '165__M_Left_index_finger.BMP', '165__M_Left_little_finger.BMP', '165__M_Left_middle_finger.BMP', '165__M_Left_ring_finger.BMP', '165__M_Left_thumb_finger.BMP', '165__M_Right_index_finger.BMP', '165__M_Right_little_finger.BMP', '165__M_Right_middle_finger.BMP', '165__M_Right_ring_finger.BMP', '165__M_Right_thumb_finger.BMP', '166__M_Left_index_finger.BMP', '166__M_Left_little_finger.BMP', '166__M_Left_middle_finger.BMP', '166__M_Left_ring_finger.BMP', '166__M_Left_thumb_finger.BMP', '166__M_Right_index_finger.BMP', '166__M_Right_little_finger.BMP', '166__M_Right_middle_finger.BMP', '166__M_Right_ring_finger.BMP', '166__M_Right_thumb_finger.BMP', '167__M_Left_index_finger.BMP', '167__M_Left_little_finger.BMP', '167__M_Left_middle_finger.BMP', '167__M_Left_ring_finger.BMP', '167__M_Left_thumb_finger.BMP', '167__M_Right_index_finger.BMP', '167__M_Right_little_finger.BMP', '167__M_Right_middle_finger.BMP', '167__M_Right_ring_finger.BMP', '167__M_Right_thumb_finger.BMP', '168__M_Left_index_finger.BMP', '168__M_Left_little_finger.BMP', '168__M_Left_middle_finger.BMP', '168__M_Left_ring_finger.BMP', '168__M_Left_thumb_finger.BMP', '168__M_Right_index_finger.BMP', '168__M_Right_little_finger.BMP', '168__M_Right_middle_finger.BMP', '168__M_Right_ring_finger.BMP', '168__M_Right_thumb_finger.BMP', '169__F_Left_index_finger.BMP', '169__F_Left_little_finger.BMP', '169__F_Left_middle_finger.BMP', '169__F_Left_ring_finger.BMP', '169__F_Left_thumb_finger.BMP', '169__F_Right_index_finger.BMP', '169__F_Right_little_finger.BMP', '169__F_Right_middle_finger.BMP', '169__F_Right_ring_finger.BMP', '169__F_Right_thumb_finger.BMP', '16__M_Left_index_finger.BMP', '16__M_Left_little_finger.BMP', '16__M_Left_middle_finger.BMP', '16__M_Left_ring_finger.BMP', '16__M_Left_thumb_finger.BMP', '16__M_Right_index_finger.BMP', '16__M_Right_little_finger.BMP', '16__M_Right_middle_finger.BMP', '16__M_Right_ring_finger.BMP', '16__M_Right_thumb_finger.BMP', '170__M_Left_index_finger.BMP', '170__M_Left_little_finger.BMP', '170__M_Left_middle_finger.BMP', '170__M_Left_ring_finger.BMP', '170__M_Left_thumb_finger.BMP', '170__M_Right_index_finger.BMP', '170__M_Right_little_finger.BMP', '170__M_Right_middle_finger.BMP', '170__M_Right_ring_finger.BMP', '170__M_Right_thumb_finger.BMP', '171__M_Left_index_finger.BMP', '171__M_Left_little_finger.BMP', '171__M_Left_middle_finger.BMP', '171__M_Left_ring_finger.BMP', '171__M_Left_thumb_finger.BMP', '171__M_Right_index_finger.BMP', '171__M_Right_little_finger.BMP', '171__M_Right_middle_finger.BMP', '171__M_Right_ring_finger.BMP', '171__M_Right_thumb_finger.BMP', '172__M_Left_index_finger.BMP', '172__M_Left_little_finger.BMP', '172__M_Left_middle_finger.BMP', '172__M_Left_ring_finger.BMP', '172__M_Left_thumb_finger.BMP', '172__M_Right_index_finger.BMP', '172__M_Right_little_finger.BMP', '172__M_Right_middle_finger.BMP', '172__M_Right_ring_finger.BMP', '172__M_Right_thumb_finger.BMP', '173__F_Left_index_finger.BMP', '173__F_Left_little_finger.BMP', '173__F_Left_middle_finger.BMP', '173__F_Left_ring_finger.BMP', '173__F_Left_thumb_finger.BMP', '173__F_Right_index_finger.BMP', '173__F_Right_little_finger.BMP', '173__F_Right_middle_finger.BMP', '173__F_Right_ring_finger.BMP', '173__F_Right_thumb_finger.BMP', '174__F_Left_index_finger.BMP', '174__F_Left_little_finger.BMP', '174__F_Left_middle_finger.BMP', '174__F_Left_ring_finger.BMP', '174__F_Left_thumb_finger.BMP', '174__F_Right_index_finger.BMP', '174__F_Right_little_finger.BMP', '174__F_Right_middle_finger.BMP', '174__F_Right_ring_finger.BMP', '174__F_Right_thumb_finger.BMP', '175__M_Left_index_finger.BMP', '175__M_Left_little_finger.BMP', '175__M_Left_middle_finger.BMP', '175__M_Left_ring_finger.BMP', '175__M_Left_thumb_finger.BMP', '175__M_Right_index_finger.BMP', '175__M_Right_little_finger.BMP', '175__M_Right_middle_finger.BMP', '175__M_Right_ring_finger.BMP', '175__M_Right_thumb_finger.BMP', '176__M_Left_index_finger.BMP', '176__M_Left_little_finger.BMP', '176__M_Left_middle_finger.BMP', '176__M_Left_ring_finger.BMP', '176__M_Left_thumb_finger.BMP', '176__M_Right_index_finger.BMP', '176__M_Right_little_finger.BMP', '176__M_Right_middle_finger.BMP', '176__M_Right_ring_finger.BMP', '176__M_Right_thumb_finger.BMP', '177__F_Left_index_finger.BMP', '177__F_Left_little_finger.BMP', '177__F_Left_middle_finger.BMP', '177__F_Left_ring_finger.BMP', '177__F_Left_thumb_finger.BMP', '177__F_Right_index_finger.BMP', '177__F_Right_little_finger.BMP', '177__F_Right_middle_finger.BMP', '177__F_Right_ring_finger.BMP', '177__F_Right_thumb_finger.BMP', '178__M_Left_index_finger.BMP', '178__M_Left_little_finger.BMP', '178__M_Left_middle_finger.BMP', '178__M_Left_ring_finger.BMP', '178__M_Left_thumb_finger.BMP', '178__M_Right_index_finger.BMP', '178__M_Right_little_finger.BMP', '178__M_Right_middle_finger.BMP', '178__M_Right_ring_finger.BMP', '178__M_Right_thumb_finger.BMP', '179__M_Left_index_finger.BMP', '179__M_Left_little_finger.BMP', '179__M_Left_middle_finger.BMP', '179__M_Left_ring_finger.BMP', '179__M_Left_thumb_finger.BMP', '179__M_Right_index_finger.BMP', '179__M_Right_little_finger.BMP', '179__M_Right_middle_finger.BMP', '179__M_Right_ring_finger.BMP', '179__M_Right_thumb_finger.BMP', '17__M_Left_index_finger.BMP', '17__M_Left_little_finger.BMP', '17__M_Left_middle_finger.BMP', '17__M_Left_ring_finger.BMP', '17__M_Left_thumb_finger.BMP', '17__M_Right_index_finger.BMP', '17__M_Right_little_finger.BMP', '17__M_Right_middle_finger.BMP', '17__M_Right_ring_finger.BMP', '17__M_Right_thumb_finger.BMP', '180__F_Left_index_finger.BMP', '180__F_Left_little_finger.BMP', '180__F_Left_middle_finger.BMP', '180__F_Left_ring_finger.BMP', '180__F_Left_thumb_finger.BMP', '180__F_Right_index_finger.BMP', '180__F_Right_little_finger.BMP', '180__F_Right_middle_finger.BMP', '180__F_Right_ring_finger.BMP', '180__F_Right_thumb_finger.BMP', '181__M_Left_index_finger.BMP', '181__M_Left_little_finger.BMP', '181__M_Left_middle_finger.BMP', '181__M_Left_ring_finger.BMP', '181__M_Left_thumb_finger.BMP', '181__M_Right_index_finger.BMP', '181__M_Right_little_finger.BMP', '181__M_Right_middle_finger.BMP', '181__M_Right_ring_finger.BMP', '181__M_Right_thumb_finger.BMP', '182__M_Left_index_finger.BMP', '182__M_Left_little_finger.BMP', '182__M_Left_middle_finger.BMP', '182__M_Left_ring_finger.BMP', '182__M_Left_thumb_finger.BMP', '182__M_Right_index_finger.BMP', '182__M_Right_little_finger.BMP', '182__M_Right_middle_finger.BMP', '182__M_Right_ring_finger.BMP', '182__M_Right_thumb_finger.BMP', '183__M_Left_index_finger.BMP', '183__M_Left_little_finger.BMP', '183__M_Left_middle_finger.BMP', '183__M_Left_ring_finger.BMP', '183__M_Left_thumb_finger.BMP', '183__M_Right_index_finger.BMP', '183__M_Right_little_finger.BMP', '183__M_Right_middle_finger.BMP', '183__M_Right_ring_finger.BMP', '183__M_Right_thumb_finger.BMP', '184__M_Left_index_finger.BMP', '184__M_Left_little_finger.BMP', '184__M_Left_middle_finger.BMP', '184__M_Left_ring_finger.BMP', '184__M_Left_thumb_finger.BMP', '184__M_Right_index_finger.BMP', '184__M_Right_little_finger.BMP', '184__M_Right_middle_finger.BMP', '184__M_Right_ring_finger.BMP', '184__M_Right_thumb_finger.BMP', '185__M_Left_index_finger.BMP', '185__M_Left_little_finger.BMP', '185__M_Left_middle_finger.BMP', '185__M_Left_ring_finger.BMP', '185__M_Left_thumb_finger.BMP', '185__M_Right_index_finger.BMP', '185__M_Right_little_finger.BMP', '185__M_Right_middle_finger.BMP', '185__M_Right_ring_finger.BMP', '185__M_Right_thumb_finger.BMP', '186__M_Left_index_finger.BMP', '186__M_Left_little_finger.BMP', '186__M_Left_middle_finger.BMP', '186__M_Left_ring_finger.BMP', '186__M_Left_thumb_finger.BMP', '186__M_Right_index_finger.BMP', '186__M_Right_little_finger.BMP', '186__M_Right_middle_finger.BMP', '186__M_Right_ring_finger.BMP', '186__M_Right_thumb_finger.BMP', '187__M_Left_index_finger.BMP', '187__M_Left_little_finger.BMP', '187__M_Left_middle_finger.BMP', '187__M_Left_ring_finger.BMP', '187__M_Left_thumb_finger.BMP', '187__M_Right_index_finger.BMP', '187__M_Right_little_finger.BMP', '187__M_Right_middle_finger.BMP', '187__M_Right_ring_finger.BMP', '187__M_Right_thumb_finger.BMP', '188__M_Left_index_finger.BMP', '188__M_Left_little_finger.BMP', '188__M_Left_middle_finger.BMP', '188__M_Left_ring_finger.BMP', '188__M_Left_thumb_finger.BMP', '188__M_Right_index_finger.BMP', '188__M_Right_little_finger.BMP', '188__M_Right_middle_finger.BMP', '188__M_Right_ring_finger.BMP', '188__M_Right_thumb_finger.BMP', '189__F_Left_index_finger.BMP', '189__F_Left_little_finger.BMP', '189__F_Left_middle_finger.BMP', '189__F_Left_ring_finger.BMP', '189__F_Left_thumb_finger.BMP', '189__F_Right_index_finger.BMP', '189__F_Right_little_finger.BMP', '189__F_Right_middle_finger.BMP', '189__F_Right_ring_finger.BMP', '189__F_Right_thumb_finger.BMP', '18__M_Left_index_finger.BMP', '18__M_Left_little_finger.BMP', '18__M_Left_middle_finger.BMP', '18__M_Left_ring_finger.BMP', '18__M_Left_thumb_finger.BMP', '18__M_Right_index_finger.BMP', '18__M_Right_little_finger.BMP', '18__M_Right_middle_finger.BMP', '18__M_Right_ring_finger.BMP', '18__M_Right_thumb_finger.BMP', '190__M_Left_index_finger.BMP', '190__M_Left_little_finger.BMP', '190__M_Left_middle_finger.BMP', '190__M_Left_ring_finger.BMP', '190__M_Left_thumb_finger.BMP', '190__M_Right_index_finger.BMP', '190__M_Right_little_finger.BMP', '190__M_Right_middle_finger.BMP', '190__M_Right_ring_finger.BMP', '190__M_Right_thumb_finger.BMP', '191__F_Left_index_finger.BMP', '191__F_Left_little_finger.BMP', '191__F_Left_middle_finger.BMP', '191__F_Left_ring_finger.BMP', '191__F_Left_thumb_finger.BMP', '191__F_Right_index_finger.BMP', '191__F_Right_little_finger.BMP', '191__F_Right_middle_finger.BMP', '191__F_Right_ring_finger.BMP', '191__F_Right_thumb_finger.BMP', '192__M_Left_index_finger.BMP', '192__M_Left_little_finger.BMP', '192__M_Left_middle_finger.BMP', '192__M_Left_ring_finger.BMP', '192__M_Left_thumb_finger.BMP', '192__M_Right_index_finger.BMP', '192__M_Right_little_finger.BMP', '192__M_Right_middle_finger.BMP', '192__M_Right_ring_finger.BMP', '192__M_Right_thumb_finger.BMP', '193__M_Left_index_finger.BMP', '193__M_Left_little_finger.BMP', '193__M_Left_middle_finger.BMP', '193__M_Left_ring_finger.BMP', '193__M_Left_thumb_finger.BMP', '193__M_Right_index_finger.BMP', '193__M_Right_little_finger.BMP', '193__M_Right_middle_finger.BMP', '193__M_Right_ring_finger.BMP', '193__M_Right_thumb_finger.BMP', '194__M_Left_index_finger.BMP', '194__M_Left_little_finger.BMP', '194__M_Left_middle_finger.BMP', '194__M_Left_ring_finger.BMP', '194__M_Left_thumb_finger.BMP', '194__M_Right_index_finger.BMP', '194__M_Right_little_finger.BMP', '194__M_Right_middle_finger.BMP', '194__M_Right_ring_finger.BMP', '194__M_Right_thumb_finger.BMP', '195__M_Left_index_finger.BMP', '195__M_Left_little_finger.BMP', '195__M_Left_middle_finger.BMP', '195__M_Left_ring_finger.BMP', '195__M_Left_thumb_finger.BMP', '195__M_Right_index_finger.BMP', '195__M_Right_little_finger.BMP', '195__M_Right_middle_finger.BMP', '195__M_Right_ring_finger.BMP', '195__M_Right_thumb_finger.BMP', '196__M_Left_index_finger.BMP', '196__M_Left_little_finger.BMP', '196__M_Left_middle_finger.BMP', '196__M_Left_ring_finger.BMP', '196__M_Left_thumb_finger.BMP', '196__M_Right_index_finger.BMP', '196__M_Right_little_finger.BMP', '196__M_Right_middle_finger.BMP', '196__M_Right_ring_finger.BMP', '196__M_Right_thumb_finger.BMP', '197__M_Left_index_finger.BMP', '197__M_Left_little_finger.BMP', '197__M_Left_middle_finger.BMP', '197__M_Left_ring_finger.BMP', '197__M_Left_thumb_finger.BMP', '197__M_Right_index_finger.BMP', '197__M_Right_little_finger.BMP', '197__M_Right_middle_finger.BMP', '197__M_Right_ring_finger.BMP', '197__M_Right_thumb_finger.BMP', '198__M_Left_index_finger.BMP', '198__M_Left_little_finger.BMP', '198__M_Left_middle_finger.BMP', '198__M_Left_ring_finger.BMP', '198__M_Left_thumb_finger.BMP', '198__M_Right_index_finger.BMP', '198__M_Right_little_finger.BMP', '198__M_Right_middle_finger.BMP', '198__M_Right_ring_finger.BMP', '198__M_Right_thumb_finger.BMP', '199__M_Left_index_finger.BMP', '199__M_Left_little_finger.BMP', '199__M_Left_middle_finger.BMP', '199__M_Left_ring_finger.BMP', '199__M_Left_thumb_finger.BMP', '199__M_Right_index_finger.BMP', '199__M_Right_little_finger.BMP', '199__M_Right_middle_finger.BMP', '199__M_Right_ring_finger.BMP', '199__M_Right_thumb_finger.BMP', '19__M_Left_index_finger.BMP', '19__M_Left_little_finger.BMP', '19__M_Left_middle_finger.BMP', '19__M_Left_ring_finger.BMP', '19__M_Left_thumb_finger.BMP', '19__M_Right_index_finger.BMP', '19__M_Right_little_finger.BMP', '19__M_Right_middle_finger.BMP', '19__M_Right_ring_finger.BMP', '19__M_Right_thumb_finger.BMP', '1__M_Left_index_finger.BMP', '1__M_Left_little_finger.BMP', '1__M_Left_middle_finger.BMP', '1__M_Left_ring_finger.BMP', '1__M_Left_thumb_finger.BMP', '1__M_Right_index_finger.BMP', '1__M_Right_little_finger.BMP', '1__M_Right_middle_finger.BMP', '1__M_Right_ring_finger.BMP', '1__M_Right_thumb_finger.BMP', '200__M_Left_index_finger.BMP', '200__M_Left_little_finger.BMP', '200__M_Left_middle_finger.BMP', '200__M_Left_ring_finger.BMP', '200__M_Left_thumb_finger.BMP', '200__M_Right_index_finger.BMP', '200__M_Right_little_finger.BMP', '200__M_Right_middle_finger.BMP', '200__M_Right_ring_finger.BMP', '200__M_Right_thumb_finger.BMP', '201__F_Left_index_finger.BMP', '201__F_Left_little_finger.BMP', '201__F_Left_middle_finger.BMP', '201__F_Left_ring_finger.BMP', '201__F_Left_thumb_finger.BMP', '201__F_Right_index_finger.BMP', '201__F_Right_little_finger.BMP', '201__F_Right_middle_finger.BMP', '201__F_Right_ring_finger.BMP', '201__F_Right_thumb_finger.BMP', '202__M_Left_index_finger.BMP', '202__M_Left_little_finger.BMP', '202__M_Left_middle_finger.BMP', '202__M_Left_ring_finger.BMP', '202__M_Left_thumb_finger.BMP', '202__M_Right_index_finger.BMP', '202__M_Right_little_finger.BMP', '202__M_Right_middle_finger.BMP', '202__M_Right_ring_finger.BMP', '202__M_Right_thumb_finger.BMP', '203__M_Left_index_finger.BMP', '203__M_Left_little_finger.BMP', '203__M_Left_middle_finger.BMP', '203__M_Left_ring_finger.BMP', '203__M_Left_thumb_finger.BMP', '203__M_Right_index_finger.BMP', '203__M_Right_little_finger.BMP', '203__M_Right_middle_finger.BMP', '203__M_Right_ring_finger.BMP', '203__M_Right_thumb_finger.BMP', '204__F_Left_index_finger.BMP', '204__F_Left_little_finger.BMP', '204__F_Left_middle_finger.BMP', '204__F_Left_ring_finger.BMP', '204__F_Left_thumb_finger.BMP', '204__F_Right_index_finger.BMP', '204__F_Right_little_finger.BMP', '204__F_Right_middle_finger.BMP', '204__F_Right_ring_finger.BMP', '204__F_Right_thumb_finger.BMP', '205__F_Left_index_finger.BMP', '205__F_Left_little_finger.BMP', '205__F_Left_middle_finger.BMP', '205__F_Left_ring_finger.BMP', '205__F_Left_thumb_finger.BMP', '205__F_Right_index_finger.BMP', '205__F_Right_little_finger.BMP', '205__F_Right_middle_finger.BMP', '205__F_Right_ring_finger.BMP', '205__F_Right_thumb_finger.BMP', '206__M_Left_index_finger.BMP', '206__M_Left_little_finger.BMP', '206__M_Left_middle_finger.BMP', '206__M_Left_ring_finger.BMP', '206__M_Left_thumb_finger.BMP', '206__M_Right_index_finger.BMP', '206__M_Right_little_finger.BMP', '206__M_Right_middle_finger.BMP', '206__M_Right_ring_finger.BMP', '206__M_Right_thumb_finger.BMP', '207__M_Left_index_finger.BMP', '207__M_Left_little_finger.BMP', '207__M_Left_middle_finger.BMP', '207__M_Left_ring_finger.BMP', '207__M_Left_thumb_finger.BMP', '207__M_Right_index_finger.BMP', '207__M_Right_little_finger.BMP', '207__M_Right_middle_finger.BMP', '207__M_Right_ring_finger.BMP', '207__M_Right_thumb_finger.BMP', '208__M_Left_index_finger.BMP', '208__M_Left_little_finger.BMP', '208__M_Left_middle_finger.BMP', '208__M_Left_ring_finger.BMP', '208__M_Left_thumb_finger.BMP', '208__M_Right_index_finger.BMP', '208__M_Right_little_finger.BMP', '208__M_Right_middle_finger.BMP', '208__M_Right_ring_finger.BMP', '208__M_Right_thumb_finger.BMP', '209__F_Left_index_finger.BMP', '209__F_Left_little_finger.BMP', '209__F_Left_middle_finger.BMP', '209__F_Left_ring_finger.BMP', '209__F_Left_thumb_finger.BMP', '209__F_Right_index_finger.BMP', '209__F_Right_little_finger.BMP', '209__F_Right_middle_finger.BMP', '209__F_Right_ring_finger.BMP', '209__F_Right_thumb_finger.BMP', '20__M_Left_index_finger.BMP', '20__M_Left_little_finger.BMP', '20__M_Left_middle_finger.BMP', '20__M_Left_ring_finger.BMP', '20__M_Left_thumb_finger.BMP', '20__M_Right_index_finger.BMP', '20__M_Right_little_finger.BMP', '20__M_Right_middle_finger.BMP', '20__M_Right_ring_finger.BMP', '20__M_Right_thumb_finger.BMP', '210__M_Left_index_finger.BMP', '210__M_Left_little_finger.BMP', '210__M_Left_middle_finger.BMP', '210__M_Left_ring_finger.BMP', '210__M_Left_thumb_finger.BMP', '210__M_Right_index_finger.BMP', '210__M_Right_little_finger.BMP', '210__M_Right_middle_finger.BMP', '210__M_Right_ring_finger.BMP', '210__M_Right_thumb_finger.BMP', '211__M_Left_index_finger.BMP', '211__M_Left_little_finger.BMP', '211__M_Left_middle_finger.BMP', '211__M_Left_ring_finger.BMP', '211__M_Left_thumb_finger.BMP', '211__M_Right_index_finger.BMP', '211__M_Right_little_finger.BMP', '211__M_Right_middle_finger.BMP', '211__M_Right_ring_finger.BMP', '211__M_Right_thumb_finger.BMP', '212__M_Left_index_finger.BMP', '212__M_Left_little_finger.BMP', '212__M_Left_middle_finger.BMP', '212__M_Left_ring_finger.BMP', '212__M_Left_thumb_finger.BMP', '212__M_Right_index_finger.BMP', '212__M_Right_little_finger.BMP', '212__M_Right_middle_finger.BMP', '212__M_Right_ring_finger.BMP', '212__M_Right_thumb_finger.BMP', '213__M_Left_index_finger.BMP', '213__M_Left_little_finger.BMP', '213__M_Left_middle_finger.BMP', '213__M_Left_ring_finger.BMP', '213__M_Left_thumb_finger.BMP', '213__M_Right_index_finger.BMP', '213__M_Right_little_finger.BMP', '213__M_Right_middle_finger.BMP', '213__M_Right_ring_finger.BMP', '213__M_Right_thumb_finger.BMP', '214__M_Left_index_finger.BMP', '214__M_Left_little_finger.BMP', '214__M_Left_middle_finger.BMP', '214__M_Left_ring_finger.BMP', '214__M_Left_thumb_finger.BMP', '214__M_Right_index_finger.BMP', '214__M_Right_little_finger.BMP', '214__M_Right_middle_finger.BMP', '214__M_Right_ring_finger.BMP', '214__M_Right_thumb_finger.BMP', '215__M_Left_index_finger.BMP', '215__M_Left_little_finger.BMP', '215__M_Left_middle_finger.BMP', '215__M_Left_ring_finger.BMP', '215__M_Left_thumb_finger.BMP', '215__M_Right_index_finger.BMP', '215__M_Right_little_finger.BMP', '215__M_Right_middle_finger.BMP', '215__M_Right_ring_finger.BMP', '215__M_Right_thumb_finger.BMP', '216__M_Left_index_finger.BMP', '216__M_Left_little_finger.BMP', '216__M_Left_middle_finger.BMP', '216__M_Left_ring_finger.BMP', '216__M_Left_thumb_finger.BMP', '216__M_Right_index_finger.BMP', '216__M_Right_little_finger.BMP', '216__M_Right_middle_finger.BMP', '216__M_Right_ring_finger.BMP', '216__M_Right_thumb_finger.BMP', '217__M_Left_index_finger.BMP', '217__M_Left_little_finger.BMP', '217__M_Left_middle_finger.BMP', '217__M_Left_ring_finger.BMP', '217__M_Left_thumb_finger.BMP', '217__M_Right_index_finger.BMP', '217__M_Right_little_finger.BMP', '217__M_Right_middle_finger.BMP', '217__M_Right_ring_finger.BMP', '217__M_Right_thumb_finger.BMP', '218__M_Left_index_finger.BMP', '218__M_Left_little_finger.BMP', '218__M_Left_middle_finger.BMP', '218__M_Left_ring_finger.BMP', '218__M_Left_thumb_finger.BMP', '218__M_Right_index_finger.BMP', '218__M_Right_little_finger.BMP', '218__M_Right_middle_finger.BMP', '218__M_Right_ring_finger.BMP', '218__M_Right_thumb_finger.BMP', '219__M_Left_index_finger.BMP', '219__M_Left_little_finger.BMP', '219__M_Left_middle_finger.BMP', '219__M_Left_ring_finger.BMP', '219__M_Left_thumb_finger.BMP', '219__M_Right_index_finger.BMP', '219__M_Right_little_finger.BMP', '219__M_Right_middle_finger.BMP', '219__M_Right_ring_finger.BMP', '219__M_Right_thumb_finger.BMP', '21__M_Left_index_finger.BMP', '21__M_Left_little_finger.BMP', '21__M_Left_middle_finger.BMP', '21__M_Left_ring_finger.BMP', '21__M_Left_thumb_finger.BMP', '21__M_Right_index_finger.BMP', '21__M_Right_little_finger.BMP', '21__M_Right_middle_finger.BMP', '21__M_Right_ring_finger.BMP', '21__M_Right_thumb_finger.BMP', '220__F_Left_index_finger.BMP', '220__F_Left_little_finger.BMP', '220__F_Left_middle_finger.BMP', '220__F_Left_ring_finger.BMP', '220__F_Left_thumb_finger.BMP', '220__F_Right_index_finger.BMP', '220__F_Right_little_finger.BMP', '220__F_Right_middle_finger.BMP', '220__F_Right_ring_finger.BMP', '220__F_Right_thumb_finger.BMP', '221__M_Left_index_finger.BMP', '221__M_Left_little_finger.BMP', '221__M_Left_middle_finger.BMP', '221__M_Left_ring_finger.BMP', '221__M_Left_thumb_finger.BMP', '221__M_Right_index_finger.BMP', '221__M_Right_little_finger.BMP', '221__M_Right_middle_finger.BMP', '221__M_Right_ring_finger.BMP', '221__M_Right_thumb_finger.BMP', '222__M_Left_index_finger.BMP', '222__M_Left_little_finger.BMP', '222__M_Left_middle_finger.BMP', '222__M_Left_ring_finger.BMP', '222__M_Left_thumb_finger.BMP', '222__M_Right_index_finger.BMP', '222__M_Right_little_finger.BMP', '222__M_Right_middle_finger.BMP', '222__M_Right_ring_finger.BMP', '222__M_Right_thumb_finger.BMP', '223__M_Left_index_finger.BMP', '223__M_Left_little_finger.BMP', '223__M_Left_middle_finger.BMP', '223__M_Left_ring_finger.BMP', '223__M_Left_thumb_finger.BMP', '223__M_Right_index_finger.BMP', '223__M_Right_little_finger.BMP', '223__M_Right_middle_finger.BMP', '223__M_Right_ring_finger.BMP', '223__M_Right_thumb_finger.BMP', '224__M_Left_index_finger.BMP', '224__M_Left_little_finger.BMP', '224__M_Left_middle_finger.BMP', '224__M_Left_ring_finger.BMP', '224__M_Left_thumb_finger.BMP', '224__M_Right_index_finger.BMP', '224__M_Right_little_finger.BMP', '224__M_Right_middle_finger.BMP', '224__M_Right_ring_finger.BMP', '224__M_Right_thumb_finger.BMP', '225__M_Left_index_finger.BMP', '225__M_Left_little_finger.BMP', '225__M_Left_middle_finger.BMP', '225__M_Left_ring_finger.BMP', '225__M_Left_thumb_finger.BMP', '225__M_Right_index_finger.BMP', '225__M_Right_little_finger.BMP', '225__M_Right_middle_finger.BMP', '225__M_Right_ring_finger.BMP', '225__M_Right_thumb_finger.BMP', '226__M_Left_index_finger.BMP', '226__M_Left_little_finger.BMP', '226__M_Left_middle_finger.BMP', '226__M_Left_ring_finger.BMP', '226__M_Left_thumb_finger.BMP', '226__M_Right_index_finger.BMP', '226__M_Right_little_finger.BMP', '226__M_Right_middle_finger.BMP', '226__M_Right_ring_finger.BMP', '226__M_Right_thumb_finger.BMP', '227__M_Left_index_finger.BMP', '227__M_Left_little_finger.BMP', '227__M_Left_middle_finger.BMP', '227__M_Left_ring_finger.BMP', '227__M_Left_thumb_finger.BMP', '227__M_Right_index_finger.BMP', '227__M_Right_little_finger.BMP', '227__M_Right_middle_finger.BMP', '227__M_Right_ring_finger.BMP', '227__M_Right_thumb_finger.BMP', '228__M_Left_index_finger.BMP', '228__M_Left_little_finger.BMP', '228__M_Left_middle_finger.BMP', '228__M_Left_ring_finger.BMP', '228__M_Left_thumb_finger.BMP', '228__M_Right_index_finger.BMP', '228__M_Right_little_finger.BMP', '228__M_Right_middle_finger.BMP', '228__M_Right_ring_finger.BMP', '228__M_Right_thumb_finger.BMP', '229__M_Left_index_finger.BMP', '229__M_Left_little_finger.BMP', '229__M_Left_middle_finger.BMP', '229__M_Left_ring_finger.BMP', '229__M_Left_thumb_finger.BMP', '229__M_Right_index_finger.BMP', '229__M_Right_little_finger.BMP', '229__M_Right_middle_finger.BMP', '229__M_Right_ring_finger.BMP', '229__M_Right_thumb_finger.BMP', '22__M_Left_index_finger.BMP', '22__M_Left_little_finger.BMP', '22__M_Left_middle_finger.BMP', '22__M_Left_ring_finger.BMP', '22__M_Left_thumb_finger.BMP', '22__M_Right_index_finger.BMP', '22__M_Right_little_finger.BMP', '22__M_Right_middle_finger.BMP', '22__M_Right_ring_finger.BMP', '22__M_Right_thumb_finger.BMP', '230__M_Left_index_finger.BMP', '230__M_Left_little_finger.BMP', '230__M_Left_middle_finger.BMP', '230__M_Left_ring_finger.BMP', '230__M_Left_thumb_finger.BMP', '230__M_Right_index_finger.BMP', '230__M_Right_little_finger.BMP', '230__M_Right_middle_finger.BMP', '230__M_Right_ring_finger.BMP', '230__M_Right_thumb_finger.BMP', '231__M_Left_index_finger.BMP', '231__M_Left_little_finger.BMP', '231__M_Left_middle_finger.BMP', '231__M_Left_ring_finger.BMP', '231__M_Left_thumb_finger.BMP', '231__M_Right_index_finger.BMP', '231__M_Right_little_finger.BMP', '231__M_Right_middle_finger.BMP', '231__M_Right_ring_finger.BMP', '231__M_Right_thumb_finger.BMP', '232__M_Left_index_finger.BMP', '232__M_Left_little_finger.BMP', '232__M_Left_middle_finger.BMP', '232__M_Left_ring_finger.BMP', '232__M_Left_thumb_finger.BMP', '232__M_Right_index_finger.BMP', '232__M_Right_little_finger.BMP', '232__M_Right_middle_finger.BMP', '232__M_Right_ring_finger.BMP', '232__M_Right_thumb_finger.BMP', '233__M_Left_index_finger.BMP', '233__M_Left_little_finger.BMP', '233__M_Left_middle_finger.BMP', '233__M_Left_ring_finger.BMP', '233__M_Left_thumb_finger.BMP', '233__M_Right_index_finger.BMP', '233__M_Right_little_finger.BMP', '233__M_Right_middle_finger.BMP', '233__M_Right_ring_finger.BMP', '233__M_Right_thumb_finger.BMP', '234__M_Left_index_finger.BMP', '234__M_Left_little_finger.BMP', '234__M_Left_middle_finger.BMP', '234__M_Left_ring_finger.BMP', '234__M_Left_thumb_finger.BMP', '234__M_Right_index_finger.BMP', '234__M_Right_little_finger.BMP', '234__M_Right_middle_finger.BMP', '234__M_Right_ring_finger.BMP', '234__M_Right_thumb_finger.BMP', '235__M_Left_index_finger.BMP', '235__M_Left_little_finger.BMP', '235__M_Left_middle_finger.BMP', '235__M_Left_ring_finger.BMP', '235__M_Left_thumb_finger.BMP', '235__M_Right_index_finger.BMP', '235__M_Right_little_finger.BMP', '235__M_Right_middle_finger.BMP', '235__M_Right_ring_finger.BMP', '235__M_Right_thumb_finger.BMP', '236__M_Left_index_finger.BMP', '236__M_Left_little_finger.BMP', '236__M_Left_middle_finger.BMP', '236__M_Left_ring_finger.BMP', '236__M_Left_thumb_finger.BMP', '236__M_Right_index_finger.BMP', '236__M_Right_little_finger.BMP', '236__M_Right_middle_finger.BMP', '236__M_Right_ring_finger.BMP', '236__M_Right_thumb_finger.BMP', '237__M_Left_index_finger.BMP', '237__M_Left_little_finger.BMP', '237__M_Left_middle_finger.BMP', '237__M_Left_ring_finger.BMP', '237__M_Left_thumb_finger.BMP', '237__M_Right_index_finger.BMP', '237__M_Right_little_finger.BMP', '237__M_Right_middle_finger.BMP', '237__M_Right_ring_finger.BMP', '237__M_Right_thumb_finger.BMP', '238__M_Left_index_finger.BMP', '238__M_Left_little_finger.BMP', '238__M_Left_middle_finger.BMP', '238__M_Left_ring_finger.BMP', '238__M_Left_thumb_finger.BMP', '238__M_Right_index_finger.BMP', '238__M_Right_little_finger.BMP', '238__M_Right_middle_finger.BMP', '238__M_Right_ring_finger.BMP', '238__M_Right_thumb_finger.BMP', '239__M_Left_index_finger.BMP', '239__M_Left_little_finger.BMP', '239__M_Left_middle_finger.BMP', '239__M_Left_ring_finger.BMP', '239__M_Left_thumb_finger.BMP', '239__M_Right_index_finger.BMP', '239__M_Right_little_finger.BMP', '239__M_Right_middle_finger.BMP', '239__M_Right_ring_finger.BMP', '239__M_Right_thumb_finger.BMP', '23__M_Left_index_finger.BMP', '23__M_Left_little_finger.BMP', '23__M_Left_middle_finger.BMP', '23__M_Left_ring_finger.BMP', '23__M_Left_thumb_finger.BMP', '23__M_Right_index_finger.BMP', '23__M_Right_little_finger.BMP', '23__M_Right_middle_finger.BMP', '23__M_Right_ring_finger.BMP', '23__M_Right_thumb_finger.BMP', '240__F_Left_index_finger.BMP', '240__F_Left_little_finger.BMP', '240__F_Left_middle_finger.BMP', '240__F_Left_ring_finger.BMP', '240__F_Left_thumb_finger.BMP', '240__F_Right_index_finger.BMP', '240__F_Right_little_finger.BMP', '240__F_Right_middle_finger.BMP', '240__F_Right_ring_finger.BMP', '240__F_Right_thumb_finger.BMP', '241__M_Left_index_finger.BMP', '241__M_Left_little_finger.BMP', '241__M_Left_middle_finger.BMP', '241__M_Left_ring_finger.BMP', '241__M_Left_thumb_finger.BMP', '241__M_Right_index_finger.BMP', '241__M_Right_little_finger.BMP', '241__M_Right_middle_finger.BMP', '241__M_Right_ring_finger.BMP', '241__M_Right_thumb_finger.BMP', '242__M_Left_index_finger.BMP', '242__M_Left_little_finger.BMP', '242__M_Left_middle_finger.BMP', '242__M_Left_ring_finger.BMP', '242__M_Left_thumb_finger.BMP', '242__M_Right_index_finger.BMP', '242__M_Right_little_finger.BMP', '242__M_Right_middle_finger.BMP', '242__M_Right_ring_finger.BMP', '242__M_Right_thumb_finger.BMP', '243__M_Left_index_finger.BMP', '243__M_Left_little_finger.BMP', '243__M_Left_middle_finger.BMP', '243__M_Left_ring_finger.BMP', '243__M_Left_thumb_finger.BMP', '243__M_Right_index_finger.BMP', '243__M_Right_little_finger.BMP', '243__M_Right_middle_finger.BMP', '243__M_Right_ring_finger.BMP', '243__M_Right_thumb_finger.BMP', '244__M_Left_index_finger.BMP', '244__M_Left_little_finger.BMP', '244__M_Left_middle_finger.BMP', '244__M_Left_ring_finger.BMP', '244__M_Left_thumb_finger.BMP', '244__M_Right_index_finger.BMP', '244__M_Right_little_finger.BMP', '244__M_Right_middle_finger.BMP', '244__M_Right_ring_finger.BMP', '244__M_Right_thumb_finger.BMP', '245__M_Left_index_finger.BMP', '245__M_Left_little_finger.BMP', '245__M_Left_middle_finger.BMP', '245__M_Left_ring_finger.BMP', '245__M_Left_thumb_finger.BMP', '245__M_Right_index_finger.BMP', '245__M_Right_little_finger.BMP', '245__M_Right_middle_finger.BMP', '245__M_Right_ring_finger.BMP', '245__M_Right_thumb_finger.BMP', '246__M_Left_index_finger.BMP', '246__M_Left_little_finger.BMP', '246__M_Left_middle_finger.BMP', '246__M_Left_ring_finger.BMP', '246__M_Left_thumb_finger.BMP', '246__M_Right_index_finger.BMP', '246__M_Right_little_finger.BMP', '246__M_Right_middle_finger.BMP', '246__M_Right_ring_finger.BMP', '246__M_Right_thumb_finger.BMP', '247__M_Left_index_finger.BMP', '247__M_Left_little_finger.BMP', '247__M_Left_middle_finger.BMP', '247__M_Left_ring_finger.BMP', '247__M_Left_thumb_finger.BMP', '247__M_Right_index_finger.BMP', '247__M_Right_little_finger.BMP', '247__M_Right_middle_finger.BMP', '247__M_Right_ring_finger.BMP', '247__M_Right_thumb_finger.BMP', '248__M_Left_index_finger.BMP', '248__M_Left_little_finger.BMP', '248__M_Left_middle_finger.BMP', '248__M_Left_ring_finger.BMP', '248__M_Left_thumb_finger.BMP', '248__M_Right_index_finger.BMP', '248__M_Right_little_finger.BMP', '248__M_Right_middle_finger.BMP', '248__M_Right_ring_finger.BMP', '248__M_Right_thumb_finger.BMP', '249__M_Left_index_finger.BMP', '249__M_Left_little_finger.BMP', '249__M_Left_middle_finger.BMP', '249__M_Left_ring_finger.BMP', '249__M_Left_thumb_finger.BMP', '249__M_Right_index_finger.BMP', '249__M_Right_little_finger.BMP', '249__M_Right_middle_finger.BMP', '249__M_Right_ring_finger.BMP', '249__M_Right_thumb_finger.BMP', '24__F_Left_index_finger.BMP', '24__F_Left_little_finger.BMP', '24__F_Left_middle_finger.BMP', '24__F_Left_ring_finger.BMP', '24__F_Left_thumb_finger.BMP', '24__F_Right_index_finger.BMP', '24__F_Right_little_finger.BMP', '24__F_Right_middle_finger.BMP', '24__F_Right_ring_finger.BMP', '24__F_Right_thumb_finger.BMP', '250__F_Left_index_finger.BMP', '250__F_Left_little_finger.BMP', '250__F_Left_middle_finger.BMP', '250__F_Left_ring_finger.BMP', '250__F_Left_thumb_finger.BMP', '250__F_Right_index_finger.BMP', '250__F_Right_little_finger.BMP', '250__F_Right_middle_finger.BMP', '250__F_Right_ring_finger.BMP', '250__F_Right_thumb_finger.BMP', '251__M_Left_index_finger.BMP', '251__M_Left_little_finger.BMP', '251__M_Left_middle_finger.BMP', '251__M_Left_ring_finger.BMP', '251__M_Left_thumb_finger.BMP', '251__M_Right_index_finger.BMP', '251__M_Right_little_finger.BMP', '251__M_Right_middle_finger.BMP', '251__M_Right_ring_finger.BMP', '251__M_Right_thumb_finger.BMP', '252__F_Left_index_finger.BMP', '252__F_Left_little_finger.BMP', '252__F_Left_middle_finger.BMP', '252__F_Left_ring_finger.BMP', '252__F_Left_thumb_finger.BMP', '252__F_Right_index_finger.BMP', '252__F_Right_little_finger.BMP', '252__F_Right_middle_finger.BMP', '252__F_Right_ring_finger.BMP', '252__F_Right_thumb_finger.BMP', '253__F_Left_index_finger.BMP', '253__F_Left_little_finger.BMP', '253__F_Left_middle_finger.BMP', '253__F_Left_ring_finger.BMP', '253__F_Left_thumb_finger.BMP', '253__F_Right_index_finger.BMP', '253__F_Right_little_finger.BMP', '253__F_Right_middle_finger.BMP', '253__F_Right_ring_finger.BMP', '253__F_Right_thumb_finger.BMP', '254__M_Left_index_finger.BMP', '254__M_Left_little_finger.BMP', '254__M_Left_middle_finger.BMP', '254__M_Left_ring_finger.BMP', '254__M_Left_thumb_finger.BMP', '254__M_Right_index_finger.BMP', '254__M_Right_little_finger.BMP', '254__M_Right_middle_finger.BMP', '254__M_Right_ring_finger.BMP', '254__M_Right_thumb_finger.BMP', '255__M_Left_index_finger.BMP', '255__M_Left_little_finger.BMP', '255__M_Left_middle_finger.BMP', '255__M_Left_ring_finger.BMP', '255__M_Left_thumb_finger.BMP', '255__M_Right_index_finger.BMP', '255__M_Right_little_finger.BMP', '255__M_Right_middle_finger.BMP', '255__M_Right_ring_finger.BMP', '255__M_Right_thumb_finger.BMP', '256__M_Left_index_finger.BMP', '256__M_Left_little_finger.BMP', '256__M_Left_middle_finger.BMP', '256__M_Left_ring_finger.BMP', '256__M_Left_thumb_finger.BMP', '256__M_Right_index_finger.BMP', '256__M_Right_little_finger.BMP', '256__M_Right_middle_finger.BMP', '256__M_Right_ring_finger.BMP', '256__M_Right_thumb_finger.BMP', '257__M_Left_index_finger.BMP', '257__M_Left_little_finger.BMP', '257__M_Left_middle_finger.BMP', '257__M_Left_ring_finger.BMP', '257__M_Left_thumb_finger.BMP', '257__M_Right_index_finger.BMP', '257__M_Right_little_finger.BMP', '257__M_Right_middle_finger.BMP', '257__M_Right_ring_finger.BMP', '257__M_Right_thumb_finger.BMP', '258__M_Left_index_finger.BMP', '258__M_Left_little_finger.BMP', '258__M_Left_middle_finger.BMP', '258__M_Left_ring_finger.BMP', '258__M_Left_thumb_finger.BMP', '258__M_Right_index_finger.BMP', '258__M_Right_little_finger.BMP', '258__M_Right_middle_finger.BMP', '258__M_Right_ring_finger.BMP', '258__M_Right_thumb_finger.BMP', '259__M_Left_index_finger.BMP', '259__M_Left_little_finger.BMP', '259__M_Left_middle_finger.BMP', '259__M_Left_ring_finger.BMP', '259__M_Left_thumb_finger.BMP', '259__M_Right_index_finger.BMP', '259__M_Right_little_finger.BMP', '259__M_Right_middle_finger.BMP', '259__M_Right_ring_finger.BMP', '259__M_Right_thumb_finger.BMP', '25__F_Left_index_finger.BMP', '25__F_Left_little_finger.BMP', '25__F_Left_middle_finger.BMP', '25__F_Left_ring_finger.BMP', '25__F_Left_thumb_finger.BMP', '25__F_Right_index_finger.BMP', '25__F_Right_little_finger.BMP', '25__F_Right_middle_finger.BMP', '25__F_Right_ring_finger.BMP', '25__F_Right_thumb_finger.BMP', '260__M_Left_index_finger.BMP', '260__M_Left_little_finger.BMP', '260__M_Left_middle_finger.BMP', '260__M_Left_ring_finger.BMP', '260__M_Left_thumb_finger.BMP', '260__M_Right_index_finger.BMP', '260__M_Right_little_finger.BMP', '260__M_Right_middle_finger.BMP', '260__M_Right_ring_finger.BMP', '260__M_Right_thumb_finger.BMP', '261__M_Left_index_finger.BMP', '261__M_Left_little_finger.BMP', '261__M_Left_middle_finger.BMP', '261__M_Left_ring_finger.BMP', '261__M_Left_thumb_finger.BMP', '261__M_Right_index_finger.BMP', '261__M_Right_little_finger.BMP', '261__M_Right_middle_finger.BMP', '261__M_Right_ring_finger.BMP', '261__M_Right_thumb_finger.BMP', '262__M_Left_index_finger.BMP', '262__M_Left_little_finger.BMP', '262__M_Left_middle_finger.BMP', '262__M_Left_ring_finger.BMP', '262__M_Left_thumb_finger.BMP', '262__M_Right_index_finger.BMP', '262__M_Right_little_finger.BMP', '262__M_Right_middle_finger.BMP', '262__M_Right_ring_finger.BMP', '262__M_Right_thumb_finger.BMP', '263__F_Left_index_finger.BMP', '263__F_Left_little_finger.BMP', '263__F_Left_middle_finger.BMP', '263__F_Left_ring_finger.BMP', '263__F_Left_thumb_finger.BMP', '263__F_Right_index_finger.BMP', '263__F_Right_little_finger.BMP', '263__F_Right_middle_finger.BMP', '263__F_Right_ring_finger.BMP', '263__F_Right_thumb_finger.BMP', '264__M_Left_index_finger.BMP', '264__M_Left_little_finger.BMP', '264__M_Left_middle_finger.BMP', '264__M_Left_ring_finger.BMP', '264__M_Left_thumb_finger.BMP', '264__M_Right_index_finger.BMP', '264__M_Right_little_finger.BMP', '264__M_Right_middle_finger.BMP', '264__M_Right_ring_finger.BMP', '264__M_Right_thumb_finger.BMP', '265__M_Left_index_finger.BMP', '265__M_Left_little_finger.BMP', '265__M_Left_middle_finger.BMP', '265__M_Left_ring_finger.BMP', '265__M_Left_thumb_finger.BMP', '265__M_Right_index_finger.BMP', '265__M_Right_little_finger.BMP', '265__M_Right_middle_finger.BMP', '265__M_Right_ring_finger.BMP', '265__M_Right_thumb_finger.BMP', '266__M_Left_index_finger.BMP', '266__M_Left_little_finger.BMP', '266__M_Left_middle_finger.BMP', '266__M_Left_ring_finger.BMP', '266__M_Left_thumb_finger.BMP', '266__M_Right_index_finger.BMP', '266__M_Right_little_finger.BMP', '266__M_Right_middle_finger.BMP', '266__M_Right_ring_finger.BMP', '266__M_Right_thumb_finger.BMP', '267__M_Left_index_finger.BMP', '267__M_Left_little_finger.BMP', '267__M_Left_middle_finger.BMP', '267__M_Left_ring_finger.BMP', '267__M_Left_thumb_finger.BMP', '267__M_Right_index_finger.BMP', '267__M_Right_little_finger.BMP', '267__M_Right_middle_finger.BMP', '267__M_Right_ring_finger.BMP', '267__M_Right_thumb_finger.BMP', '268__M_Left_index_finger.BMP', '268__M_Left_little_finger.BMP', '268__M_Left_middle_finger.BMP', '268__M_Left_ring_finger.BMP', '268__M_Left_thumb_finger.BMP', '268__M_Right_index_finger.BMP', '268__M_Right_little_finger.BMP', '268__M_Right_middle_finger.BMP', '268__M_Right_ring_finger.BMP', '268__M_Right_thumb_finger.BMP', '269__M_Left_index_finger.BMP', '269__M_Left_little_finger.BMP', '269__M_Left_middle_finger.BMP', '269__M_Left_ring_finger.BMP', '269__M_Left_thumb_finger.BMP', '269__M_Right_index_finger.BMP', '269__M_Right_little_finger.BMP', '269__M_Right_middle_finger.BMP', '269__M_Right_ring_finger.BMP', '269__M_Right_thumb_finger.BMP', '26__M_Left_index_finger.BMP', '26__M_Left_little_finger.BMP', '26__M_Left_middle_finger.BMP', '26__M_Left_ring_finger.BMP', '26__M_Left_thumb_finger.BMP', '26__M_Right_index_finger.BMP', '26__M_Right_little_finger.BMP', '26__M_Right_middle_finger.BMP', '26__M_Right_ring_finger.BMP', '26__M_Right_thumb_finger.BMP', '270__M_Left_index_finger.BMP', '270__M_Left_little_finger.BMP', '270__M_Left_middle_finger.BMP', '270__M_Left_ring_finger.BMP', '270__M_Left_thumb_finger.BMP', '270__M_Right_index_finger.BMP', '270__M_Right_little_finger.BMP', '270__M_Right_middle_finger.BMP', '270__M_Right_ring_finger.BMP', '270__M_Right_thumb_finger.BMP', '271__M_Left_index_finger.BMP', '271__M_Left_little_finger.BMP', '271__M_Left_middle_finger.BMP', '271__M_Left_ring_finger.BMP', '271__M_Left_thumb_finger.BMP', '271__M_Right_index_finger.BMP', '271__M_Right_little_finger.BMP', '271__M_Right_middle_finger.BMP', '271__M_Right_ring_finger.BMP', '271__M_Right_thumb_finger.BMP', '272__M_Left_index_finger.BMP', '272__M_Left_little_finger.BMP', '272__M_Left_middle_finger.BMP', '272__M_Left_ring_finger.BMP', '272__M_Left_thumb_finger.BMP', '272__M_Right_index_finger.BMP', '272__M_Right_little_finger.BMP', '272__M_Right_middle_finger.BMP', '272__M_Right_ring_finger.BMP', '272__M_Right_thumb_finger.BMP', '273__M_Left_index_finger.BMP', '273__M_Left_little_finger.BMP', '273__M_Left_middle_finger.BMP', '273__M_Left_ring_finger.BMP', '273__M_Left_thumb_finger.BMP', '273__M_Right_index_finger.BMP', '273__M_Right_little_finger.BMP', '273__M_Right_middle_finger.BMP', '273__M_Right_ring_finger.BMP', '273__M_Right_thumb_finger.BMP', '274__M_Left_index_finger.BMP', '274__M_Left_little_finger.BMP', '274__M_Left_middle_finger.BMP', '274__M_Left_ring_finger.BMP', '274__M_Left_thumb_finger.BMP', '274__M_Right_index_finger.BMP', '274__M_Right_little_finger.BMP', '274__M_Right_middle_finger.BMP', '274__M_Right_ring_finger.BMP', '274__M_Right_thumb_finger.BMP', '275__F_Left_index_finger.BMP', '275__F_Left_little_finger.BMP', '275__F_Left_middle_finger.BMP', '275__F_Left_ring_finger.BMP', '275__F_Left_thumb_finger.BMP', '275__F_Right_index_finger.BMP', '275__F_Right_little_finger.BMP', '275__F_Right_middle_finger.BMP', '275__F_Right_ring_finger.BMP', '275__F_Right_thumb_finger.BMP', '276__M_Left_index_finger.BMP', '276__M_Left_little_finger.BMP', '276__M_Left_middle_finger.BMP', '276__M_Left_ring_finger.BMP', '276__M_Left_thumb_finger.BMP', '276__M_Right_index_finger.BMP', '276__M_Right_little_finger.BMP', '276__M_Right_middle_finger.BMP', '276__M_Right_ring_finger.BMP', '276__M_Right_thumb_finger.BMP', '277__M_Left_index_finger.BMP', '277__M_Left_little_finger.BMP', '277__M_Left_middle_finger.BMP', '277__M_Left_ring_finger.BMP', '277__M_Left_thumb_finger.BMP', '277__M_Right_index_finger.BMP', '277__M_Right_little_finger.BMP', '277__M_Right_middle_finger.BMP', '277__M_Right_ring_finger.BMP', '277__M_Right_thumb_finger.BMP', '278__M_Left_index_finger.BMP', '278__M_Left_little_finger.BMP', '278__M_Left_middle_finger.BMP', '278__M_Left_ring_finger.BMP', '278__M_Left_thumb_finger.BMP', '278__M_Right_index_finger.BMP', '278__M_Right_little_finger.BMP', '278__M_Right_middle_finger.BMP', '278__M_Right_ring_finger.BMP', '278__M_Right_thumb_finger.BMP', '279__M_Left_index_finger.BMP', '279__M_Left_little_finger.BMP', '279__M_Left_middle_finger.BMP', '279__M_Left_ring_finger.BMP', '279__M_Left_thumb_finger.BMP', '279__M_Right_index_finger.BMP', '279__M_Right_little_finger.BMP', '279__M_Right_middle_finger.BMP', '279__M_Right_ring_finger.BMP', '279__M_Right_thumb_finger.BMP', '27__M_Left_index_finger.BMP', '27__M_Left_little_finger.BMP', '27__M_Left_middle_finger.BMP', '27__M_Left_ring_finger.BMP', '27__M_Left_thumb_finger.BMP', '27__M_Right_index_finger.BMP', '27__M_Right_little_finger.BMP', '27__M_Right_middle_finger.BMP', '27__M_Right_ring_finger.BMP', '27__M_Right_thumb_finger.BMP', '280__M_Left_index_finger.BMP', '280__M_Left_little_finger.BMP', '280__M_Left_middle_finger.BMP', '280__M_Left_ring_finger.BMP', '280__M_Left_thumb_finger.BMP', '280__M_Right_index_finger.BMP', '280__M_Right_little_finger.BMP', '280__M_Right_middle_finger.BMP', '280__M_Right_ring_finger.BMP', '280__M_Right_thumb_finger.BMP', '281__M_Left_index_finger.BMP', '281__M_Left_little_finger.BMP', '281__M_Left_middle_finger.BMP', '281__M_Left_ring_finger.BMP', '281__M_Left_thumb_finger.BMP', '281__M_Right_index_finger.BMP', '281__M_Right_little_finger.BMP', '281__M_Right_middle_finger.BMP', '281__M_Right_ring_finger.BMP', '281__M_Right_thumb_finger.BMP', '282__M_Left_index_finger.BMP', '282__M_Left_little_finger.BMP', '282__M_Left_middle_finger.BMP', '282__M_Left_ring_finger.BMP', '282__M_Left_thumb_finger.BMP', '282__M_Right_index_finger.BMP', '282__M_Right_little_finger.BMP', '282__M_Right_middle_finger.BMP', '282__M_Right_ring_finger.BMP', '282__M_Right_thumb_finger.BMP', '283__M_Left_index_finger.BMP', '283__M_Left_little_finger.BMP', '283__M_Left_middle_finger.BMP', '283__M_Left_ring_finger.BMP', '283__M_Left_thumb_finger.BMP', '283__M_Right_index_finger.BMP', '283__M_Right_little_finger.BMP', '283__M_Right_middle_finger.BMP', '283__M_Right_ring_finger.BMP', '283__M_Right_thumb_finger.BMP', '284__M_Left_index_finger.BMP', '284__M_Left_little_finger.BMP', '284__M_Left_middle_finger.BMP', '284__M_Left_ring_finger.BMP', '284__M_Left_thumb_finger.BMP', '284__M_Right_index_finger.BMP', '284__M_Right_little_finger.BMP', '284__M_Right_middle_finger.BMP', '284__M_Right_ring_finger.BMP', '284__M_Right_thumb_finger.BMP', '285__M_Left_index_finger.BMP', '285__M_Left_little_finger.BMP', '285__M_Left_middle_finger.BMP', '285__M_Left_ring_finger.BMP', '285__M_Left_thumb_finger.BMP', '285__M_Right_index_finger.BMP', '285__M_Right_little_finger.BMP', '285__M_Right_middle_finger.BMP', '285__M_Right_ring_finger.BMP', '285__M_Right_thumb_finger.BMP', '286__M_Left_index_finger.BMP', '286__M_Left_little_finger.BMP', '286__M_Left_middle_finger.BMP', '286__M_Left_ring_finger.BMP', '286__M_Left_thumb_finger.BMP', '286__M_Right_index_finger.BMP', '286__M_Right_little_finger.BMP', '286__M_Right_middle_finger.BMP', '286__M_Right_ring_finger.BMP', '286__M_Right_thumb_finger.BMP', '287__M_Left_index_finger.BMP', '287__M_Left_little_finger.BMP', '287__M_Left_middle_finger.BMP', '287__M_Left_ring_finger.BMP', '287__M_Left_thumb_finger.BMP', '287__M_Right_index_finger.BMP', '287__M_Right_little_finger.BMP', '287__M_Right_middle_finger.BMP', '287__M_Right_ring_finger.BMP', '287__M_Right_thumb_finger.BMP', '288__M_Left_index_finger.BMP', '288__M_Left_little_finger.BMP', '288__M_Left_middle_finger.BMP', '288__M_Left_ring_finger.BMP', '288__M_Left_thumb_finger.BMP', '288__M_Right_index_finger.BMP', '288__M_Right_little_finger.BMP', '288__M_Right_middle_finger.BMP', '288__M_Right_ring_finger.BMP', '288__M_Right_thumb_finger.BMP', '289__F_Left_index_finger.BMP', '289__F_Left_little_finger.BMP', '289__F_Left_middle_finger.BMP', '289__F_Left_ring_finger.BMP', '289__F_Left_thumb_finger.BMP', '289__F_Right_index_finger.BMP', '289__F_Right_little_finger.BMP', '289__F_Right_middle_finger.BMP', '289__F_Right_ring_finger.BMP', '289__F_Right_thumb_finger.BMP', '28__M_Left_index_finger.BMP', '28__M_Left_little_finger.BMP', '28__M_Left_middle_finger.BMP', '28__M_Left_ring_finger.BMP', '28__M_Left_thumb_finger.BMP', '28__M_Right_index_finger.BMP', '28__M_Right_little_finger.BMP', '28__M_Right_middle_finger.BMP', '28__M_Right_ring_finger.BMP', '28__M_Right_thumb_finger.BMP', '290__M_Left_index_finger.BMP', '290__M_Left_little_finger.BMP', '290__M_Left_middle_finger.BMP', '290__M_Left_ring_finger.BMP', '290__M_Left_thumb_finger.BMP', '290__M_Right_index_finger.BMP', '290__M_Right_little_finger.BMP', '290__M_Right_middle_finger.BMP', '290__M_Right_ring_finger.BMP', '290__M_Right_thumb_finger.BMP', '291__M_Left_index_finger.BMP', '291__M_Left_little_finger.BMP', '291__M_Left_middle_finger.BMP', '291__M_Left_ring_finger.BMP', '291__M_Left_thumb_finger.BMP', '291__M_Right_index_finger.BMP', '291__M_Right_little_finger.BMP', '291__M_Right_middle_finger.BMP', '291__M_Right_ring_finger.BMP', '291__M_Right_thumb_finger.BMP', '292__M_Left_index_finger.BMP', '292__M_Left_little_finger.BMP', '292__M_Left_middle_finger.BMP', '292__M_Left_ring_finger.BMP', '292__M_Left_thumb_finger.BMP', '292__M_Right_index_finger.BMP', '292__M_Right_little_finger.BMP', '292__M_Right_middle_finger.BMP', '292__M_Right_ring_finger.BMP', '292__M_Right_thumb_finger.BMP', '293__F_Left_index_finger.BMP', '293__F_Left_little_finger.BMP', '293__F_Left_middle_finger.BMP', '293__F_Left_ring_finger.BMP', '293__F_Left_thumb_finger.BMP', '293__F_Right_index_finger.BMP', '293__F_Right_little_finger.BMP', '293__F_Right_middle_finger.BMP', '293__F_Right_ring_finger.BMP', '293__F_Right_thumb_finger.BMP', '294__M_Left_index_finger.BMP', '294__M_Left_little_finger.BMP', '294__M_Left_middle_finger.BMP', '294__M_Left_ring_finger.BMP', '294__M_Left_thumb_finger.BMP', '294__M_Right_index_finger.BMP', '294__M_Right_little_finger.BMP', '294__M_Right_middle_finger.BMP', '294__M_Right_ring_finger.BMP', '294__M_Right_thumb_finger.BMP', '295__M_Left_index_finger.BMP', '295__M_Left_little_finger.BMP', '295__M_Left_middle_finger.BMP', '295__M_Left_ring_finger.BMP', '295__M_Left_thumb_finger.BMP', '295__M_Right_index_finger.BMP', '295__M_Right_little_finger.BMP', '295__M_Right_middle_finger.BMP', '295__M_Right_ring_finger.BMP', '295__M_Right_thumb_finger.BMP', '296__M_Left_index_finger.BMP', '296__M_Left_little_finger.BMP', '296__M_Left_middle_finger.BMP', '296__M_Left_ring_finger.BMP', '296__M_Left_thumb_finger.BMP', '296__M_Right_index_finger.BMP', '296__M_Right_little_finger.BMP', '296__M_Right_middle_finger.BMP', '296__M_Right_ring_finger.BMP', '296__M_Right_thumb_finger.BMP', '297__M_Left_index_finger.BMP', '297__M_Left_little_finger.BMP', '297__M_Left_middle_finger.BMP', '297__M_Left_ring_finger.BMP', '297__M_Left_thumb_finger.BMP', '297__M_Right_index_finger.BMP', '297__M_Right_little_finger.BMP', '297__M_Right_middle_finger.BMP', '297__M_Right_ring_finger.BMP', '297__M_Right_thumb_finger.BMP', '298__M_Left_index_finger.BMP', '298__M_Left_little_finger.BMP', '298__M_Left_middle_finger.BMP', '298__M_Left_ring_finger.BMP', '298__M_Left_thumb_finger.BMP', '298__M_Right_index_finger.BMP', '298__M_Right_little_finger.BMP', '298__M_Right_middle_finger.BMP', '298__M_Right_ring_finger.BMP', '298__M_Right_thumb_finger.BMP', '299__M_Left_index_finger.BMP', '299__M_Left_little_finger.BMP', '299__M_Left_middle_finger.BMP', '299__M_Left_ring_finger.BMP', '299__M_Left_thumb_finger.BMP', '299__M_Right_index_finger.BMP', '299__M_Right_little_finger.BMP', '299__M_Right_middle_finger.BMP', '299__M_Right_ring_finger.BMP', '299__M_Right_thumb_finger.BMP', '29__F_Left_index_finger.BMP', '29__F_Left_little_finger.BMP', '29__F_Left_middle_finger.BMP', '29__F_Left_ring_finger.BMP', '29__F_Left_thumb_finger.BMP', '29__F_Right_index_finger.BMP', '29__F_Right_little_finger.BMP', '29__F_Right_middle_finger.BMP', '29__F_Right_ring_finger.BMP', '29__F_Right_thumb_finger.BMP', '2__F_Left_index_finger.BMP', '2__F_Left_little_finger.BMP', '2__F_Left_middle_finger.BMP', '2__F_Left_ring_finger.BMP', '2__F_Left_thumb_finger.BMP', '2__F_Right_index_finger.BMP', '2__F_Right_little_finger.BMP', '2__F_Right_middle_finger.BMP', '2__F_Right_ring_finger.BMP', '2__F_Right_thumb_finger.BMP', '300__F_Left_index_finger.BMP', '300__F_Left_little_finger.BMP', '300__F_Left_middle_finger.BMP', '300__F_Left_ring_finger.BMP', '300__F_Left_thumb_finger.BMP', '300__F_Right_index_finger.BMP', '300__F_Right_little_finger.BMP', '300__F_Right_middle_finger.BMP', '300__F_Right_ring_finger.BMP', '300__F_Right_thumb_finger.BMP', '301__F_Left_index_finger.BMP', '301__F_Left_little_finger.BMP', '301__F_Left_middle_finger.BMP', '301__F_Left_ring_finger.BMP', '301__F_Left_thumb_finger.BMP', '301__F_Right_index_finger.BMP', '301__F_Right_little_finger.BMP', '301__F_Right_middle_finger.BMP', '301__F_Right_ring_finger.BMP', '301__F_Right_thumb_finger.BMP', '302__M_Left_index_finger.BMP', '302__M_Left_little_finger.BMP', '302__M_Left_middle_finger.BMP', '302__M_Left_ring_finger.BMP', '302__M_Left_thumb_finger.BMP', '302__M_Right_index_finger.BMP', '302__M_Right_little_finger.BMP', '302__M_Right_middle_finger.BMP', '302__M_Right_ring_finger.BMP', '302__M_Right_thumb_finger.BMP', '303__F_Left_index_finger.BMP', '303__F_Left_little_finger.BMP', '303__F_Left_middle_finger.BMP', '303__F_Left_ring_finger.BMP', '303__F_Left_thumb_finger.BMP', '303__F_Right_index_finger.BMP', '303__F_Right_little_finger.BMP', '303__F_Right_middle_finger.BMP', '303__F_Right_ring_finger.BMP', '303__F_Right_thumb_finger.BMP', '304__M_Left_index_finger.BMP', '304__M_Left_little_finger.BMP', '304__M_Left_middle_finger.BMP', '304__M_Left_ring_finger.BMP', '304__M_Left_thumb_finger.BMP', '304__M_Right_index_finger.BMP', '304__M_Right_little_finger.BMP', '304__M_Right_middle_finger.BMP', '304__M_Right_ring_finger.BMP', '304__M_Right_thumb_finger.BMP', '305__M_Left_index_finger.BMP', '305__M_Left_little_finger.BMP', '305__M_Left_middle_finger.BMP', '305__M_Left_ring_finger.BMP', '305__M_Left_thumb_finger.BMP', '305__M_Right_index_finger.BMP', '305__M_Right_little_finger.BMP', '305__M_Right_middle_finger.BMP', '305__M_Right_ring_finger.BMP', '305__M_Right_thumb_finger.BMP', '306__M_Left_index_finger.BMP', '306__M_Left_little_finger.BMP', '306__M_Left_middle_finger.BMP', '306__M_Left_ring_finger.BMP', '306__M_Left_thumb_finger.BMP', '306__M_Right_index_finger.BMP', '306__M_Right_little_finger.BMP', '306__M_Right_middle_finger.BMP', '306__M_Right_ring_finger.BMP', '306__M_Right_thumb_finger.BMP', '307__M_Left_index_finger.BMP', '307__M_Left_little_finger.BMP', '307__M_Left_middle_finger.BMP', '307__M_Left_ring_finger.BMP', '307__M_Left_thumb_finger.BMP', '307__M_Right_index_finger.BMP', '307__M_Right_little_finger.BMP', '307__M_Right_middle_finger.BMP', '307__M_Right_ring_finger.BMP', '307__M_Right_thumb_finger.BMP', '308__M_Left_index_finger.BMP', '308__M_Left_little_finger.BMP', '308__M_Left_middle_finger.BMP', '308__M_Left_ring_finger.BMP', '308__M_Left_thumb_finger.BMP', '308__M_Right_index_finger.BMP', '308__M_Right_little_finger.BMP', '308__M_Right_middle_finger.BMP', '308__M_Right_ring_finger.BMP', '308__M_Right_thumb_finger.BMP', '309__M_Left_index_finger.BMP', '309__M_Left_little_finger.BMP', '309__M_Left_middle_finger.BMP', '309__M_Left_ring_finger.BMP', '309__M_Left_thumb_finger.BMP', '309__M_Right_index_finger.BMP', '309__M_Right_little_finger.BMP', '309__M_Right_middle_finger.BMP', '309__M_Right_ring_finger.BMP', '309__M_Right_thumb_finger.BMP', '30__F_Left_index_finger.BMP', '30__F_Left_little_finger.BMP', '30__F_Left_middle_finger.BMP', '30__F_Left_ring_finger.BMP', '30__F_Left_thumb_finger.BMP', '30__F_Right_index_finger.BMP', '30__F_Right_little_finger.BMP', '30__F_Right_middle_finger.BMP', '30__F_Right_ring_finger.BMP', '30__F_Right_thumb_finger.BMP', '310__M_Left_index_finger.BMP', '310__M_Left_little_finger.BMP', '310__M_Left_middle_finger.BMP', '310__M_Left_ring_finger.BMP', '310__M_Left_thumb_finger.BMP', '310__M_Right_index_finger.BMP', '310__M_Right_little_finger.BMP', '310__M_Right_middle_finger.BMP', '310__M_Right_ring_finger.BMP', '310__M_Right_thumb_finger.BMP', '311__M_Left_index_finger.BMP', '311__M_Left_little_finger.BMP', '311__M_Left_middle_finger.BMP', '311__M_Left_ring_finger.BMP', '311__M_Left_thumb_finger.BMP', '311__M_Right_index_finger.BMP', '311__M_Right_little_finger.BMP', '311__M_Right_middle_finger.BMP', '311__M_Right_ring_finger.BMP', '311__M_Right_thumb_finger.BMP', '312__M_Left_index_finger.BMP', '312__M_Left_little_finger.BMP', '312__M_Left_middle_finger.BMP', '312__M_Left_ring_finger.BMP', '312__M_Left_thumb_finger.BMP', '312__M_Right_index_finger.BMP', '312__M_Right_little_finger.BMP', '312__M_Right_middle_finger.BMP', '312__M_Right_ring_finger.BMP', '312__M_Right_thumb_finger.BMP', '313__M_Left_index_finger.BMP', '313__M_Left_little_finger.BMP', '313__M_Left_middle_finger.BMP', '313__M_Left_ring_finger.BMP', '313__M_Left_thumb_finger.BMP', '313__M_Right_index_finger.BMP', '313__M_Right_little_finger.BMP', '313__M_Right_middle_finger.BMP', '313__M_Right_ring_finger.BMP', '313__M_Right_thumb_finger.BMP', '314__M_Left_index_finger.BMP', '314__M_Left_little_finger.BMP', '314__M_Left_middle_finger.BMP', '314__M_Left_ring_finger.BMP', '314__M_Left_thumb_finger.BMP', '314__M_Right_index_finger.BMP', '314__M_Right_little_finger.BMP', '314__M_Right_middle_finger.BMP', '314__M_Right_ring_finger.BMP', '314__M_Right_thumb_finger.BMP', '315__F_Left_index_finger.BMP', '315__F_Left_little_finger.BMP', '315__F_Left_middle_finger.BMP', '315__F_Left_ring_finger.BMP', '315__F_Left_thumb_finger.BMP', '315__F_Right_index_finger.BMP', '315__F_Right_little_finger.BMP', '315__F_Right_middle_finger.BMP', '315__F_Right_ring_finger.BMP', '315__F_Right_thumb_finger.BMP', '316__M_Left_index_finger.BMP', '316__M_Left_little_finger.BMP', '316__M_Left_middle_finger.BMP', '316__M_Left_ring_finger.BMP', '316__M_Left_thumb_finger.BMP', '316__M_Right_index_finger.BMP', '316__M_Right_little_finger.BMP', '316__M_Right_middle_finger.BMP', '316__M_Right_ring_finger.BMP', '316__M_Right_thumb_finger.BMP', '317__M_Left_index_finger.BMP', '317__M_Left_little_finger.BMP', '317__M_Left_middle_finger.BMP', '317__M_Left_ring_finger.BMP', '317__M_Left_thumb_finger.BMP', '317__M_Right_index_finger.BMP', '317__M_Right_little_finger.BMP', '317__M_Right_middle_finger.BMP', '317__M_Right_ring_finger.BMP', '317__M_Right_thumb_finger.BMP', '318__F_Left_index_finger.BMP', '318__F_Left_little_finger.BMP', '318__F_Left_middle_finger.BMP', '318__F_Left_ring_finger.BMP', '318__F_Left_thumb_finger.BMP', '318__F_Right_index_finger.BMP', '318__F_Right_little_finger.BMP', '318__F_Right_middle_finger.BMP', '318__F_Right_ring_finger.BMP', '318__F_Right_thumb_finger.BMP', '319__M_Left_index_finger.BMP', '319__M_Left_little_finger.BMP', '319__M_Left_middle_finger.BMP', '319__M_Left_ring_finger.BMP', '319__M_Left_thumb_finger.BMP', '319__M_Right_index_finger.BMP', '319__M_Right_little_finger.BMP', '319__M_Right_middle_finger.BMP', '319__M_Right_ring_finger.BMP', '319__M_Right_thumb_finger.BMP', '31__F_Left_index_finger.BMP', '31__F_Left_little_finger.BMP', '31__F_Left_middle_finger.BMP', '31__F_Left_ring_finger.BMP', '31__F_Left_thumb_finger.BMP', '31__F_Right_index_finger.BMP', '31__F_Right_little_finger.BMP', '31__F_Right_middle_finger.BMP', '31__F_Right_ring_finger.BMP', '31__F_Right_thumb_finger.BMP', '320__M_Left_index_finger.BMP', '320__M_Left_little_finger.BMP', '320__M_Left_middle_finger.BMP', '320__M_Left_ring_finger.BMP', '320__M_Left_thumb_finger.BMP', '320__M_Right_index_finger.BMP', '320__M_Right_little_finger.BMP', '320__M_Right_middle_finger.BMP', '320__M_Right_ring_finger.BMP', '320__M_Right_thumb_finger.BMP', '321__M_Left_index_finger.BMP', '321__M_Left_little_finger.BMP', '321__M_Left_middle_finger.BMP', '321__M_Left_ring_finger.BMP', '321__M_Left_thumb_finger.BMP', '321__M_Right_index_finger.BMP', '321__M_Right_little_finger.BMP', '321__M_Right_middle_finger.BMP', '321__M_Right_ring_finger.BMP', '321__M_Right_thumb_finger.BMP', '322__M_Left_index_finger.BMP', '322__M_Left_little_finger.BMP', '322__M_Left_middle_finger.BMP', '322__M_Left_ring_finger.BMP', '322__M_Left_thumb_finger.BMP', '322__M_Right_index_finger.BMP', '322__M_Right_little_finger.BMP', '322__M_Right_middle_finger.BMP', '322__M_Right_ring_finger.BMP', '322__M_Right_thumb_finger.BMP', '323__M_Left_index_finger.BMP', '323__M_Left_little_finger.BMP', '323__M_Left_middle_finger.BMP', '323__M_Left_ring_finger.BMP', '323__M_Left_thumb_finger.BMP', '323__M_Right_index_finger.BMP', '323__M_Right_little_finger.BMP', '323__M_Right_middle_finger.BMP', '323__M_Right_ring_finger.BMP', '323__M_Right_thumb_finger.BMP', '324__M_Left_index_finger.BMP', '324__M_Left_little_finger.BMP', '324__M_Left_middle_finger.BMP', '324__M_Left_ring_finger.BMP', '324__M_Left_thumb_finger.BMP', '324__M_Right_index_finger.BMP', '324__M_Right_little_finger.BMP', '324__M_Right_middle_finger.BMP', '324__M_Right_ring_finger.BMP', '324__M_Right_thumb_finger.BMP', '325__M_Left_index_finger.BMP', '325__M_Left_little_finger.BMP', '325__M_Left_middle_finger.BMP', '325__M_Left_ring_finger.BMP', '325__M_Left_thumb_finger.BMP', '325__M_Right_index_finger.BMP', '325__M_Right_little_finger.BMP', '325__M_Right_middle_finger.BMP', '325__M_Right_ring_finger.BMP', '325__M_Right_thumb_finger.BMP', '326__M_Left_index_finger.BMP', '326__M_Left_little_finger.BMP', '326__M_Left_middle_finger.BMP', '326__M_Left_ring_finger.BMP', '326__M_Left_thumb_finger.BMP', '326__M_Right_index_finger.BMP', '326__M_Right_little_finger.BMP', '326__M_Right_middle_finger.BMP', '326__M_Right_ring_finger.BMP', '326__M_Right_thumb_finger.BMP', '327__M_Left_index_finger.BMP', '327__M_Left_little_finger.BMP', '327__M_Left_middle_finger.BMP', '327__M_Left_ring_finger.BMP', '327__M_Left_thumb_finger.BMP', '327__M_Right_index_finger.BMP', '327__M_Right_little_finger.BMP', '327__M_Right_middle_finger.BMP', '327__M_Right_ring_finger.BMP', '327__M_Right_thumb_finger.BMP', '328__M_Left_index_finger.BMP', '328__M_Left_little_finger.BMP', '328__M_Left_middle_finger.BMP', '328__M_Left_ring_finger.BMP', '328__M_Left_thumb_finger.BMP', '328__M_Right_index_finger.BMP', '328__M_Right_little_finger.BMP', '328__M_Right_middle_finger.BMP', '328__M_Right_ring_finger.BMP', '328__M_Right_thumb_finger.BMP', '329__M_Left_index_finger.BMP', '329__M_Left_little_finger.BMP', '329__M_Left_middle_finger.BMP', '329__M_Left_ring_finger.BMP', '329__M_Left_thumb_finger.BMP', '329__M_Right_index_finger.BMP', '329__M_Right_little_finger.BMP', '329__M_Right_middle_finger.BMP', '329__M_Right_ring_finger.BMP', '329__M_Right_thumb_finger.BMP', '32__M_Left_index_finger.BMP', '32__M_Left_little_finger.BMP', '32__M_Left_middle_finger.BMP', '32__M_Left_ring_finger.BMP', '32__M_Left_thumb_finger.BMP', '32__M_Right_index_finger.BMP', '32__M_Right_little_finger.BMP', '32__M_Right_middle_finger.BMP', '32__M_Right_ring_finger.BMP', '32__M_Right_thumb_finger.BMP', '330__M_Left_index_finger.BMP', '330__M_Left_little_finger.BMP', '330__M_Left_middle_finger.BMP', '330__M_Left_ring_finger.BMP', '330__M_Left_thumb_finger.BMP', '330__M_Right_index_finger.BMP', '330__M_Right_little_finger.BMP', '330__M_Right_middle_finger.BMP', '330__M_Right_ring_finger.BMP', '330__M_Right_thumb_finger.BMP', '331__M_Left_index_finger.BMP', '331__M_Left_little_finger.BMP', '331__M_Left_middle_finger.BMP', '331__M_Left_ring_finger.BMP', '331__M_Left_thumb_finger.BMP', '331__M_Right_index_finger.BMP', '331__M_Right_little_finger.BMP', '331__M_Right_middle_finger.BMP', '331__M_Right_ring_finger.BMP', '331__M_Right_thumb_finger.BMP', '332__M_Left_index_finger.BMP', '332__M_Left_little_finger.BMP', '332__M_Left_middle_finger.BMP', '332__M_Left_ring_finger.BMP', '332__M_Left_thumb_finger.BMP', '332__M_Right_index_finger.BMP', '332__M_Right_little_finger.BMP', '332__M_Right_middle_finger.BMP', '332__M_Right_ring_finger.BMP', '332__M_Right_thumb_finger.BMP', '333__M_Left_index_finger.BMP', '333__M_Left_little_finger.BMP', '333__M_Left_middle_finger.BMP', '333__M_Left_ring_finger.BMP', '333__M_Left_thumb_finger.BMP', '333__M_Right_index_finger.BMP', '333__M_Right_little_finger.BMP', '333__M_Right_middle_finger.BMP', '333__M_Right_ring_finger.BMP', '333__M_Right_thumb_finger.BMP', '334__F_Left_index_finger.BMP', '334__F_Left_little_finger.BMP', '334__F_Left_middle_finger.BMP', '334__F_Left_ring_finger.BMP', '334__F_Left_thumb_finger.BMP', '334__F_Right_index_finger.BMP', '334__F_Right_little_finger.BMP', '334__F_Right_middle_finger.BMP', '334__F_Right_ring_finger.BMP', '334__F_Right_thumb_finger.BMP', '335__M_Left_index_finger.BMP', '335__M_Left_little_finger.BMP', '335__M_Left_middle_finger.BMP', '335__M_Left_ring_finger.BMP', '335__M_Left_thumb_finger.BMP', '335__M_Right_index_finger.BMP', '335__M_Right_little_finger.BMP', '335__M_Right_middle_finger.BMP', '335__M_Right_ring_finger.BMP', '335__M_Right_thumb_finger.BMP', '336__M_Left_index_finger.BMP', '336__M_Left_little_finger.BMP', '336__M_Left_middle_finger.BMP', '336__M_Left_ring_finger.BMP', '336__M_Left_thumb_finger.BMP', '336__M_Right_index_finger.BMP', '336__M_Right_little_finger.BMP', '336__M_Right_middle_finger.BMP', '336__M_Right_ring_finger.BMP', '336__M_Right_thumb_finger.BMP', '337__F_Left_index_finger.BMP', '337__F_Left_little_finger.BMP', '337__F_Left_middle_finger.BMP', '337__F_Left_ring_finger.BMP', '337__F_Left_thumb_finger.BMP', '337__F_Right_index_finger.BMP', '337__F_Right_little_finger.BMP', '337__F_Right_middle_finger.BMP', '337__F_Right_ring_finger.BMP', '337__F_Right_thumb_finger.BMP', '338__M_Left_index_finger.BMP', '338__M_Left_little_finger.BMP', '338__M_Left_middle_finger.BMP', '338__M_Left_ring_finger.BMP', '338__M_Left_thumb_finger.BMP', '338__M_Right_index_finger.BMP', '338__M_Right_little_finger.BMP', '338__M_Right_middle_finger.BMP', '338__M_Right_ring_finger.BMP', '338__M_Right_thumb_finger.BMP', '339__M_Left_index_finger.BMP', '339__M_Left_little_finger.BMP', '339__M_Left_middle_finger.BMP', '339__M_Left_ring_finger.BMP', '339__M_Left_thumb_finger.BMP', '339__M_Right_index_finger.BMP', '339__M_Right_little_finger.BMP', '339__M_Right_middle_finger.BMP', '339__M_Right_ring_finger.BMP', '339__M_Right_thumb_finger.BMP', '33__M_Left_index_finger.BMP', '33__M_Left_little_finger.BMP', '33__M_Left_middle_finger.BMP', '33__M_Left_ring_finger.BMP', '33__M_Left_thumb_finger.BMP', '33__M_Right_index_finger.BMP', '33__M_Right_little_finger.BMP', '33__M_Right_middle_finger.BMP', '33__M_Right_ring_finger.BMP', '33__M_Right_thumb_finger.BMP', '340__M_Left_index_finger.BMP', '340__M_Left_little_finger.BMP', '340__M_Left_middle_finger.BMP', '340__M_Left_ring_finger.BMP', '340__M_Left_thumb_finger.BMP', '340__M_Right_index_finger.BMP', '340__M_Right_little_finger.BMP', '340__M_Right_middle_finger.BMP', '340__M_Right_ring_finger.BMP', '340__M_Right_thumb_finger.BMP', '341__M_Left_index_finger.BMP', '341__M_Left_little_finger.BMP', '341__M_Left_middle_finger.BMP', '341__M_Left_ring_finger.BMP', '341__M_Left_thumb_finger.BMP', '341__M_Right_index_finger.BMP', '341__M_Right_little_finger.BMP', '341__M_Right_middle_finger.BMP', '341__M_Right_ring_finger.BMP', '341__M_Right_thumb_finger.BMP', '342__M_Left_index_finger.BMP', '342__M_Left_little_finger.BMP', '342__M_Left_middle_finger.BMP', '342__M_Left_ring_finger.BMP', '342__M_Left_thumb_finger.BMP', '342__M_Right_index_finger.BMP', '342__M_Right_little_finger.BMP', '342__M_Right_middle_finger.BMP', '342__M_Right_ring_finger.BMP', '342__M_Right_thumb_finger.BMP', '343__M_Left_index_finger.BMP', '343__M_Left_little_finger.BMP', '343__M_Left_middle_finger.BMP', '343__M_Left_ring_finger.BMP', '343__M_Left_thumb_finger.BMP', '343__M_Right_index_finger.BMP', '343__M_Right_little_finger.BMP', '343__M_Right_middle_finger.BMP', '343__M_Right_ring_finger.BMP', '343__M_Right_thumb_finger.BMP', '344__M_Left_index_finger.BMP', '344__M_Left_little_finger.BMP', '344__M_Left_middle_finger.BMP', '344__M_Left_ring_finger.BMP', '344__M_Left_thumb_finger.BMP', '344__M_Right_index_finger.BMP', '344__M_Right_little_finger.BMP', '344__M_Right_middle_finger.BMP', '344__M_Right_ring_finger.BMP', '344__M_Right_thumb_finger.BMP', '345__M_Left_index_finger.BMP', '345__M_Left_little_finger.BMP', '345__M_Left_middle_finger.BMP', '345__M_Left_ring_finger.BMP', '345__M_Left_thumb_finger.BMP', '345__M_Right_index_finger.BMP', '345__M_Right_little_finger.BMP', '345__M_Right_middle_finger.BMP', '345__M_Right_ring_finger.BMP', '345__M_Right_thumb_finger.BMP', '346__M_Left_index_finger.BMP', '346__M_Left_little_finger.BMP', '346__M_Left_middle_finger.BMP', '346__M_Left_ring_finger.BMP', '346__M_Left_thumb_finger.BMP', '346__M_Right_index_finger.BMP', '346__M_Right_little_finger.BMP', '346__M_Right_middle_finger.BMP', '346__M_Right_ring_finger.BMP', '346__M_Right_thumb_finger.BMP', '347__M_Left_index_finger.BMP', '347__M_Left_little_finger.BMP', '347__M_Left_middle_finger.BMP', '347__M_Left_ring_finger.BMP', '347__M_Left_thumb_finger.BMP', '347__M_Right_index_finger.BMP', '347__M_Right_little_finger.BMP', '347__M_Right_middle_finger.BMP', '347__M_Right_ring_finger.BMP', '347__M_Right_thumb_finger.BMP', '348__F_Left_index_finger.BMP', '348__F_Left_little_finger.BMP', '348__F_Left_middle_finger.BMP', '348__F_Left_ring_finger.BMP', '348__F_Left_thumb_finger.BMP', '348__F_Right_index_finger.BMP', '348__F_Right_little_finger.BMP', '348__F_Right_middle_finger.BMP', '348__F_Right_ring_finger.BMP', '348__F_Right_thumb_finger.BMP', '349__M_Left_index_finger.BMP', '349__M_Left_little_finger.BMP', '349__M_Left_middle_finger.BMP', '349__M_Left_ring_finger.BMP', '349__M_Left_thumb_finger.BMP', '349__M_Right_index_finger.BMP', '349__M_Right_little_finger.BMP', '349__M_Right_middle_finger.BMP', '349__M_Right_ring_finger.BMP', '349__M_Right_thumb_finger.BMP', '34__M_Left_index_finger.BMP', '34__M_Left_little_finger.BMP', '34__M_Left_middle_finger.BMP', '34__M_Left_ring_finger.BMP', '34__M_Left_thumb_finger.BMP', '34__M_Right_index_finger.BMP', '34__M_Right_little_finger.BMP', '34__M_Right_middle_finger.BMP', '34__M_Right_ring_finger.BMP', '34__M_Right_thumb_finger.BMP', '350__M_Left_index_finger.BMP', '350__M_Left_little_finger.BMP', '350__M_Left_middle_finger.BMP', '350__M_Left_ring_finger.BMP', '350__M_Left_thumb_finger.BMP', '350__M_Right_index_finger.BMP', '350__M_Right_little_finger.BMP', '350__M_Right_middle_finger.BMP', '350__M_Right_ring_finger.BMP', '350__M_Right_thumb_finger.BMP', '351__M_Left_index_finger.BMP', '351__M_Left_little_finger.BMP', '351__M_Left_middle_finger.BMP', '351__M_Left_ring_finger.BMP', '351__M_Left_thumb_finger.BMP', '351__M_Right_index_finger.BMP', '351__M_Right_little_finger.BMP', '351__M_Right_middle_finger.BMP', '351__M_Right_ring_finger.BMP', '351__M_Right_thumb_finger.BMP', '352__M_Left_index_finger.BMP', '352__M_Left_little_finger.BMP', '352__M_Left_middle_finger.BMP', '352__M_Left_ring_finger.BMP', '352__M_Left_thumb_finger.BMP', '352__M_Right_index_finger.BMP', '352__M_Right_little_finger.BMP', '352__M_Right_middle_finger.BMP', '352__M_Right_ring_finger.BMP', '352__M_Right_thumb_finger.BMP', '353__M_Left_index_finger.BMP', '353__M_Left_little_finger.BMP', '353__M_Left_middle_finger.BMP', '353__M_Left_ring_finger.BMP', '353__M_Left_thumb_finger.BMP', '353__M_Right_index_finger.BMP', '353__M_Right_little_finger.BMP', '353__M_Right_middle_finger.BMP', '353__M_Right_ring_finger.BMP', '353__M_Right_thumb_finger.BMP', '354__M_Left_index_finger.BMP', '354__M_Left_little_finger.BMP', '354__M_Left_middle_finger.BMP', '354__M_Left_ring_finger.BMP', '354__M_Left_thumb_finger.BMP', '354__M_Right_index_finger.BMP', '354__M_Right_little_finger.BMP', '354__M_Right_middle_finger.BMP', '354__M_Right_ring_finger.BMP', '354__M_Right_thumb_finger.BMP', '355__M_Left_index_finger.BMP', '355__M_Left_little_finger.BMP', '355__M_Left_middle_finger.BMP', '355__M_Left_ring_finger.BMP', '355__M_Left_thumb_finger.BMP', '355__M_Right_index_finger.BMP', '355__M_Right_little_finger.BMP', '355__M_Right_middle_finger.BMP', '355__M_Right_ring_finger.BMP', '355__M_Right_thumb_finger.BMP', '356__M_Left_index_finger.BMP', '356__M_Left_little_finger.BMP', '356__M_Left_middle_finger.BMP', '356__M_Left_ring_finger.BMP', '356__M_Left_thumb_finger.BMP', '356__M_Right_index_finger.BMP', '356__M_Right_little_finger.BMP', '356__M_Right_middle_finger.BMP', '356__M_Right_ring_finger.BMP', '356__M_Right_thumb_finger.BMP', '357__M_Left_index_finger.BMP', '357__M_Left_little_finger.BMP', '357__M_Left_middle_finger.BMP', '357__M_Left_ring_finger.BMP', '357__M_Left_thumb_finger.BMP', '357__M_Right_index_finger.BMP', '357__M_Right_little_finger.BMP', '357__M_Right_middle_finger.BMP', '357__M_Right_ring_finger.BMP', '357__M_Right_thumb_finger.BMP', '358__M_Left_index_finger.BMP', '358__M_Left_little_finger.BMP', '358__M_Left_middle_finger.BMP', '358__M_Left_ring_finger.BMP', '358__M_Left_thumb_finger.BMP', '358__M_Right_index_finger.BMP', '358__M_Right_little_finger.BMP', '358__M_Right_middle_finger.BMP', '358__M_Right_ring_finger.BMP', '358__M_Right_thumb_finger.BMP', '359__M_Left_index_finger.BMP', '359__M_Left_little_finger.BMP', '359__M_Left_middle_finger.BMP', '359__M_Left_ring_finger.BMP', '359__M_Left_thumb_finger.BMP', '359__M_Right_index_finger.BMP', '359__M_Right_little_finger.BMP', '359__M_Right_middle_finger.BMP', '359__M_Right_ring_finger.BMP', '359__M_Right_thumb_finger.BMP', '35__M_Left_index_finger.BMP', '35__M_Left_little_finger.BMP', '35__M_Left_middle_finger.BMP', '35__M_Left_ring_finger.BMP', '35__M_Left_thumb_finger.BMP', '35__M_Right_index_finger.BMP', '35__M_Right_little_finger.BMP', '35__M_Right_middle_finger.BMP', '35__M_Right_ring_finger.BMP', '35__M_Right_thumb_finger.BMP', '360__M_Left_index_finger.BMP', '360__M_Left_little_finger.BMP', '360__M_Left_middle_finger.BMP', '360__M_Left_ring_finger.BMP', '360__M_Left_thumb_finger.BMP', '360__M_Right_index_finger.BMP', '360__M_Right_little_finger.BMP', '360__M_Right_middle_finger.BMP', '360__M_Right_ring_finger.BMP', '360__M_Right_thumb_finger.BMP', '361__M_Left_index_finger.BMP', '361__M_Left_little_finger.BMP', '361__M_Left_middle_finger.BMP', '361__M_Left_ring_finger.BMP', '361__M_Left_thumb_finger.BMP', '361__M_Right_index_finger.BMP', '361__M_Right_little_finger.BMP', '361__M_Right_middle_finger.BMP', '361__M_Right_ring_finger.BMP', '361__M_Right_thumb_finger.BMP', '362__M_Left_index_finger.BMP', '362__M_Left_little_finger.BMP', '362__M_Left_middle_finger.BMP', '362__M_Left_ring_finger.BMP', '362__M_Left_thumb_finger.BMP', '362__M_Right_index_finger.BMP', '362__M_Right_little_finger.BMP', '362__M_Right_middle_finger.BMP', '362__M_Right_ring_finger.BMP', '362__M_Right_thumb_finger.BMP', '363__M_Left_index_finger.BMP', '363__M_Left_little_finger.BMP', '363__M_Left_middle_finger.BMP', '363__M_Left_ring_finger.BMP', '363__M_Left_thumb_finger.BMP', '363__M_Right_index_finger.BMP', '363__M_Right_little_finger.BMP', '363__M_Right_middle_finger.BMP', '363__M_Right_ring_finger.BMP', '363__M_Right_thumb_finger.BMP', '364__M_Left_index_finger.BMP', '364__M_Left_little_finger.BMP', '364__M_Left_middle_finger.BMP', '364__M_Left_ring_finger.BMP', '364__M_Left_thumb_finger.BMP', '364__M_Right_index_finger.BMP', '364__M_Right_little_finger.BMP', '364__M_Right_middle_finger.BMP', '364__M_Right_ring_finger.BMP', '364__M_Right_thumb_finger.BMP', '365__M_Left_index_finger.BMP', '365__M_Left_little_finger.BMP', '365__M_Left_middle_finger.BMP', '365__M_Left_ring_finger.BMP', '365__M_Left_thumb_finger.BMP', '365__M_Right_index_finger.BMP', '365__M_Right_little_finger.BMP', '365__M_Right_middle_finger.BMP', '365__M_Right_ring_finger.BMP', '365__M_Right_thumb_finger.BMP', '366__M_Left_index_finger.BMP', '366__M_Left_little_finger.BMP', '366__M_Left_middle_finger.BMP', '366__M_Left_ring_finger.BMP', '366__M_Left_thumb_finger.BMP', '366__M_Right_index_finger.BMP', '366__M_Right_little_finger.BMP', '366__M_Right_middle_finger.BMP', '366__M_Right_ring_finger.BMP', '366__M_Right_thumb_finger.BMP', '367__M_Left_index_finger.BMP', '367__M_Left_little_finger.BMP', '367__M_Left_middle_finger.BMP', '367__M_Left_ring_finger.BMP', '367__M_Left_thumb_finger.BMP', '367__M_Right_index_finger.BMP', '367__M_Right_little_finger.BMP', '367__M_Right_middle_finger.BMP', '367__M_Right_ring_finger.BMP', '367__M_Right_thumb_finger.BMP', '368__M_Left_index_finger.BMP', '368__M_Left_little_finger.BMP', '368__M_Left_middle_finger.BMP', '368__M_Left_ring_finger.BMP', '368__M_Left_thumb_finger.BMP', '368__M_Right_index_finger.BMP', '368__M_Right_little_finger.BMP', '368__M_Right_middle_finger.BMP', '368__M_Right_ring_finger.BMP', '368__M_Right_thumb_finger.BMP', '369__F_Left_index_finger.BMP', '369__F_Left_little_finger.BMP', '369__F_Left_middle_finger.BMP', '369__F_Left_ring_finger.BMP', '369__F_Left_thumb_finger.BMP', '369__F_Right_index_finger.BMP', '369__F_Right_little_finger.BMP', '369__F_Right_middle_finger.BMP', '369__F_Right_ring_finger.BMP', '369__F_Right_thumb_finger.BMP', '36__M_Left_index_finger.BMP', '36__M_Left_little_finger.BMP', '36__M_Left_middle_finger.BMP', '36__M_Left_ring_finger.BMP', '36__M_Left_thumb_finger.BMP', '36__M_Right_index_finger.BMP', '36__M_Right_little_finger.BMP', '36__M_Right_middle_finger.BMP', '36__M_Right_ring_finger.BMP', '36__M_Right_thumb_finger.BMP', '370__M_Left_index_finger.BMP', '370__M_Left_little_finger.BMP', '370__M_Left_middle_finger.BMP', '370__M_Left_ring_finger.BMP', '370__M_Left_thumb_finger.BMP', '370__M_Right_index_finger.BMP', '370__M_Right_little_finger.BMP', '370__M_Right_middle_finger.BMP', '370__M_Right_ring_finger.BMP', '370__M_Right_thumb_finger.BMP', '371__M_Left_index_finger.BMP', '371__M_Left_little_finger.BMP', '371__M_Left_middle_finger.BMP', '371__M_Left_ring_finger.BMP', '371__M_Left_thumb_finger.BMP', '371__M_Right_index_finger.BMP', '371__M_Right_little_finger.BMP', '371__M_Right_middle_finger.BMP', '371__M_Right_ring_finger.BMP', '371__M_Right_thumb_finger.BMP', '372__M_Left_index_finger.BMP', '372__M_Left_little_finger.BMP', '372__M_Left_middle_finger.BMP', '372__M_Left_ring_finger.BMP', '372__M_Left_thumb_finger.BMP', '372__M_Right_index_finger.BMP', '372__M_Right_little_finger.BMP', '372__M_Right_middle_finger.BMP', '372__M_Right_ring_finger.BMP', '372__M_Right_thumb_finger.BMP', '373__M_Left_index_finger.BMP', '373__M_Left_little_finger.BMP', '373__M_Left_middle_finger.BMP', '373__M_Left_ring_finger.BMP', '373__M_Left_thumb_finger.BMP', '373__M_Right_index_finger.BMP', '373__M_Right_little_finger.BMP', '373__M_Right_middle_finger.BMP', '373__M_Right_ring_finger.BMP', '373__M_Right_thumb_finger.BMP', '374__M_Left_index_finger.BMP', '374__M_Left_little_finger.BMP', '374__M_Left_middle_finger.BMP', '374__M_Left_ring_finger.BMP', '374__M_Left_thumb_finger.BMP', '374__M_Right_index_finger.BMP', '374__M_Right_little_finger.BMP', '374__M_Right_middle_finger.BMP', '374__M_Right_ring_finger.BMP', '374__M_Right_thumb_finger.BMP', '375__M_Left_index_finger.BMP', '375__M_Left_little_finger.BMP', '375__M_Left_middle_finger.BMP', '375__M_Left_ring_finger.BMP', '375__M_Left_thumb_finger.BMP', '375__M_Right_index_finger.BMP', '375__M_Right_little_finger.BMP', '375__M_Right_middle_finger.BMP', '375__M_Right_ring_finger.BMP', '375__M_Right_thumb_finger.BMP', '376__F_Left_index_finger.BMP', '376__F_Left_little_finger.BMP', '376__F_Left_middle_finger.BMP', '376__F_Left_ring_finger.BMP', '376__F_Left_thumb_finger.BMP', '376__F_Right_index_finger.BMP', '376__F_Right_little_finger.BMP', '376__F_Right_middle_finger.BMP', '376__F_Right_ring_finger.BMP', '376__F_Right_thumb_finger.BMP', '377__M_Left_index_finger.BMP', '377__M_Left_little_finger.BMP', '377__M_Left_middle_finger.BMP', '377__M_Left_ring_finger.BMP', '377__M_Left_thumb_finger.BMP', '377__M_Right_index_finger.BMP', '377__M_Right_little_finger.BMP', '377__M_Right_middle_finger.BMP', '377__M_Right_ring_finger.BMP', '377__M_Right_thumb_finger.BMP', '378__F_Left_index_finger.BMP', '378__F_Left_little_finger.BMP', '378__F_Left_middle_finger.BMP', '378__F_Left_ring_finger.BMP', '378__F_Left_thumb_finger.BMP', '378__F_Right_index_finger.BMP', '378__F_Right_little_finger.BMP', '378__F_Right_middle_finger.BMP', '378__F_Right_ring_finger.BMP', '378__F_Right_thumb_finger.BMP', '379__F_Left_index_finger.BMP', '379__F_Left_little_finger.BMP', '379__F_Left_middle_finger.BMP', '379__F_Left_ring_finger.BMP', '379__F_Left_thumb_finger.BMP', '379__F_Right_index_finger.BMP', '379__F_Right_little_finger.BMP', '379__F_Right_middle_finger.BMP', '379__F_Right_ring_finger.BMP', '379__F_Right_thumb_finger.BMP', '37__M_Left_index_finger.BMP', '37__M_Left_little_finger.BMP', '37__M_Left_middle_finger.BMP', '37__M_Left_ring_finger.BMP', '37__M_Left_thumb_finger.BMP', '37__M_Right_index_finger.BMP', '37__M_Right_little_finger.BMP', '37__M_Right_middle_finger.BMP', '37__M_Right_ring_finger.BMP', '37__M_Right_thumb_finger.BMP', '380__M_Left_index_finger.BMP', '380__M_Left_little_finger.BMP', '380__M_Left_middle_finger.BMP', '380__M_Left_ring_finger.BMP', '380__M_Left_thumb_finger.BMP', '380__M_Right_index_finger.BMP', '380__M_Right_little_finger.BMP', '380__M_Right_middle_finger.BMP', '380__M_Right_ring_finger.BMP', '380__M_Right_thumb_finger.BMP', '381__M_Left_index_finger.BMP', '381__M_Left_little_finger.BMP', '381__M_Left_middle_finger.BMP', '381__M_Left_ring_finger.BMP', '381__M_Left_thumb_finger.BMP', '381__M_Right_index_finger.BMP', '381__M_Right_little_finger.BMP', '381__M_Right_middle_finger.BMP', '381__M_Right_ring_finger.BMP', '381__M_Right_thumb_finger.BMP', '382__M_Left_index_finger.BMP', '382__M_Left_little_finger.BMP', '382__M_Left_middle_finger.BMP', '382__M_Left_ring_finger.BMP', '382__M_Left_thumb_finger.BMP', '382__M_Right_index_finger.BMP', '382__M_Right_little_finger.BMP', '382__M_Right_middle_finger.BMP', '382__M_Right_ring_finger.BMP', '382__M_Right_thumb_finger.BMP', '383__M_Left_index_finger.BMP', '383__M_Left_little_finger.BMP', '383__M_Left_middle_finger.BMP', '383__M_Left_ring_finger.BMP', '383__M_Left_thumb_finger.BMP', '383__M_Right_index_finger.BMP', '383__M_Right_little_finger.BMP', '383__M_Right_middle_finger.BMP', '383__M_Right_ring_finger.BMP', '383__M_Right_thumb_finger.BMP', '384__F_Left_index_finger.BMP', '384__F_Left_little_finger.BMP', '384__F_Left_middle_finger.BMP', '384__F_Left_ring_finger.BMP', '384__F_Left_thumb_finger.BMP', '384__F_Right_index_finger.BMP', '384__F_Right_little_finger.BMP', '384__F_Right_middle_finger.BMP', '384__F_Right_ring_finger.BMP', '384__F_Right_thumb_finger.BMP', '385__M_Left_index_finger.BMP', '385__M_Left_little_finger.BMP', '385__M_Left_middle_finger.BMP', '385__M_Left_ring_finger.BMP', '385__M_Left_thumb_finger.BMP', '385__M_Right_index_finger.BMP', '385__M_Right_little_finger.BMP', '385__M_Right_middle_finger.BMP', '385__M_Right_ring_finger.BMP', '385__M_Right_thumb_finger.BMP', '386__M_Left_index_finger.BMP', '386__M_Left_little_finger.BMP', '386__M_Left_middle_finger.BMP', '386__M_Left_ring_finger.BMP', '386__M_Left_thumb_finger.BMP', '386__M_Right_index_finger.BMP', '386__M_Right_little_finger.BMP', '386__M_Right_middle_finger.BMP', '386__M_Right_ring_finger.BMP', '386__M_Right_thumb_finger.BMP', '387__F_Left_index_finger.BMP', '387__F_Left_little_finger.BMP', '387__F_Left_middle_finger.BMP', '387__F_Left_ring_finger.BMP', '387__F_Left_thumb_finger.BMP', '387__F_Right_index_finger.BMP', '387__F_Right_little_finger.BMP', '387__F_Right_middle_finger.BMP', '387__F_Right_ring_finger.BMP', '387__F_Right_thumb_finger.BMP', '388__F_Left_index_finger.BMP', '388__F_Left_little_finger.BMP', '388__F_Left_middle_finger.BMP', '388__F_Left_ring_finger.BMP', '388__F_Left_thumb_finger.BMP', '388__F_Right_index_finger.BMP', '388__F_Right_little_finger.BMP', '388__F_Right_middle_finger.BMP', '388__F_Right_ring_finger.BMP', '388__F_Right_thumb_finger.BMP', '389__F_Left_index_finger.BMP', '389__F_Left_little_finger.BMP', '389__F_Left_middle_finger.BMP', '389__F_Left_ring_finger.BMP', '389__F_Left_thumb_finger.BMP', '389__F_Right_index_finger.BMP', '389__F_Right_little_finger.BMP', '389__F_Right_middle_finger.BMP', '389__F_Right_ring_finger.BMP', '389__F_Right_thumb_finger.BMP', '38__M_Left_index_finger.BMP', '38__M_Left_little_finger.BMP', '38__M_Left_middle_finger.BMP', '38__M_Left_ring_finger.BMP', '38__M_Left_thumb_finger.BMP', '38__M_Right_index_finger.BMP', '38__M_Right_little_finger.BMP', '38__M_Right_middle_finger.BMP', '38__M_Right_ring_finger.BMP', '38__M_Right_thumb_finger.BMP', '390__F_Left_index_finger.BMP', '390__F_Left_little_finger.BMP', '390__F_Left_middle_finger.BMP', '390__F_Left_ring_finger.BMP', '390__F_Left_thumb_finger.BMP', '390__F_Right_index_finger.BMP', '390__F_Right_little_finger.BMP', '390__F_Right_middle_finger.BMP', '390__F_Right_ring_finger.BMP', '390__F_Right_thumb_finger.BMP', '391__M_Left_index_finger.BMP', '391__M_Left_little_finger.BMP', '391__M_Left_middle_finger.BMP', '391__M_Left_ring_finger.BMP', '391__M_Left_thumb_finger.BMP', '391__M_Right_index_finger.BMP', '391__M_Right_little_finger.BMP', '391__M_Right_middle_finger.BMP', '391__M_Right_ring_finger.BMP', '391__M_Right_thumb_finger.BMP', '392__M_Left_index_finger.BMP', '392__M_Left_little_finger.BMP', '392__M_Left_middle_finger.BMP', '392__M_Left_ring_finger.BMP', '392__M_Left_thumb_finger.BMP', '392__M_Right_index_finger.BMP', '392__M_Right_little_finger.BMP', '392__M_Right_middle_finger.BMP', '392__M_Right_ring_finger.BMP', '392__M_Right_thumb_finger.BMP', '393__M_Left_index_finger.BMP', '393__M_Left_little_finger.BMP', '393__M_Left_middle_finger.BMP', '393__M_Left_ring_finger.BMP', '393__M_Left_thumb_finger.BMP', '393__M_Right_index_finger.BMP', '393__M_Right_little_finger.BMP', '393__M_Right_middle_finger.BMP', '393__M_Right_ring_finger.BMP', '393__M_Right_thumb_finger.BMP', '394__M_Left_index_finger.BMP', '394__M_Left_little_finger.BMP', '394__M_Left_middle_finger.BMP', '394__M_Left_ring_finger.BMP', '394__M_Left_thumb_finger.BMP', '394__M_Right_index_finger.BMP', '394__M_Right_little_finger.BMP', '394__M_Right_middle_finger.BMP', '394__M_Right_ring_finger.BMP', '394__M_Right_thumb_finger.BMP', '395__M_Left_index_finger.BMP', '395__M_Left_little_finger.BMP', '395__M_Left_middle_finger.BMP', '395__M_Left_ring_finger.BMP', '395__M_Left_thumb_finger.BMP', '395__M_Right_index_finger.BMP', '395__M_Right_little_finger.BMP', '395__M_Right_middle_finger.BMP', '395__M_Right_ring_finger.BMP', '395__M_Right_thumb_finger.BMP', '396__M_Left_index_finger.BMP', '396__M_Left_little_finger.BMP', '396__M_Left_middle_finger.BMP', '396__M_Left_ring_finger.BMP', '396__M_Left_thumb_finger.BMP', '396__M_Right_index_finger.BMP', '396__M_Right_little_finger.BMP', '396__M_Right_middle_finger.BMP', '396__M_Right_ring_finger.BMP', '396__M_Right_thumb_finger.BMP', '397__M_Left_index_finger.BMP', '397__M_Left_little_finger.BMP', '397__M_Left_middle_finger.BMP', '397__M_Left_ring_finger.BMP', '397__M_Left_thumb_finger.BMP', '397__M_Right_index_finger.BMP', '397__M_Right_little_finger.BMP', '397__M_Right_middle_finger.BMP', '397__M_Right_ring_finger.BMP', '397__M_Right_thumb_finger.BMP', '398__M_Left_index_finger.BMP', '398__M_Left_little_finger.BMP', '398__M_Left_middle_finger.BMP', '398__M_Left_ring_finger.BMP', '398__M_Left_thumb_finger.BMP', '398__M_Right_index_finger.BMP', '398__M_Right_little_finger.BMP', '398__M_Right_middle_finger.BMP', '398__M_Right_ring_finger.BMP', '398__M_Right_thumb_finger.BMP', '399__M_Left_index_finger.BMP', '399__M_Left_little_finger.BMP', '399__M_Left_middle_finger.BMP', '399__M_Left_ring_finger.BMP', '399__M_Left_thumb_finger.BMP', '399__M_Right_index_finger.BMP', '399__M_Right_little_finger.BMP', '399__M_Right_middle_finger.BMP', '399__M_Right_ring_finger.BMP', '399__M_Right_thumb_finger.BMP', '39__M_Left_index_finger.BMP', '39__M_Left_little_finger.BMP', '39__M_Left_middle_finger.BMP', '39__M_Left_ring_finger.BMP', '39__M_Left_thumb_finger.BMP', '39__M_Right_index_finger.BMP', '39__M_Right_little_finger.BMP', '39__M_Right_middle_finger.BMP', '39__M_Right_ring_finger.BMP', '39__M_Right_thumb_finger.BMP', '3__M_Left_index_finger.BMP', '3__M_Left_little_finger.BMP', '3__M_Left_middle_finger.BMP', '3__M_Left_ring_finger.BMP', '3__M_Left_thumb_finger.BMP', '3__M_Right_index_finger.BMP', '3__M_Right_little_finger.BMP', '3__M_Right_middle_finger.BMP', '3__M_Right_ring_finger.BMP', '3__M_Right_thumb_finger.BMP', '400__M_Left_index_finger.BMP', '400__M_Left_little_finger.BMP', '400__M_Left_middle_finger.BMP', '400__M_Left_ring_finger.BMP', '400__M_Left_thumb_finger.BMP', '400__M_Right_index_finger.BMP', '400__M_Right_little_finger.BMP', '400__M_Right_middle_finger.BMP', '400__M_Right_ring_finger.BMP', '400__M_Right_thumb_finger.BMP', '401__M_Left_index_finger.BMP', '401__M_Left_little_finger.BMP', '401__M_Left_middle_finger.BMP', '401__M_Left_ring_finger.BMP', '401__M_Left_thumb_finger.BMP', '401__M_Right_index_finger.BMP', '401__M_Right_little_finger.BMP', '401__M_Right_middle_finger.BMP', '401__M_Right_ring_finger.BMP', '401__M_Right_thumb_finger.BMP', '402__M_Left_index_finger.BMP', '402__M_Left_little_finger.BMP', '402__M_Left_middle_finger.BMP', '402__M_Left_ring_finger.BMP', '402__M_Left_thumb_finger.BMP', '402__M_Right_index_finger.BMP', '402__M_Right_little_finger.BMP', '402__M_Right_middle_finger.BMP', '402__M_Right_ring_finger.BMP', '402__M_Right_thumb_finger.BMP', '403__M_Left_index_finger.BMP', '403__M_Left_little_finger.BMP', '403__M_Left_middle_finger.BMP', '403__M_Left_ring_finger.BMP', '403__M_Left_thumb_finger.BMP', '403__M_Right_index_finger.BMP', '403__M_Right_little_finger.BMP', '403__M_Right_middle_finger.BMP', '403__M_Right_ring_finger.BMP', '403__M_Right_thumb_finger.BMP', '404__M_Left_index_finger.BMP', '404__M_Left_little_finger.BMP', '404__M_Left_middle_finger.BMP', '404__M_Left_ring_finger.BMP', '404__M_Left_thumb_finger.BMP', '404__M_Right_index_finger.BMP', '404__M_Right_little_finger.BMP', '404__M_Right_middle_finger.BMP', '404__M_Right_ring_finger.BMP', '404__M_Right_thumb_finger.BMP', '405__M_Left_index_finger.BMP', '405__M_Left_little_finger.BMP', '405__M_Left_middle_finger.BMP', '405__M_Left_ring_finger.BMP', '405__M_Left_thumb_finger.BMP', '405__M_Right_index_finger.BMP', '405__M_Right_little_finger.BMP', '405__M_Right_middle_finger.BMP', '405__M_Right_ring_finger.BMP', '405__M_Right_thumb_finger.BMP', '406__M_Left_index_finger.BMP', '406__M_Left_little_finger.BMP', '406__M_Left_middle_finger.BMP', '406__M_Left_ring_finger.BMP', '406__M_Left_thumb_finger.BMP', '406__M_Right_index_finger.BMP', '406__M_Right_little_finger.BMP', '406__M_Right_middle_finger.BMP', '406__M_Right_ring_finger.BMP', '406__M_Right_thumb_finger.BMP', '407__M_Left_index_finger.BMP', '407__M_Left_little_finger.BMP', '407__M_Left_middle_finger.BMP', '407__M_Left_ring_finger.BMP', '407__M_Left_thumb_finger.BMP', '407__M_Right_index_finger.BMP', '407__M_Right_little_finger.BMP', '407__M_Right_middle_finger.BMP', '407__M_Right_ring_finger.BMP', '407__M_Right_thumb_finger.BMP', '408__M_Left_index_finger.BMP', '408__M_Left_little_finger.BMP', '408__M_Left_middle_finger.BMP', '408__M_Left_ring_finger.BMP', '408__M_Left_thumb_finger.BMP', '408__M_Right_index_finger.BMP', '408__M_Right_little_finger.BMP', '408__M_Right_middle_finger.BMP', '408__M_Right_ring_finger.BMP', '408__M_Right_thumb_finger.BMP', '409__M_Left_index_finger.BMP', '409__M_Left_little_finger.BMP', '409__M_Left_middle_finger.BMP', '409__M_Left_ring_finger.BMP', '409__M_Left_thumb_finger.BMP', '409__M_Right_index_finger.BMP', '409__M_Right_little_finger.BMP', '409__M_Right_middle_finger.BMP', '409__M_Right_ring_finger.BMP', '409__M_Right_thumb_finger.BMP', '40__F_Left_index_finger.BMP', '40__F_Left_little_finger.BMP', '40__F_Left_middle_finger.BMP', '40__F_Left_ring_finger.BMP', '40__F_Left_thumb_finger.BMP', '40__F_Right_index_finger.BMP', '40__F_Right_little_finger.BMP', '40__F_Right_middle_finger.BMP', '40__F_Right_ring_finger.BMP', '40__F_Right_thumb_finger.BMP', '410__M_Left_index_finger.BMP', '410__M_Left_little_finger.BMP', '410__M_Left_middle_finger.BMP', '410__M_Left_ring_finger.BMP', '410__M_Left_thumb_finger.BMP', '410__M_Right_index_finger.BMP', '410__M_Right_little_finger.BMP', '410__M_Right_middle_finger.BMP', '410__M_Right_ring_finger.BMP', '410__M_Right_thumb_finger.BMP', '411__M_Left_index_finger.BMP', '411__M_Left_little_finger.BMP', '411__M_Left_middle_finger.BMP', '411__M_Left_ring_finger.BMP', '411__M_Left_thumb_finger.BMP', '411__M_Right_index_finger.BMP', '411__M_Right_little_finger.BMP', '411__M_Right_middle_finger.BMP', '411__M_Right_ring_finger.BMP', '411__M_Right_thumb_finger.BMP', '412__M_Left_index_finger.BMP', '412__M_Left_little_finger.BMP', '412__M_Left_middle_finger.BMP', '412__M_Left_ring_finger.BMP', '412__M_Left_thumb_finger.BMP', '412__M_Right_index_finger.BMP', '412__M_Right_little_finger.BMP', '412__M_Right_middle_finger.BMP', '412__M_Right_ring_finger.BMP', '412__M_Right_thumb_finger.BMP', '413__M_Left_index_finger.BMP', '413__M_Left_little_finger.BMP', '413__M_Left_middle_finger.BMP', '413__M_Left_ring_finger.BMP', '413__M_Left_thumb_finger.BMP', '413__M_Right_index_finger.BMP', '413__M_Right_little_finger.BMP', '413__M_Right_middle_finger.BMP', '413__M_Right_ring_finger.BMP', '413__M_Right_thumb_finger.BMP', '414__M_Left_index_finger.BMP', '414__M_Left_little_finger.BMP', '414__M_Left_middle_finger.BMP', '414__M_Left_ring_finger.BMP', '414__M_Left_thumb_finger.BMP', '414__M_Right_index_finger.BMP', '414__M_Right_little_finger.BMP', '414__M_Right_middle_finger.BMP', '414__M_Right_ring_finger.BMP', '414__M_Right_thumb_finger.BMP', '415__M_Left_index_finger.BMP', '415__M_Left_little_finger.BMP', '415__M_Left_middle_finger.BMP', '415__M_Left_ring_finger.BMP', '415__M_Left_thumb_finger.BMP', '415__M_Right_index_finger.BMP', '415__M_Right_little_finger.BMP', '415__M_Right_middle_finger.BMP', '415__M_Right_ring_finger.BMP', '415__M_Right_thumb_finger.BMP', '416__M_Left_index_finger.BMP', '416__M_Left_little_finger.BMP', '416__M_Left_middle_finger.BMP', '416__M_Left_ring_finger.BMP', '416__M_Left_thumb_finger.BMP', '416__M_Right_index_finger.BMP', '416__M_Right_little_finger.BMP', '416__M_Right_middle_finger.BMP', '416__M_Right_ring_finger.BMP', '416__M_Right_thumb_finger.BMP', '417__M_Left_index_finger.BMP', '417__M_Left_little_finger.BMP', '417__M_Left_middle_finger.BMP', '417__M_Left_ring_finger.BMP', '417__M_Left_thumb_finger.BMP', '417__M_Right_index_finger.BMP', '417__M_Right_little_finger.BMP', '417__M_Right_middle_finger.BMP', '417__M_Right_ring_finger.BMP', '417__M_Right_thumb_finger.BMP', '418__F_Left_index_finger.BMP', '418__F_Left_little_finger.BMP', '418__F_Left_middle_finger.BMP', '418__F_Left_ring_finger.BMP', '418__F_Left_thumb_finger.BMP', '418__F_Right_index_finger.BMP', '418__F_Right_little_finger.BMP', '418__F_Right_middle_finger.BMP', '418__F_Right_ring_finger.BMP', '418__F_Right_thumb_finger.BMP', '419__F_Left_index_finger.BMP', '419__F_Left_little_finger.BMP', '419__F_Left_middle_finger.BMP', '419__F_Left_ring_finger.BMP', '419__F_Left_thumb_finger.BMP', '419__F_Right_index_finger.BMP', '419__F_Right_little_finger.BMP', '419__F_Right_middle_finger.BMP', '419__F_Right_ring_finger.BMP', '419__F_Right_thumb_finger.BMP', '41__M_Left_index_finger.BMP', '41__M_Left_little_finger.BMP', '41__M_Left_middle_finger.BMP', '41__M_Left_ring_finger.BMP', '41__M_Left_thumb_finger.BMP', '41__M_Right_index_finger.BMP', '41__M_Right_little_finger.BMP', '41__M_Right_middle_finger.BMP', '41__M_Right_ring_finger.BMP', '41__M_Right_thumb_finger.BMP', '420__M_Left_index_finger.BMP', '420__M_Left_little_finger.BMP', '420__M_Left_middle_finger.BMP', '420__M_Left_ring_finger.BMP', '420__M_Left_thumb_finger.BMP', '420__M_Right_index_finger.BMP', '420__M_Right_little_finger.BMP', '420__M_Right_middle_finger.BMP', '420__M_Right_ring_finger.BMP', '420__M_Right_thumb_finger.BMP', '421__F_Left_index_finger.BMP', '421__F_Left_little_finger.BMP', '421__F_Left_middle_finger.BMP', '421__F_Left_ring_finger.BMP', '421__F_Left_thumb_finger.BMP', '421__F_Right_index_finger.BMP', '421__F_Right_little_finger.BMP', '421__F_Right_middle_finger.BMP', '421__F_Right_ring_finger.BMP', '421__F_Right_thumb_finger.BMP', '422__M_Left_index_finger.BMP', '422__M_Left_little_finger.BMP', '422__M_Left_middle_finger.BMP', '422__M_Left_ring_finger.BMP', '422__M_Left_thumb_finger.BMP', '422__M_Right_index_finger.BMP', '422__M_Right_little_finger.BMP', '422__M_Right_middle_finger.BMP', '422__M_Right_ring_finger.BMP', '422__M_Right_thumb_finger.BMP', '423__M_Left_index_finger.BMP', '423__M_Left_little_finger.BMP', '423__M_Left_middle_finger.BMP', '423__M_Left_ring_finger.BMP', '423__M_Left_thumb_finger.BMP', '423__M_Right_index_finger.BMP', '423__M_Right_little_finger.BMP', '423__M_Right_middle_finger.BMP', '423__M_Right_ring_finger.BMP', '423__M_Right_thumb_finger.BMP', '424__M_Left_index_finger.BMP', '424__M_Left_little_finger.BMP', '424__M_Left_middle_finger.BMP', '424__M_Left_ring_finger.BMP', '424__M_Left_thumb_finger.BMP', '424__M_Right_index_finger.BMP', '424__M_Right_little_finger.BMP', '424__M_Right_middle_finger.BMP', '424__M_Right_ring_finger.BMP', '424__M_Right_thumb_finger.BMP', '425__M_Left_index_finger.BMP', '425__M_Left_little_finger.BMP', '425__M_Left_middle_finger.BMP', '425__M_Left_ring_finger.BMP', '425__M_Left_thumb_finger.BMP', '425__M_Right_index_finger.BMP', '425__M_Right_little_finger.BMP', '425__M_Right_middle_finger.BMP', '425__M_Right_ring_finger.BMP', '425__M_Right_thumb_finger.BMP', '426__M_Left_index_finger.BMP', '426__M_Left_little_finger.BMP', '426__M_Left_middle_finger.BMP', '426__M_Left_ring_finger.BMP', '426__M_Left_thumb_finger.BMP', '426__M_Right_index_finger.BMP', '426__M_Right_little_finger.BMP', '426__M_Right_middle_finger.BMP', '426__M_Right_ring_finger.BMP', '426__M_Right_thumb_finger.BMP', '427__M_Left_index_finger.BMP', '427__M_Left_little_finger.BMP', '427__M_Left_middle_finger.BMP', '427__M_Left_ring_finger.BMP', '427__M_Left_thumb_finger.BMP', '427__M_Right_index_finger.BMP', '427__M_Right_little_finger.BMP', '427__M_Right_middle_finger.BMP', '427__M_Right_ring_finger.BMP', '427__M_Right_thumb_finger.BMP', '428__M_Left_index_finger.BMP', '428__M_Left_little_finger.BMP', '428__M_Left_middle_finger.BMP', '428__M_Left_ring_finger.BMP', '428__M_Left_thumb_finger.BMP', '428__M_Right_index_finger.BMP', '428__M_Right_little_finger.BMP', '428__M_Right_middle_finger.BMP', '428__M_Right_ring_finger.BMP', '428__M_Right_thumb_finger.BMP', '429__M_Left_index_finger.BMP', '429__M_Left_little_finger.BMP', '429__M_Left_middle_finger.BMP', '429__M_Left_ring_finger.BMP', '429__M_Left_thumb_finger.BMP', '429__M_Right_index_finger.BMP', '429__M_Right_little_finger.BMP', '429__M_Right_middle_finger.BMP', '429__M_Right_ring_finger.BMP', '429__M_Right_thumb_finger.BMP', '42__F_Left_index_finger.BMP', '42__F_Left_little_finger.BMP', '42__F_Left_middle_finger.BMP', '42__F_Left_ring_finger.BMP', '42__F_Left_thumb_finger.BMP', '42__F_Right_index_finger.BMP', '42__F_Right_little_finger.BMP', '42__F_Right_middle_finger.BMP', '42__F_Right_ring_finger.BMP', '42__F_Right_thumb_finger.BMP', '430__F_Left_index_finger.BMP', '430__F_Left_little_finger.BMP', '430__F_Left_middle_finger.BMP', '430__F_Left_ring_finger.BMP', '430__F_Left_thumb_finger.BMP', '430__F_Right_index_finger.BMP', '430__F_Right_little_finger.BMP', '430__F_Right_middle_finger.BMP', '430__F_Right_ring_finger.BMP', '430__F_Right_thumb_finger.BMP', '431__M_Left_index_finger.BMP', '431__M_Left_little_finger.BMP', '431__M_Left_middle_finger.BMP', '431__M_Left_ring_finger.BMP', '431__M_Left_thumb_finger.BMP', '431__M_Right_index_finger.BMP', '431__M_Right_little_finger.BMP', '431__M_Right_middle_finger.BMP', '431__M_Right_ring_finger.BMP', '431__M_Right_thumb_finger.BMP', '432__M_Left_index_finger.BMP', '432__M_Left_little_finger.BMP', '432__M_Left_middle_finger.BMP', '432__M_Left_ring_finger.BMP', '432__M_Left_thumb_finger.BMP', '432__M_Right_index_finger.BMP', '432__M_Right_little_finger.BMP', '432__M_Right_middle_finger.BMP', '432__M_Right_ring_finger.BMP', '432__M_Right_thumb_finger.BMP', '433__M_Left_index_finger.BMP', '433__M_Left_little_finger.BMP', '433__M_Left_middle_finger.BMP', '433__M_Left_ring_finger.BMP', '433__M_Left_thumb_finger.BMP', '433__M_Right_index_finger.BMP', '433__M_Right_little_finger.BMP', '433__M_Right_middle_finger.BMP', '433__M_Right_ring_finger.BMP', '433__M_Right_thumb_finger.BMP', '434__M_Left_index_finger.BMP', '434__M_Left_little_finger.BMP', '434__M_Left_middle_finger.BMP', '434__M_Left_ring_finger.BMP', '434__M_Left_thumb_finger.BMP', '434__M_Right_index_finger.BMP', '434__M_Right_little_finger.BMP', '434__M_Right_middle_finger.BMP', '434__M_Right_ring_finger.BMP', '434__M_Right_thumb_finger.BMP', '435__F_Left_index_finger.BMP', '435__F_Left_little_finger.BMP', '435__F_Left_middle_finger.BMP', '435__F_Left_ring_finger.BMP', '435__F_Left_thumb_finger.BMP', '435__F_Right_index_finger.BMP', '435__F_Right_little_finger.BMP', '435__F_Right_middle_finger.BMP', '435__F_Right_ring_finger.BMP', '435__F_Right_thumb_finger.BMP', '436__M_Left_index_finger.BMP', '436__M_Left_little_finger.BMP', '436__M_Left_middle_finger.BMP', '436__M_Left_ring_finger.BMP', '436__M_Left_thumb_finger.BMP', '436__M_Right_index_finger.BMP', '436__M_Right_little_finger.BMP', '436__M_Right_middle_finger.BMP', '436__M_Right_ring_finger.BMP', '436__M_Right_thumb_finger.BMP', '437__M_Left_index_finger.BMP', '437__M_Left_little_finger.BMP', '437__M_Left_middle_finger.BMP', '437__M_Left_ring_finger.BMP', '437__M_Left_thumb_finger.BMP', '437__M_Right_index_finger.BMP', '437__M_Right_little_finger.BMP', '437__M_Right_middle_finger.BMP', '437__M_Right_ring_finger.BMP', '437__M_Right_thumb_finger.BMP', '438__M_Left_index_finger.BMP', '438__M_Left_little_finger.BMP', '438__M_Left_middle_finger.BMP', '438__M_Left_ring_finger.BMP', '438__M_Left_thumb_finger.BMP', '438__M_Right_index_finger.BMP', '438__M_Right_little_finger.BMP', '438__M_Right_middle_finger.BMP', '438__M_Right_ring_finger.BMP', '438__M_Right_thumb_finger.BMP', '439__M_Left_index_finger.BMP', '439__M_Left_little_finger.BMP', '439__M_Left_middle_finger.BMP', '439__M_Left_ring_finger.BMP', '439__M_Left_thumb_finger.BMP', '439__M_Right_index_finger.BMP', '439__M_Right_little_finger.BMP', '439__M_Right_middle_finger.BMP', '439__M_Right_ring_finger.BMP', '439__M_Right_thumb_finger.BMP', '43__F_Left_index_finger.BMP', '43__F_Left_little_finger.BMP', '43__F_Left_middle_finger.BMP', '43__F_Left_ring_finger.BMP', '43__F_Left_thumb_finger.BMP', '43__F_Right_index_finger.BMP', '43__F_Right_little_finger.BMP', '43__F_Right_middle_finger.BMP', '43__F_Right_ring_finger.BMP', '43__F_Right_thumb_finger.BMP', '440__M_Left_index_finger.BMP', '440__M_Left_little_finger.BMP', '440__M_Left_middle_finger.BMP', '440__M_Left_ring_finger.BMP', '440__M_Left_thumb_finger.BMP', '440__M_Right_index_finger.BMP', '440__M_Right_little_finger.BMP', '440__M_Right_middle_finger.BMP', '440__M_Right_ring_finger.BMP', '440__M_Right_thumb_finger.BMP', '441__M_Left_index_finger.BMP', '441__M_Left_little_finger.BMP', '441__M_Left_middle_finger.BMP', '441__M_Left_ring_finger.BMP', '441__M_Left_thumb_finger.BMP', '441__M_Right_index_finger.BMP', '441__M_Right_little_finger.BMP', '441__M_Right_middle_finger.BMP', '441__M_Right_ring_finger.BMP', '441__M_Right_thumb_finger.BMP', '442__F_Left_index_finger.BMP', '442__F_Left_little_finger.BMP', '442__F_Left_middle_finger.BMP', '442__F_Left_ring_finger.BMP', '442__F_Left_thumb_finger.BMP', '442__F_Right_index_finger.BMP', '442__F_Right_little_finger.BMP', '442__F_Right_middle_finger.BMP', '442__F_Right_ring_finger.BMP', '442__F_Right_thumb_finger.BMP', '443__M_Left_index_finger.BMP', '443__M_Left_little_finger.BMP', '443__M_Left_middle_finger.BMP', '443__M_Left_ring_finger.BMP', '443__M_Left_thumb_finger.BMP', '443__M_Right_index_finger.BMP', '443__M_Right_little_finger.BMP', '443__M_Right_middle_finger.BMP', '443__M_Right_ring_finger.BMP', '443__M_Right_thumb_finger.BMP', '444__M_Left_index_finger.BMP', '444__M_Left_little_finger.BMP', '444__M_Left_middle_finger.BMP', '444__M_Left_ring_finger.BMP', '444__M_Left_thumb_finger.BMP', '444__M_Right_index_finger.BMP', '444__M_Right_little_finger.BMP', '444__M_Right_middle_finger.BMP', '444__M_Right_ring_finger.BMP', '444__M_Right_thumb_finger.BMP', '445__M_Left_index_finger.BMP', '445__M_Left_little_finger.BMP', '445__M_Left_middle_finger.BMP', '445__M_Left_ring_finger.BMP', '445__M_Left_thumb_finger.BMP', '445__M_Right_index_finger.BMP', '445__M_Right_little_finger.BMP', '445__M_Right_middle_finger.BMP', '445__M_Right_ring_finger.BMP', '445__M_Right_thumb_finger.BMP', '446__M_Left_index_finger.BMP', '446__M_Left_little_finger.BMP', '446__M_Left_middle_finger.BMP', '446__M_Left_ring_finger.BMP', '446__M_Left_thumb_finger.BMP', '446__M_Right_index_finger.BMP', '446__M_Right_little_finger.BMP', '446__M_Right_middle_finger.BMP', '446__M_Right_ring_finger.BMP', '446__M_Right_thumb_finger.BMP', '447__M_Left_index_finger.BMP', '447__M_Left_little_finger.BMP', '447__M_Left_middle_finger.BMP', '447__M_Left_ring_finger.BMP', '447__M_Left_thumb_finger.BMP', '447__M_Right_index_finger.BMP', '447__M_Right_little_finger.BMP', '447__M_Right_middle_finger.BMP', '447__M_Right_ring_finger.BMP', '447__M_Right_thumb_finger.BMP', '448__M_Left_index_finger.BMP', '448__M_Left_little_finger.BMP', '448__M_Left_middle_finger.BMP', '448__M_Left_ring_finger.BMP', '448__M_Left_thumb_finger.BMP', '448__M_Right_index_finger.BMP', '448__M_Right_little_finger.BMP', '448__M_Right_middle_finger.BMP', '448__M_Right_ring_finger.BMP', '448__M_Right_thumb_finger.BMP', '449__F_Left_index_finger.BMP', '449__F_Left_little_finger.BMP', '449__F_Left_middle_finger.BMP', '449__F_Left_ring_finger.BMP', '449__F_Left_thumb_finger.BMP', '449__F_Right_index_finger.BMP', '449__F_Right_little_finger.BMP', '449__F_Right_middle_finger.BMP', '449__F_Right_ring_finger.BMP', '449__F_Right_thumb_finger.BMP', '44__M_Left_index_finger.BMP', '44__M_Left_little_finger.BMP', '44__M_Left_middle_finger.BMP', '44__M_Left_ring_finger.BMP', '44__M_Left_thumb_finger.BMP', '44__M_Right_index_finger.BMP', '44__M_Right_little_finger.BMP', '44__M_Right_middle_finger.BMP', '44__M_Right_ring_finger.BMP', '44__M_Right_thumb_finger.BMP', '450__M_Left_index_finger.BMP', '450__M_Left_little_finger.BMP', '450__M_Left_middle_finger.BMP', '450__M_Left_ring_finger.BMP', '450__M_Left_thumb_finger.BMP', '450__M_Right_index_finger.BMP', '450__M_Right_little_finger.BMP', '450__M_Right_middle_finger.BMP', '450__M_Right_ring_finger.BMP', '450__M_Right_thumb_finger.BMP', '451__M_Left_index_finger.BMP', '451__M_Left_little_finger.BMP', '451__M_Left_middle_finger.BMP', '451__M_Left_ring_finger.BMP', '451__M_Left_thumb_finger.BMP', '451__M_Right_index_finger.BMP', '451__M_Right_little_finger.BMP', '451__M_Right_middle_finger.BMP', '451__M_Right_ring_finger.BMP', '451__M_Right_thumb_finger.BMP', '452__F_Left_index_finger.BMP', '452__F_Left_little_finger.BMP', '452__F_Left_middle_finger.BMP', '452__F_Left_ring_finger.BMP', '452__F_Left_thumb_finger.BMP', '452__F_Right_index_finger.BMP', '452__F_Right_little_finger.BMP', '452__F_Right_middle_finger.BMP', '452__F_Right_ring_finger.BMP', '452__F_Right_thumb_finger.BMP', '453__F_Left_index_finger.BMP', '453__F_Left_little_finger.BMP', '453__F_Left_middle_finger.BMP', '453__F_Left_ring_finger.BMP', '453__F_Left_thumb_finger.BMP', '453__F_Right_index_finger.BMP', '453__F_Right_little_finger.BMP', '453__F_Right_middle_finger.BMP', '453__F_Right_ring_finger.BMP', '453__F_Right_thumb_finger.BMP', '454__M_Left_index_finger.BMP', '454__M_Left_little_finger.BMP', '454__M_Left_middle_finger.BMP', '454__M_Left_ring_finger.BMP', '454__M_Left_thumb_finger.BMP', '454__M_Right_index_finger.BMP', '454__M_Right_little_finger.BMP', '454__M_Right_middle_finger.BMP', '454__M_Right_ring_finger.BMP', '454__M_Right_thumb_finger.BMP', '455__M_Left_index_finger.BMP', '455__M_Left_little_finger.BMP', '455__M_Left_middle_finger.BMP', '455__M_Left_ring_finger.BMP', '455__M_Left_thumb_finger.BMP', '455__M_Right_index_finger.BMP', '455__M_Right_little_finger.BMP', '455__M_Right_middle_finger.BMP', '455__M_Right_ring_finger.BMP', '455__M_Right_thumb_finger.BMP', '456__M_Left_index_finger.BMP', '456__M_Left_little_finger.BMP', '456__M_Left_middle_finger.BMP', '456__M_Left_ring_finger.BMP', '456__M_Left_thumb_finger.BMP', '456__M_Right_index_finger.BMP', '456__M_Right_little_finger.BMP', '456__M_Right_middle_finger.BMP', '456__M_Right_ring_finger.BMP', '456__M_Right_thumb_finger.BMP', '457__M_Left_index_finger.BMP', '457__M_Left_little_finger.BMP', '457__M_Left_middle_finger.BMP', '457__M_Left_ring_finger.BMP', '457__M_Left_thumb_finger.BMP', '457__M_Right_index_finger.BMP', '457__M_Right_little_finger.BMP', '457__M_Right_middle_finger.BMP', '457__M_Right_ring_finger.BMP', '457__M_Right_thumb_finger.BMP', '458__M_Left_index_finger.BMP', '458__M_Left_little_finger.BMP', '458__M_Left_middle_finger.BMP', '458__M_Left_ring_finger.BMP', '458__M_Left_thumb_finger.BMP', '458__M_Right_index_finger.BMP', '458__M_Right_little_finger.BMP', '458__M_Right_middle_finger.BMP', '458__M_Right_ring_finger.BMP', '458__M_Right_thumb_finger.BMP', '459__M_Left_index_finger.BMP', '459__M_Left_little_finger.BMP', '459__M_Left_middle_finger.BMP', '459__M_Left_ring_finger.BMP', '459__M_Left_thumb_finger.BMP', '459__M_Right_index_finger.BMP', '459__M_Right_little_finger.BMP', '459__M_Right_middle_finger.BMP', '459__M_Right_ring_finger.BMP', '459__M_Right_thumb_finger.BMP', '45__M_Left_index_finger.BMP', '45__M_Left_little_finger.BMP', '45__M_Left_middle_finger.BMP', '45__M_Left_ring_finger.BMP', '45__M_Left_thumb_finger.BMP', '45__M_Right_index_finger.BMP', '45__M_Right_little_finger.BMP', '45__M_Right_middle_finger.BMP', '45__M_Right_ring_finger.BMP', '45__M_Right_thumb_finger.BMP', '460__M_Left_index_finger.BMP', '460__M_Left_little_finger.BMP', '460__M_Left_middle_finger.BMP', '460__M_Left_ring_finger.BMP', '460__M_Left_thumb_finger.BMP', '460__M_Right_index_finger.BMP', '460__M_Right_little_finger.BMP', '460__M_Right_middle_finger.BMP', '460__M_Right_ring_finger.BMP', '460__M_Right_thumb_finger.BMP', '461__F_Left_index_finger.BMP', '461__F_Left_little_finger.BMP', '461__F_Left_middle_finger.BMP', '461__F_Left_ring_finger.BMP', '461__F_Left_thumb_finger.BMP', '461__F_Right_index_finger.BMP', '461__F_Right_little_finger.BMP', '461__F_Right_middle_finger.BMP', '461__F_Right_ring_finger.BMP', '461__F_Right_thumb_finger.BMP', '462__M_Left_index_finger.BMP', '462__M_Left_little_finger.BMP', '462__M_Left_middle_finger.BMP', '462__M_Left_ring_finger.BMP', '462__M_Left_thumb_finger.BMP', '462__M_Right_index_finger.BMP', '462__M_Right_little_finger.BMP', '462__M_Right_middle_finger.BMP', '462__M_Right_ring_finger.BMP', '462__M_Right_thumb_finger.BMP', '463__F_Left_index_finger.BMP', '463__F_Left_little_finger.BMP', '463__F_Left_middle_finger.BMP', '463__F_Left_ring_finger.BMP', '463__F_Left_thumb_finger.BMP', '463__F_Right_index_finger.BMP', '463__F_Right_little_finger.BMP', '463__F_Right_middle_finger.BMP', '463__F_Right_ring_finger.BMP', '463__F_Right_thumb_finger.BMP', '464__M_Left_index_finger.BMP', '464__M_Left_little_finger.BMP', '464__M_Left_middle_finger.BMP', '464__M_Left_ring_finger.BMP', '464__M_Left_thumb_finger.BMP', '464__M_Right_index_finger.BMP', '464__M_Right_little_finger.BMP', '464__M_Right_middle_finger.BMP', '464__M_Right_ring_finger.BMP', '464__M_Right_thumb_finger.BMP', '465__F_Left_index_finger.BMP', '465__F_Left_little_finger.BMP', '465__F_Left_middle_finger.BMP', '465__F_Left_ring_finger.BMP', '465__F_Left_thumb_finger.BMP', '465__F_Right_index_finger.BMP', '465__F_Right_little_finger.BMP', '465__F_Right_middle_finger.BMP', '465__F_Right_ring_finger.BMP', '465__F_Right_thumb_finger.BMP', '466__F_Left_index_finger.BMP', '466__F_Left_little_finger.BMP', '466__F_Left_middle_finger.BMP', '466__F_Left_ring_finger.BMP', '466__F_Left_thumb_finger.BMP', '466__F_Right_index_finger.BMP', '466__F_Right_little_finger.BMP', '466__F_Right_middle_finger.BMP', '466__F_Right_ring_finger.BMP', '466__F_Right_thumb_finger.BMP', '467__M_Left_index_finger.BMP', '467__M_Left_little_finger.BMP', '467__M_Left_middle_finger.BMP', '467__M_Left_ring_finger.BMP', '467__M_Left_thumb_finger.BMP', '467__M_Right_index_finger.BMP', '467__M_Right_little_finger.BMP', '467__M_Right_middle_finger.BMP', '467__M_Right_ring_finger.BMP', '467__M_Right_thumb_finger.BMP', '468__F_Left_index_finger.BMP', '468__F_Left_little_finger.BMP', '468__F_Left_middle_finger.BMP', '468__F_Left_ring_finger.BMP', '468__F_Left_thumb_finger.BMP', '468__F_Right_index_finger.BMP', '468__F_Right_little_finger.BMP', '468__F_Right_middle_finger.BMP', '468__F_Right_ring_finger.BMP', '468__F_Right_thumb_finger.BMP', '469__M_Left_index_finger.BMP', '469__M_Left_little_finger.BMP', '469__M_Left_middle_finger.BMP', '469__M_Left_ring_finger.BMP', '469__M_Left_thumb_finger.BMP', '469__M_Right_index_finger.BMP', '469__M_Right_little_finger.BMP', '469__M_Right_middle_finger.BMP', '469__M_Right_ring_finger.BMP', '469__M_Right_thumb_finger.BMP', '46__M_Left_index_finger.BMP', '46__M_Left_little_finger.BMP', '46__M_Left_middle_finger.BMP', '46__M_Left_ring_finger.BMP', '46__M_Left_thumb_finger.BMP', '46__M_Right_index_finger.BMP', '46__M_Right_little_finger.BMP', '46__M_Right_middle_finger.BMP', '46__M_Right_ring_finger.BMP', '46__M_Right_thumb_finger.BMP', '470__F_Left_index_finger.BMP', '470__F_Left_little_finger.BMP', '470__F_Left_middle_finger.BMP', '470__F_Left_ring_finger.BMP', '470__F_Left_thumb_finger.BMP', '470__F_Right_index_finger.BMP', '470__F_Right_little_finger.BMP', '470__F_Right_middle_finger.BMP', '470__F_Right_ring_finger.BMP', '470__F_Right_thumb_finger.BMP', '471__M_Left_index_finger.BMP', '471__M_Left_little_finger.BMP', '471__M_Left_middle_finger.BMP', '471__M_Left_ring_finger.BMP', '471__M_Left_thumb_finger.BMP', '471__M_Right_index_finger.BMP', '471__M_Right_little_finger.BMP', '471__M_Right_middle_finger.BMP', '471__M_Right_ring_finger.BMP', '471__M_Right_thumb_finger.BMP', '472__M_Left_index_finger.BMP', '472__M_Left_little_finger.BMP', '472__M_Left_middle_finger.BMP', '472__M_Left_ring_finger.BMP', '472__M_Left_thumb_finger.BMP', '472__M_Right_index_finger.BMP', '472__M_Right_little_finger.BMP', '472__M_Right_middle_finger.BMP', '472__M_Right_ring_finger.BMP', '472__M_Right_thumb_finger.BMP', '473__M_Left_index_finger.BMP', '473__M_Left_little_finger.BMP', '473__M_Left_middle_finger.BMP', '473__M_Left_ring_finger.BMP', '473__M_Left_thumb_finger.BMP', '473__M_Right_index_finger.BMP', '473__M_Right_little_finger.BMP', '473__M_Right_middle_finger.BMP', '473__M_Right_ring_finger.BMP', '473__M_Right_thumb_finger.BMP', '474__M_Left_index_finger.BMP', '474__M_Left_little_finger.BMP', '474__M_Left_middle_finger.BMP', '474__M_Left_ring_finger.BMP', '474__M_Left_thumb_finger.BMP', '474__M_Right_index_finger.BMP', '474__M_Right_little_finger.BMP', '474__M_Right_middle_finger.BMP', '474__M_Right_ring_finger.BMP', '474__M_Right_thumb_finger.BMP', '475__M_Left_index_finger.BMP', '475__M_Left_little_finger.BMP', '475__M_Left_middle_finger.BMP', '475__M_Left_ring_finger.BMP', '475__M_Left_thumb_finger.BMP', '475__M_Right_index_finger.BMP', '475__M_Right_little_finger.BMP', '475__M_Right_middle_finger.BMP', '475__M_Right_ring_finger.BMP', '475__M_Right_thumb_finger.BMP', '476__M_Left_index_finger.BMP', '476__M_Left_little_finger.BMP', '476__M_Left_middle_finger.BMP', '476__M_Left_ring_finger.BMP', '476__M_Left_thumb_finger.BMP', '476__M_Right_index_finger.BMP', '476__M_Right_little_finger.BMP', '476__M_Right_middle_finger.BMP', '476__M_Right_ring_finger.BMP', '476__M_Right_thumb_finger.BMP', '477__M_Left_index_finger.BMP', '477__M_Left_little_finger.BMP', '477__M_Left_middle_finger.BMP', '477__M_Left_ring_finger.BMP', '477__M_Left_thumb_finger.BMP', '477__M_Right_index_finger.BMP', '477__M_Right_little_finger.BMP', '477__M_Right_middle_finger.BMP', '477__M_Right_ring_finger.BMP', '477__M_Right_thumb_finger.BMP', '478__M_Left_index_finger.BMP', '478__M_Left_little_finger.BMP', '478__M_Left_middle_finger.BMP', '478__M_Left_ring_finger.BMP', '478__M_Left_thumb_finger.BMP', '478__M_Right_index_finger.BMP', '478__M_Right_little_finger.BMP', '478__M_Right_middle_finger.BMP', '478__M_Right_ring_finger.BMP', '478__M_Right_thumb_finger.BMP', '479__F_Left_index_finger.BMP', '479__F_Left_little_finger.BMP', '479__F_Left_middle_finger.BMP', '479__F_Left_ring_finger.BMP', '479__F_Left_thumb_finger.BMP', '479__F_Right_index_finger.BMP', '479__F_Right_little_finger.BMP', '479__F_Right_middle_finger.BMP', '479__F_Right_ring_finger.BMP', '479__F_Right_thumb_finger.BMP', '47__F_Left_index_finger.BMP', '47__F_Left_little_finger.BMP', '47__F_Left_middle_finger.BMP', '47__F_Left_ring_finger.BMP', '47__F_Left_thumb_finger.BMP', '47__F_Right_index_finger.BMP', '47__F_Right_little_finger.BMP', '47__F_Right_middle_finger.BMP', '47__F_Right_ring_finger.BMP', '47__F_Right_thumb_finger.BMP', '480__M_Left_index_finger.BMP', '480__M_Left_little_finger.BMP', '480__M_Left_middle_finger.BMP', '480__M_Left_ring_finger.BMP', '480__M_Left_thumb_finger.BMP', '480__M_Right_index_finger.BMP', '480__M_Right_little_finger.BMP', '480__M_Right_middle_finger.BMP', '480__M_Right_ring_finger.BMP', '480__M_Right_thumb_finger.BMP', '481__F_Left_index_finger.BMP', '481__F_Left_little_finger.BMP', '481__F_Left_middle_finger.BMP', '481__F_Left_ring_finger.BMP', '481__F_Left_thumb_finger.BMP', '481__F_Right_index_finger.BMP', '481__F_Right_little_finger.BMP', '481__F_Right_middle_finger.BMP', '481__F_Right_ring_finger.BMP', '481__F_Right_thumb_finger.BMP', '482__M_Left_index_finger.BMP', '482__M_Left_little_finger.BMP', '482__M_Left_middle_finger.BMP', '482__M_Left_ring_finger.BMP', '482__M_Left_thumb_finger.BMP', '482__M_Right_index_finger.BMP', '482__M_Right_little_finger.BMP', '482__M_Right_middle_finger.BMP', '482__M_Right_ring_finger.BMP', '482__M_Right_thumb_finger.BMP', '483__M_Left_index_finger.BMP', '483__M_Left_little_finger.BMP', '483__M_Left_middle_finger.BMP', '483__M_Left_ring_finger.BMP', '483__M_Left_thumb_finger.BMP', '483__M_Right_index_finger.BMP', '483__M_Right_little_finger.BMP', '483__M_Right_middle_finger.BMP', '483__M_Right_ring_finger.BMP', '483__M_Right_thumb_finger.BMP', '484__M_Left_index_finger.BMP', '484__M_Left_little_finger.BMP', '484__M_Left_middle_finger.BMP', '484__M_Left_ring_finger.BMP', '484__M_Left_thumb_finger.BMP', '484__M_Right_index_finger.BMP', '484__M_Right_little_finger.BMP', '484__M_Right_middle_finger.BMP', '484__M_Right_ring_finger.BMP', '484__M_Right_thumb_finger.BMP', '485__M_Left_index_finger.BMP', '485__M_Left_little_finger.BMP', '485__M_Left_middle_finger.BMP', '485__M_Left_ring_finger.BMP', '485__M_Left_thumb_finger.BMP', '485__M_Right_index_finger.BMP', '485__M_Right_little_finger.BMP', '485__M_Right_middle_finger.BMP', '485__M_Right_ring_finger.BMP', '485__M_Right_thumb_finger.BMP', '486__M_Left_index_finger.BMP', '486__M_Left_little_finger.BMP', '486__M_Left_middle_finger.BMP', '486__M_Left_ring_finger.BMP', '486__M_Left_thumb_finger.BMP', '486__M_Right_index_finger.BMP', '486__M_Right_little_finger.BMP', '486__M_Right_middle_finger.BMP', '486__M_Right_ring_finger.BMP', '486__M_Right_thumb_finger.BMP', '487__M_Left_index_finger.BMP', '487__M_Left_little_finger.BMP', '487__M_Left_middle_finger.BMP', '487__M_Left_ring_finger.BMP', '487__M_Left_thumb_finger.BMP', '487__M_Right_index_finger.BMP', '487__M_Right_little_finger.BMP', '487__M_Right_middle_finger.BMP', '487__M_Right_ring_finger.BMP', '487__M_Right_thumb_finger.BMP', '488__M_Left_index_finger.BMP', '488__M_Left_little_finger.BMP', '488__M_Left_middle_finger.BMP', '488__M_Left_ring_finger.BMP', '488__M_Left_thumb_finger.BMP', '488__M_Right_index_finger.BMP', '488__M_Right_little_finger.BMP', '488__M_Right_middle_finger.BMP', '488__M_Right_ring_finger.BMP', '488__M_Right_thumb_finger.BMP', '489__M_Left_index_finger.BMP', '489__M_Left_little_finger.BMP', '489__M_Left_middle_finger.BMP', '489__M_Left_ring_finger.BMP', '489__M_Left_thumb_finger.BMP', '489__M_Right_index_finger.BMP', '489__M_Right_little_finger.BMP', '489__M_Right_middle_finger.BMP', '489__M_Right_ring_finger.BMP', '489__M_Right_thumb_finger.BMP', '48__F_Left_index_finger.BMP', '48__F_Left_little_finger.BMP', '48__F_Left_middle_finger.BMP', '48__F_Left_ring_finger.BMP', '48__F_Left_thumb_finger.BMP', '48__F_Right_index_finger.BMP', '48__F_Right_little_finger.BMP', '48__F_Right_middle_finger.BMP', '48__F_Right_ring_finger.BMP', '48__F_Right_thumb_finger.BMP', '490__M_Left_index_finger.BMP', '490__M_Left_little_finger.BMP', '490__M_Left_middle_finger.BMP', '490__M_Left_ring_finger.BMP', '490__M_Left_thumb_finger.BMP', '490__M_Right_index_finger.BMP', '490__M_Right_little_finger.BMP', '490__M_Right_middle_finger.BMP', '490__M_Right_ring_finger.BMP', '490__M_Right_thumb_finger.BMP', '491__M_Left_index_finger.BMP', '491__M_Left_little_finger.BMP', '491__M_Left_middle_finger.BMP', '491__M_Left_ring_finger.BMP', '491__M_Left_thumb_finger.BMP', '491__M_Right_index_finger.BMP', '491__M_Right_little_finger.BMP', '491__M_Right_middle_finger.BMP', '491__M_Right_ring_finger.BMP', '491__M_Right_thumb_finger.BMP', '492__F_Left_index_finger.BMP', '492__F_Left_little_finger.BMP', '492__F_Left_middle_finger.BMP', '492__F_Left_ring_finger.BMP', '492__F_Left_thumb_finger.BMP', '492__F_Right_index_finger.BMP', '492__F_Right_little_finger.BMP', '492__F_Right_middle_finger.BMP', '492__F_Right_ring_finger.BMP', '492__F_Right_thumb_finger.BMP', '493__M_Left_index_finger.BMP', '493__M_Left_little_finger.BMP', '493__M_Left_middle_finger.BMP', '493__M_Left_ring_finger.BMP', '493__M_Left_thumb_finger.BMP', '493__M_Right_index_finger.BMP', '493__M_Right_little_finger.BMP', '493__M_Right_middle_finger.BMP', '493__M_Right_ring_finger.BMP', '493__M_Right_thumb_finger.BMP', '494__F_Left_index_finger.BMP', '494__F_Left_little_finger.BMP', '494__F_Left_middle_finger.BMP', '494__F_Left_ring_finger.BMP', '494__F_Left_thumb_finger.BMP', '494__F_Right_index_finger.BMP', '494__F_Right_little_finger.BMP', '494__F_Right_middle_finger.BMP', '494__F_Right_ring_finger.BMP', '494__F_Right_thumb_finger.BMP', '495__M_Left_index_finger.BMP', '495__M_Left_little_finger.BMP', '495__M_Left_middle_finger.BMP', '495__M_Left_ring_finger.BMP', '495__M_Left_thumb_finger.BMP', '495__M_Right_index_finger.BMP', '495__M_Right_little_finger.BMP', '495__M_Right_middle_finger.BMP', '495__M_Right_ring_finger.BMP', '495__M_Right_thumb_finger.BMP', '496__M_Left_index_finger.BMP', '496__M_Left_little_finger.BMP', '496__M_Left_middle_finger.BMP', '496__M_Left_ring_finger.BMP', '496__M_Left_thumb_finger.BMP', '496__M_Right_index_finger.BMP', '496__M_Right_little_finger.BMP', '496__M_Right_middle_finger.BMP', '496__M_Right_ring_finger.BMP', '496__M_Right_thumb_finger.BMP', '497__M_Left_index_finger.BMP', '497__M_Left_little_finger.BMP', '497__M_Left_middle_finger.BMP', '497__M_Left_ring_finger.BMP', '497__M_Left_thumb_finger.BMP', '497__M_Right_index_finger.BMP', '497__M_Right_little_finger.BMP', '497__M_Right_middle_finger.BMP', '497__M_Right_ring_finger.BMP', '497__M_Right_thumb_finger.BMP', '498__M_Left_index_finger.BMP', '498__M_Left_little_finger.BMP', '498__M_Left_middle_finger.BMP', '498__M_Left_ring_finger.BMP', '498__M_Left_thumb_finger.BMP', '498__M_Right_index_finger.BMP', '498__M_Right_little_finger.BMP', '498__M_Right_middle_finger.BMP', '498__M_Right_ring_finger.BMP', '498__M_Right_thumb_finger.BMP', '499__M_Left_index_finger.BMP', '499__M_Left_little_finger.BMP', '499__M_Left_middle_finger.BMP', '499__M_Left_ring_finger.BMP', '499__M_Left_thumb_finger.BMP', '499__M_Right_index_finger.BMP', '499__M_Right_little_finger.BMP', '499__M_Right_middle_finger.BMP', '499__M_Right_ring_finger.BMP', '499__M_Right_thumb_finger.BMP', '49__M_Left_index_finger.BMP', '49__M_Left_little_finger.BMP', '49__M_Left_middle_finger.BMP', '49__M_Left_ring_finger.BMP', '49__M_Left_thumb_finger.BMP', '49__M_Right_index_finger.BMP', '49__M_Right_little_finger.BMP', '49__M_Right_middle_finger.BMP', '49__M_Right_ring_finger.BMP', '49__M_Right_thumb_finger.BMP', '4__M_Left_index_finger.BMP', '4__M_Left_little_finger.BMP', '4__M_Left_middle_finger.BMP', '4__M_Left_ring_finger.BMP', '4__M_Left_thumb_finger.BMP', '4__M_Right_index_finger.BMP', '4__M_Right_little_finger.BMP', '4__M_Right_middle_finger.BMP', '4__M_Right_ring_finger.BMP', '4__M_Right_thumb_finger.BMP', '500__M_Left_index_finger.BMP', '500__M_Left_little_finger.BMP', '500__M_Left_middle_finger.BMP', '500__M_Left_ring_finger.BMP', '500__M_Left_thumb_finger.BMP', '500__M_Right_index_finger.BMP', '500__M_Right_little_finger.BMP', '500__M_Right_middle_finger.BMP', '500__M_Right_ring_finger.BMP', '500__M_Right_thumb_finger.BMP', '501__M_Left_index_finger.BMP', '501__M_Left_little_finger.BMP', '501__M_Left_middle_finger.BMP', '501__M_Left_ring_finger.BMP', '501__M_Left_thumb_finger.BMP', '501__M_Right_index_finger.BMP', '501__M_Right_little_finger.BMP', '501__M_Right_middle_finger.BMP', '501__M_Right_ring_finger.BMP', '501__M_Right_thumb_finger.BMP', '502__F_Left_index_finger.BMP', '502__F_Left_little_finger.BMP', '502__F_Left_middle_finger.BMP', '502__F_Left_ring_finger.BMP', '502__F_Left_thumb_finger.BMP', '502__F_Right_index_finger.BMP', '502__F_Right_little_finger.BMP', '502__F_Right_middle_finger.BMP', '502__F_Right_ring_finger.BMP', '502__F_Right_thumb_finger.BMP', '503__M_Left_index_finger.BMP', '503__M_Left_little_finger.BMP', '503__M_Left_middle_finger.BMP', '503__M_Left_ring_finger.BMP', '503__M_Left_thumb_finger.BMP', '503__M_Right_index_finger.BMP', '503__M_Right_little_finger.BMP', '503__M_Right_middle_finger.BMP', '503__M_Right_ring_finger.BMP', '503__M_Right_thumb_finger.BMP', '504__M_Left_index_finger.BMP', '504__M_Left_little_finger.BMP', '504__M_Left_middle_finger.BMP', '504__M_Left_ring_finger.BMP', '504__M_Left_thumb_finger.BMP', '504__M_Right_index_finger.BMP', '504__M_Right_little_finger.BMP', '504__M_Right_middle_finger.BMP', '504__M_Right_ring_finger.BMP', '504__M_Right_thumb_finger.BMP', '505__M_Left_index_finger.BMP', '505__M_Left_little_finger.BMP', '505__M_Left_middle_finger.BMP', '505__M_Left_ring_finger.BMP', '505__M_Left_thumb_finger.BMP', '505__M_Right_index_finger.BMP', '505__M_Right_little_finger.BMP', '505__M_Right_middle_finger.BMP', '505__M_Right_ring_finger.BMP', '505__M_Right_thumb_finger.BMP', '506__M_Left_index_finger.BMP', '506__M_Left_little_finger.BMP', '506__M_Left_middle_finger.BMP', '506__M_Left_ring_finger.BMP', '506__M_Left_thumb_finger.BMP', '506__M_Right_index_finger.BMP', '506__M_Right_little_finger.BMP', '506__M_Right_middle_finger.BMP', '506__M_Right_ring_finger.BMP', '506__M_Right_thumb_finger.BMP', '507__M_Left_index_finger.BMP', '507__M_Left_little_finger.BMP', '507__M_Left_middle_finger.BMP', '507__M_Left_ring_finger.BMP', '507__M_Left_thumb_finger.BMP', '507__M_Right_index_finger.BMP', '507__M_Right_little_finger.BMP', '507__M_Right_middle_finger.BMP', '507__M_Right_ring_finger.BMP', '507__M_Right_thumb_finger.BMP', '508__F_Left_index_finger.BMP', '508__F_Left_little_finger.BMP', '508__F_Left_middle_finger.BMP', '508__F_Left_ring_finger.BMP', '508__F_Left_thumb_finger.BMP', '508__F_Right_index_finger.BMP', '508__F_Right_little_finger.BMP', '508__F_Right_middle_finger.BMP', '508__F_Right_ring_finger.BMP', '508__F_Right_thumb_finger.BMP', '509__M_Left_index_finger.BMP', '509__M_Left_little_finger.BMP', '509__M_Left_middle_finger.BMP', '509__M_Left_ring_finger.BMP', '509__M_Left_thumb_finger.BMP', '509__M_Right_index_finger.BMP', '509__M_Right_little_finger.BMP', '509__M_Right_middle_finger.BMP', '509__M_Right_ring_finger.BMP', '509__M_Right_thumb_finger.BMP', '50__M_Left_index_finger.BMP', '50__M_Left_little_finger.BMP', '50__M_Left_middle_finger.BMP', '50__M_Left_ring_finger.BMP', '50__M_Left_thumb_finger.BMP', '50__M_Right_index_finger.BMP', '50__M_Right_little_finger.BMP', '50__M_Right_middle_finger.BMP', '50__M_Right_ring_finger.BMP', '50__M_Right_thumb_finger.BMP', '510__M_Left_index_finger.BMP', '510__M_Left_little_finger.BMP', '510__M_Left_middle_finger.BMP', '510__M_Left_ring_finger.BMP', '510__M_Left_thumb_finger.BMP', '510__M_Right_index_finger.BMP', '510__M_Right_little_finger.BMP', '510__M_Right_middle_finger.BMP', '510__M_Right_ring_finger.BMP', '510__M_Right_thumb_finger.BMP', '511__M_Left_index_finger.BMP', '511__M_Left_little_finger.BMP', '511__M_Left_middle_finger.BMP', '511__M_Left_ring_finger.BMP', '511__M_Left_thumb_finger.BMP', '511__M_Right_index_finger.BMP', '511__M_Right_little_finger.BMP', '511__M_Right_middle_finger.BMP', '511__M_Right_ring_finger.BMP', '511__M_Right_thumb_finger.BMP', '512__M_Left_index_finger.BMP', '512__M_Left_little_finger.BMP', '512__M_Left_middle_finger.BMP', '512__M_Left_ring_finger.BMP', '512__M_Left_thumb_finger.BMP', '512__M_Right_index_finger.BMP', '512__M_Right_little_finger.BMP', '512__M_Right_middle_finger.BMP', '512__M_Right_ring_finger.BMP', '512__M_Right_thumb_finger.BMP', '513__M_Left_index_finger.BMP', '513__M_Left_little_finger.BMP', '513__M_Left_middle_finger.BMP', '513__M_Left_ring_finger.BMP', '513__M_Left_thumb_finger.BMP', '513__M_Right_index_finger.BMP', '513__M_Right_little_finger.BMP', '513__M_Right_middle_finger.BMP', '513__M_Right_ring_finger.BMP', '513__M_Right_thumb_finger.BMP', '514__F_Left_index_finger.BMP', '514__F_Left_little_finger.BMP', '514__F_Left_middle_finger.BMP', '514__F_Left_ring_finger.BMP', '514__F_Left_thumb_finger.BMP', '514__F_Right_index_finger.BMP', '514__F_Right_little_finger.BMP', '514__F_Right_middle_finger.BMP', '514__F_Right_ring_finger.BMP', '514__F_Right_thumb_finger.BMP', '515__M_Left_index_finger.BMP', '515__M_Left_little_finger.BMP', '515__M_Left_middle_finger.BMP', '515__M_Left_ring_finger.BMP', '515__M_Left_thumb_finger.BMP', '515__M_Right_index_finger.BMP', '515__M_Right_little_finger.BMP', '515__M_Right_middle_finger.BMP', '515__M_Right_ring_finger.BMP', '515__M_Right_thumb_finger.BMP', '516__M_Left_index_finger.BMP', '516__M_Left_little_finger.BMP', '516__M_Left_middle_finger.BMP', '516__M_Left_ring_finger.BMP', '516__M_Left_thumb_finger.BMP', '516__M_Right_index_finger.BMP', '516__M_Right_little_finger.BMP', '516__M_Right_middle_finger.BMP', '516__M_Right_ring_finger.BMP', '516__M_Right_thumb_finger.BMP', '517__M_Left_index_finger.BMP', '517__M_Left_little_finger.BMP', '517__M_Left_middle_finger.BMP', '517__M_Left_ring_finger.BMP', '517__M_Left_thumb_finger.BMP', '517__M_Right_index_finger.BMP', '517__M_Right_little_finger.BMP', '517__M_Right_middle_finger.BMP', '517__M_Right_ring_finger.BMP', '517__M_Right_thumb_finger.BMP', '518__M_Left_index_finger.BMP', '518__M_Left_little_finger.BMP', '518__M_Left_middle_finger.BMP', '518__M_Left_ring_finger.BMP', '518__M_Left_thumb_finger.BMP', '518__M_Right_index_finger.BMP', '518__M_Right_little_finger.BMP', '518__M_Right_middle_finger.BMP', '518__M_Right_ring_finger.BMP', '518__M_Right_thumb_finger.BMP', '519__M_Left_index_finger.BMP', '519__M_Left_little_finger.BMP', '519__M_Left_middle_finger.BMP', '519__M_Left_ring_finger.BMP', '519__M_Left_thumb_finger.BMP', '519__M_Right_index_finger.BMP', '519__M_Right_little_finger.BMP', '519__M_Right_middle_finger.BMP', '519__M_Right_ring_finger.BMP', '519__M_Right_thumb_finger.BMP', '51__M_Left_index_finger.BMP', '51__M_Left_little_finger.BMP', '51__M_Left_middle_finger.BMP', '51__M_Left_ring_finger.BMP', '51__M_Left_thumb_finger.BMP', '51__M_Right_index_finger.BMP', '51__M_Right_little_finger.BMP', '51__M_Right_middle_finger.BMP', '51__M_Right_ring_finger.BMP', '51__M_Right_thumb_finger.BMP', '520__M_Left_index_finger.BMP', '520__M_Left_little_finger.BMP', '520__M_Left_middle_finger.BMP', '520__M_Left_ring_finger.BMP', '520__M_Left_thumb_finger.BMP', '520__M_Right_index_finger.BMP', '520__M_Right_little_finger.BMP', '520__M_Right_middle_finger.BMP', '520__M_Right_ring_finger.BMP', '520__M_Right_thumb_finger.BMP', '521__M_Left_index_finger.BMP', '521__M_Left_little_finger.BMP', '521__M_Left_middle_finger.BMP', '521__M_Left_ring_finger.BMP', '521__M_Left_thumb_finger.BMP', '521__M_Right_index_finger.BMP', '521__M_Right_little_finger.BMP', '521__M_Right_middle_finger.BMP', '521__M_Right_ring_finger.BMP', '521__M_Right_thumb_finger.BMP', '522__M_Left_index_finger.BMP', '522__M_Left_little_finger.BMP', '522__M_Left_middle_finger.BMP', '522__M_Left_ring_finger.BMP', '522__M_Left_thumb_finger.BMP', '522__M_Right_index_finger.BMP', '522__M_Right_little_finger.BMP', '522__M_Right_middle_finger.BMP', '522__M_Right_ring_finger.BMP', '522__M_Right_thumb_finger.BMP', '523__F_Left_index_finger.BMP', '523__F_Left_little_finger.BMP', '523__F_Left_middle_finger.BMP', '523__F_Left_ring_finger.BMP', '523__F_Left_thumb_finger.BMP', '523__F_Right_index_finger.BMP', '523__F_Right_little_finger.BMP', '523__F_Right_middle_finger.BMP', '523__F_Right_ring_finger.BMP', '523__F_Right_thumb_finger.BMP', '524__M_Left_index_finger.BMP', '524__M_Left_little_finger.BMP', '524__M_Left_middle_finger.BMP', '524__M_Left_ring_finger.BMP', '524__M_Left_thumb_finger.BMP', '524__M_Right_index_finger.BMP', '524__M_Right_little_finger.BMP', '524__M_Right_middle_finger.BMP', '524__M_Right_ring_finger.BMP', '524__M_Right_thumb_finger.BMP', '525__M_Left_index_finger.BMP', '525__M_Left_little_finger.BMP', '525__M_Left_middle_finger.BMP', '525__M_Left_ring_finger.BMP', '525__M_Left_thumb_finger.BMP', '525__M_Right_index_finger.BMP', '525__M_Right_little_finger.BMP', '525__M_Right_middle_finger.BMP', '525__M_Right_ring_finger.BMP', '525__M_Right_thumb_finger.BMP', '526__F_Left_index_finger.BMP', '526__F_Left_little_finger.BMP', '526__F_Left_middle_finger.BMP', '526__F_Left_ring_finger.BMP', '526__F_Left_thumb_finger.BMP', '526__F_Right_index_finger.BMP', '526__F_Right_little_finger.BMP', '526__F_Right_middle_finger.BMP', '526__F_Right_ring_finger.BMP', '526__F_Right_thumb_finger.BMP', '527__M_Left_index_finger.BMP', '527__M_Left_little_finger.BMP', '527__M_Left_middle_finger.BMP', '527__M_Left_ring_finger.BMP', '527__M_Left_thumb_finger.BMP', '527__M_Right_index_finger.BMP', '527__M_Right_little_finger.BMP', '527__M_Right_middle_finger.BMP', '527__M_Right_ring_finger.BMP', '527__M_Right_thumb_finger.BMP', '528__F_Left_index_finger.BMP', '528__F_Left_little_finger.BMP', '528__F_Left_middle_finger.BMP', '528__F_Left_ring_finger.BMP', '528__F_Left_thumb_finger.BMP', '528__F_Right_index_finger.BMP', '528__F_Right_little_finger.BMP', '528__F_Right_middle_finger.BMP', '528__F_Right_ring_finger.BMP', '528__F_Right_thumb_finger.BMP', '529__M_Left_index_finger.BMP', '529__M_Left_little_finger.BMP', '529__M_Left_middle_finger.BMP', '529__M_Left_ring_finger.BMP', '529__M_Left_thumb_finger.BMP', '529__M_Right_index_finger.BMP', '529__M_Right_little_finger.BMP', '529__M_Right_middle_finger.BMP', '529__M_Right_ring_finger.BMP', '529__M_Right_thumb_finger.BMP', '52__M_Left_index_finger.BMP', '52__M_Left_little_finger.BMP', '52__M_Left_middle_finger.BMP', '52__M_Left_ring_finger.BMP', '52__M_Left_thumb_finger.BMP', '52__M_Right_index_finger.BMP', '52__M_Right_little_finger.BMP', '52__M_Right_middle_finger.BMP', '52__M_Right_ring_finger.BMP', '52__M_Right_thumb_finger.BMP', '530__M_Left_index_finger.BMP', '530__M_Left_little_finger.BMP', '530__M_Left_middle_finger.BMP', '530__M_Left_ring_finger.BMP', '530__M_Left_thumb_finger.BMP', '530__M_Right_index_finger.BMP', '530__M_Right_little_finger.BMP', '530__M_Right_middle_finger.BMP', '530__M_Right_ring_finger.BMP', '530__M_Right_thumb_finger.BMP', '531__M_Left_index_finger.BMP', '531__M_Left_little_finger.BMP', '531__M_Left_middle_finger.BMP', '531__M_Left_ring_finger.BMP', '531__M_Left_thumb_finger.BMP', '531__M_Right_index_finger.BMP', '531__M_Right_little_finger.BMP', '531__M_Right_middle_finger.BMP', '531__M_Right_ring_finger.BMP', '531__M_Right_thumb_finger.BMP', '532__M_Left_index_finger.BMP', '532__M_Left_little_finger.BMP', '532__M_Left_middle_finger.BMP', '532__M_Left_ring_finger.BMP', '532__M_Left_thumb_finger.BMP', '532__M_Right_index_finger.BMP', '532__M_Right_little_finger.BMP', '532__M_Right_middle_finger.BMP', '532__M_Right_ring_finger.BMP', '532__M_Right_thumb_finger.BMP', '533__M_Left_index_finger.BMP', '533__M_Left_little_finger.BMP', '533__M_Left_middle_finger.BMP', '533__M_Left_ring_finger.BMP', '533__M_Left_thumb_finger.BMP', '533__M_Right_index_finger.BMP', '533__M_Right_little_finger.BMP', '533__M_Right_middle_finger.BMP', '533__M_Right_ring_finger.BMP', '533__M_Right_thumb_finger.BMP', '534__F_Left_index_finger.BMP', '534__F_Left_little_finger.BMP', '534__F_Left_middle_finger.BMP', '534__F_Left_ring_finger.BMP', '534__F_Left_thumb_finger.BMP', '534__F_Right_index_finger.BMP', '534__F_Right_little_finger.BMP', '534__F_Right_middle_finger.BMP', '534__F_Right_ring_finger.BMP', '534__F_Right_thumb_finger.BMP', '535__M_Left_index_finger.BMP', '535__M_Left_little_finger.BMP', '535__M_Left_middle_finger.BMP', '535__M_Left_ring_finger.BMP', '535__M_Left_thumb_finger.BMP', '535__M_Right_index_finger.BMP', '535__M_Right_little_finger.BMP', '535__M_Right_middle_finger.BMP', '535__M_Right_ring_finger.BMP', '535__M_Right_thumb_finger.BMP', '536__F_Left_index_finger.BMP', '536__F_Left_little_finger.BMP', '536__F_Left_middle_finger.BMP', '536__F_Left_ring_finger.BMP', '536__F_Left_thumb_finger.BMP', '536__F_Right_index_finger.BMP', '536__F_Right_little_finger.BMP', '536__F_Right_middle_finger.BMP', '536__F_Right_ring_finger.BMP', '536__F_Right_thumb_finger.BMP', '537__M_Left_index_finger.BMP', '537__M_Left_little_finger.BMP', '537__M_Left_middle_finger.BMP', '537__M_Left_ring_finger.BMP', '537__M_Left_thumb_finger.BMP', '537__M_Right_index_finger.BMP', '537__M_Right_little_finger.BMP', '537__M_Right_middle_finger.BMP', '537__M_Right_ring_finger.BMP', '537__M_Right_thumb_finger.BMP', '538__M_Left_index_finger.BMP', '538__M_Left_little_finger.BMP', '538__M_Left_middle_finger.BMP', '538__M_Left_ring_finger.BMP', '538__M_Left_thumb_finger.BMP', '538__M_Right_index_finger.BMP', '538__M_Right_little_finger.BMP', '538__M_Right_middle_finger.BMP', '538__M_Right_ring_finger.BMP', '538__M_Right_thumb_finger.BMP', '539__F_Left_index_finger.BMP', '539__F_Left_little_finger.BMP', '539__F_Left_middle_finger.BMP', '539__F_Left_ring_finger.BMP', '539__F_Left_thumb_finger.BMP', '539__F_Right_index_finger.BMP', '539__F_Right_little_finger.BMP', '539__F_Right_middle_finger.BMP', '539__F_Right_ring_finger.BMP', '539__F_Right_thumb_finger.BMP', '53__M_Left_index_finger.BMP', '53__M_Left_little_finger.BMP', '53__M_Left_middle_finger.BMP', '53__M_Left_ring_finger.BMP', '53__M_Left_thumb_finger.BMP', '53__M_Right_index_finger.BMP', '53__M_Right_little_finger.BMP', '53__M_Right_middle_finger.BMP', '53__M_Right_ring_finger.BMP', '53__M_Right_thumb_finger.BMP', '540__F_Left_index_finger.BMP', '540__F_Left_little_finger.BMP', '540__F_Left_middle_finger.BMP', '540__F_Left_ring_finger.BMP', '540__F_Left_thumb_finger.BMP', '540__F_Right_index_finger.BMP', '540__F_Right_little_finger.BMP', '540__F_Right_middle_finger.BMP', '540__F_Right_ring_finger.BMP', '540__F_Right_thumb_finger.BMP', '541__M_Left_index_finger.BMP', '541__M_Left_little_finger.BMP', '541__M_Left_middle_finger.BMP', '541__M_Left_ring_finger.BMP', '541__M_Left_thumb_finger.BMP', '541__M_Right_index_finger.BMP', '541__M_Right_little_finger.BMP', '541__M_Right_middle_finger.BMP', '541__M_Right_ring_finger.BMP', '541__M_Right_thumb_finger.BMP', '542__M_Left_index_finger.BMP', '542__M_Left_little_finger.BMP', '542__M_Left_middle_finger.BMP', '542__M_Left_ring_finger.BMP', '542__M_Left_thumb_finger.BMP', '542__M_Right_index_finger.BMP', '542__M_Right_little_finger.BMP', '542__M_Right_middle_finger.BMP', '542__M_Right_ring_finger.BMP', '542__M_Right_thumb_finger.BMP', '543__M_Left_index_finger.BMP', '543__M_Left_little_finger.BMP', '543__M_Left_middle_finger.BMP', '543__M_Left_ring_finger.BMP', '543__M_Left_thumb_finger.BMP', '543__M_Right_index_finger.BMP', '543__M_Right_little_finger.BMP', '543__M_Right_middle_finger.BMP', '543__M_Right_ring_finger.BMP', '543__M_Right_thumb_finger.BMP', '544__M_Left_index_finger.BMP', '544__M_Left_little_finger.BMP', '544__M_Left_middle_finger.BMP', '544__M_Left_ring_finger.BMP', '544__M_Left_thumb_finger.BMP', '544__M_Right_index_finger.BMP', '544__M_Right_little_finger.BMP', '544__M_Right_middle_finger.BMP', '544__M_Right_ring_finger.BMP', '544__M_Right_thumb_finger.BMP', '545__M_Left_index_finger.BMP', '545__M_Left_little_finger.BMP', '545__M_Left_middle_finger.BMP', '545__M_Left_ring_finger.BMP', '545__M_Left_thumb_finger.BMP', '545__M_Right_index_finger.BMP', '545__M_Right_little_finger.BMP', '545__M_Right_middle_finger.BMP', '545__M_Right_ring_finger.BMP', '545__M_Right_thumb_finger.BMP', '546__M_Left_index_finger.BMP', '546__M_Left_little_finger.BMP', '546__M_Left_middle_finger.BMP', '546__M_Left_ring_finger.BMP', '546__M_Left_thumb_finger.BMP', '546__M_Right_index_finger.BMP', '546__M_Right_little_finger.BMP', '546__M_Right_middle_finger.BMP', '546__M_Right_ring_finger.BMP', '546__M_Right_thumb_finger.BMP', '547__M_Left_index_finger.BMP', '547__M_Left_little_finger.BMP', '547__M_Left_middle_finger.BMP', '547__M_Left_ring_finger.BMP', '547__M_Left_thumb_finger.BMP', '547__M_Right_index_finger.BMP', '547__M_Right_little_finger.BMP', '547__M_Right_middle_finger.BMP', '547__M_Right_ring_finger.BMP', '547__M_Right_thumb_finger.BMP', '548__F_Left_index_finger.BMP', '548__F_Left_little_finger.BMP', '548__F_Left_middle_finger.BMP', '548__F_Left_ring_finger.BMP', '548__F_Left_thumb_finger.BMP', '548__F_Right_index_finger.BMP', '548__F_Right_little_finger.BMP', '548__F_Right_middle_finger.BMP', '548__F_Right_ring_finger.BMP', '548__F_Right_thumb_finger.BMP', '549__M_Left_index_finger.BMP', '549__M_Left_little_finger.BMP', '549__M_Left_middle_finger.BMP', '549__M_Left_ring_finger.BMP', '549__M_Left_thumb_finger.BMP', '549__M_Right_index_finger.BMP', '549__M_Right_little_finger.BMP', '549__M_Right_middle_finger.BMP', '549__M_Right_ring_finger.BMP', '549__M_Right_thumb_finger.BMP', '54__M_Left_index_finger.BMP', '54__M_Left_little_finger.BMP', '54__M_Left_middle_finger.BMP', '54__M_Left_ring_finger.BMP', '54__M_Left_thumb_finger.BMP', '54__M_Right_index_finger.BMP', '54__M_Right_little_finger.BMP', '54__M_Right_middle_finger.BMP', '54__M_Right_ring_finger.BMP', '54__M_Right_thumb_finger.BMP', '550__F_Left_index_finger.BMP', '550__F_Left_little_finger.BMP', '550__F_Left_middle_finger.BMP', '550__F_Left_ring_finger.BMP', '550__F_Left_thumb_finger.BMP', '550__F_Right_index_finger.BMP', '550__F_Right_little_finger.BMP', '550__F_Right_middle_finger.BMP', '550__F_Right_ring_finger.BMP', '550__F_Right_thumb_finger.BMP', '551__F_Left_index_finger.BMP', '551__F_Left_little_finger.BMP', '551__F_Left_middle_finger.BMP', '551__F_Left_ring_finger.BMP', '551__F_Left_thumb_finger.BMP', '551__F_Right_index_finger.BMP', '551__F_Right_little_finger.BMP', '551__F_Right_middle_finger.BMP', '551__F_Right_ring_finger.BMP', '551__F_Right_thumb_finger.BMP', '552__M_Left_index_finger.BMP', '552__M_Left_little_finger.BMP', '552__M_Left_middle_finger.BMP', '552__M_Left_ring_finger.BMP', '552__M_Left_thumb_finger.BMP', '552__M_Right_index_finger.BMP', '552__M_Right_little_finger.BMP', '552__M_Right_middle_finger.BMP', '552__M_Right_ring_finger.BMP', '552__M_Right_thumb_finger.BMP', '553__M_Left_index_finger.BMP', '553__M_Left_little_finger.BMP', '553__M_Left_middle_finger.BMP', '553__M_Left_ring_finger.BMP', '553__M_Left_thumb_finger.BMP', '553__M_Right_index_finger.BMP', '553__M_Right_little_finger.BMP', '553__M_Right_middle_finger.BMP', '553__M_Right_ring_finger.BMP', '553__M_Right_thumb_finger.BMP', '554__M_Left_index_finger.BMP', '554__M_Left_little_finger.BMP', '554__M_Left_middle_finger.BMP', '554__M_Left_ring_finger.BMP', '554__M_Left_thumb_finger.BMP', '554__M_Right_index_finger.BMP', '554__M_Right_little_finger.BMP', '554__M_Right_middle_finger.BMP', '554__M_Right_ring_finger.BMP', '554__M_Right_thumb_finger.BMP', '555__M_Left_index_finger.BMP', '555__M_Left_little_finger.BMP', '555__M_Left_middle_finger.BMP', '555__M_Left_ring_finger.BMP', '555__M_Left_thumb_finger.BMP', '555__M_Right_index_finger.BMP', '555__M_Right_little_finger.BMP', '555__M_Right_middle_finger.BMP', '555__M_Right_ring_finger.BMP', '555__M_Right_thumb_finger.BMP', '556__F_Left_index_finger.BMP', '556__F_Left_little_finger.BMP', '556__F_Left_middle_finger.BMP', '556__F_Left_ring_finger.BMP', '556__F_Left_thumb_finger.BMP', '556__F_Right_index_finger.BMP', '556__F_Right_little_finger.BMP', '556__F_Right_middle_finger.BMP', '556__F_Right_ring_finger.BMP', '556__F_Right_thumb_finger.BMP', '557__M_Left_index_finger.BMP', '557__M_Left_little_finger.BMP', '557__M_Left_middle_finger.BMP', '557__M_Left_ring_finger.BMP', '557__M_Left_thumb_finger.BMP', '557__M_Right_index_finger.BMP', '557__M_Right_little_finger.BMP', '557__M_Right_middle_finger.BMP', '557__M_Right_ring_finger.BMP', '557__M_Right_thumb_finger.BMP', '558__M_Left_index_finger.BMP', '558__M_Left_little_finger.BMP', '558__M_Left_middle_finger.BMP', '558__M_Left_ring_finger.BMP', '558__M_Left_thumb_finger.BMP', '558__M_Right_index_finger.BMP', '558__M_Right_little_finger.BMP', '558__M_Right_middle_finger.BMP', '558__M_Right_ring_finger.BMP', '558__M_Right_thumb_finger.BMP', '559__M_Left_index_finger.BMP', '559__M_Left_little_finger.BMP', '559__M_Left_middle_finger.BMP', '559__M_Left_ring_finger.BMP', '559__M_Left_thumb_finger.BMP', '559__M_Right_index_finger.BMP', '559__M_Right_little_finger.BMP', '559__M_Right_middle_finger.BMP', '559__M_Right_ring_finger.BMP', '559__M_Right_thumb_finger.BMP', '55__M_Left_index_finger.BMP', '55__M_Left_little_finger.BMP', '55__M_Left_middle_finger.BMP', '55__M_Left_ring_finger.BMP', '55__M_Left_thumb_finger.BMP', '55__M_Right_index_finger.BMP', '55__M_Right_little_finger.BMP', '55__M_Right_middle_finger.BMP', '55__M_Right_ring_finger.BMP', '55__M_Right_thumb_finger.BMP', '560__F_Left_index_finger.BMP', '560__F_Left_little_finger.BMP', '560__F_Left_middle_finger.BMP', '560__F_Left_ring_finger.BMP', '560__F_Left_thumb_finger.BMP', '560__F_Right_index_finger.BMP', '560__F_Right_little_finger.BMP', '560__F_Right_middle_finger.BMP', '560__F_Right_ring_finger.BMP', '560__F_Right_thumb_finger.BMP', '561__M_Left_index_finger.BMP', '561__M_Left_little_finger.BMP', '561__M_Left_middle_finger.BMP', '561__M_Left_ring_finger.BMP', '561__M_Left_thumb_finger.BMP', '561__M_Right_index_finger.BMP', '561__M_Right_little_finger.BMP', '561__M_Right_middle_finger.BMP', '561__M_Right_ring_finger.BMP', '561__M_Right_thumb_finger.BMP', '562__F_Left_index_finger.BMP', '562__F_Left_little_finger.BMP', '562__F_Left_middle_finger.BMP', '562__F_Left_ring_finger.BMP', '562__F_Left_thumb_finger.BMP', '562__F_Right_index_finger.BMP', '562__F_Right_little_finger.BMP', '562__F_Right_middle_finger.BMP', '562__F_Right_ring_finger.BMP', '562__F_Right_thumb_finger.BMP', '563__M_Left_index_finger.BMP', '563__M_Left_little_finger.BMP', '563__M_Left_middle_finger.BMP', '563__M_Left_ring_finger.BMP', '563__M_Left_thumb_finger.BMP', '563__M_Right_index_finger.BMP', '563__M_Right_little_finger.BMP', '563__M_Right_middle_finger.BMP', '563__M_Right_ring_finger.BMP', '563__M_Right_thumb_finger.BMP', '564__M_Left_index_finger.BMP', '564__M_Left_little_finger.BMP', '564__M_Left_middle_finger.BMP', '564__M_Left_ring_finger.BMP', '564__M_Left_thumb_finger.BMP', '564__M_Right_index_finger.BMP', '564__M_Right_little_finger.BMP', '564__M_Right_middle_finger.BMP', '564__M_Right_ring_finger.BMP', '564__M_Right_thumb_finger.BMP', '565__M_Left_index_finger.BMP', '565__M_Left_little_finger.BMP', '565__M_Left_middle_finger.BMP', '565__M_Left_ring_finger.BMP', '565__M_Left_thumb_finger.BMP', '565__M_Right_index_finger.BMP', '565__M_Right_little_finger.BMP', '565__M_Right_middle_finger.BMP', '565__M_Right_ring_finger.BMP', '565__M_Right_thumb_finger.BMP', '566__M_Left_index_finger.BMP', '566__M_Left_little_finger.BMP', '566__M_Left_middle_finger.BMP', '566__M_Left_ring_finger.BMP', '566__M_Left_thumb_finger.BMP', '566__M_Right_index_finger.BMP', '566__M_Right_little_finger.BMP', '566__M_Right_middle_finger.BMP', '566__M_Right_ring_finger.BMP', '566__M_Right_thumb_finger.BMP', '567__M_Left_index_finger.BMP', '567__M_Left_little_finger.BMP', '567__M_Left_middle_finger.BMP', '567__M_Left_ring_finger.BMP', '567__M_Left_thumb_finger.BMP', '567__M_Right_index_finger.BMP', '567__M_Right_little_finger.BMP', '567__M_Right_middle_finger.BMP', '567__M_Right_ring_finger.BMP', '567__M_Right_thumb_finger.BMP', '568__M_Left_index_finger.BMP', '568__M_Left_little_finger.BMP', '568__M_Left_middle_finger.BMP', '568__M_Left_ring_finger.BMP', '568__M_Left_thumb_finger.BMP', '568__M_Right_index_finger.BMP', '568__M_Right_little_finger.BMP', '568__M_Right_middle_finger.BMP', '568__M_Right_ring_finger.BMP', '568__M_Right_thumb_finger.BMP', '569__M_Left_index_finger.BMP', '569__M_Left_little_finger.BMP', '569__M_Left_middle_finger.BMP', '569__M_Left_ring_finger.BMP', '569__M_Left_thumb_finger.BMP', '569__M_Right_index_finger.BMP', '569__M_Right_little_finger.BMP', '569__M_Right_middle_finger.BMP', '569__M_Right_ring_finger.BMP', '569__M_Right_thumb_finger.BMP', '56__F_Left_index_finger.BMP', '56__F_Left_little_finger.BMP', '56__F_Left_middle_finger.BMP', '56__F_Left_ring_finger.BMP', '56__F_Left_thumb_finger.BMP', '56__F_Right_index_finger.BMP', '56__F_Right_little_finger.BMP', '56__F_Right_middle_finger.BMP', '56__F_Right_ring_finger.BMP', '56__F_Right_thumb_finger.BMP', '570__M_Left_index_finger.BMP', '570__M_Left_little_finger.BMP', '570__M_Left_middle_finger.BMP', '570__M_Left_ring_finger.BMP', '570__M_Left_thumb_finger.BMP', '570__M_Right_index_finger.BMP', '570__M_Right_little_finger.BMP', '570__M_Right_middle_finger.BMP', '570__M_Right_ring_finger.BMP', '570__M_Right_thumb_finger.BMP', '571__F_Left_index_finger.BMP', '571__F_Left_little_finger.BMP', '571__F_Left_middle_finger.BMP', '571__F_Left_ring_finger.BMP', '571__F_Left_thumb_finger.BMP', '571__F_Right_index_finger.BMP', '571__F_Right_little_finger.BMP', '571__F_Right_middle_finger.BMP', '571__F_Right_ring_finger.BMP', '571__F_Right_thumb_finger.BMP', '572__F_Left_index_finger.BMP', '572__F_Left_little_finger.BMP', '572__F_Left_middle_finger.BMP', '572__F_Left_ring_finger.BMP', '572__F_Left_thumb_finger.BMP', '572__F_Right_index_finger.BMP', '572__F_Right_little_finger.BMP', '572__F_Right_middle_finger.BMP', '572__F_Right_ring_finger.BMP', '572__F_Right_thumb_finger.BMP', '573__F_Left_index_finger.BMP', '573__F_Left_little_finger.BMP', '573__F_Left_middle_finger.BMP', '573__F_Left_ring_finger.BMP', '573__F_Left_thumb_finger.BMP', '573__F_Right_index_finger.BMP', '573__F_Right_little_finger.BMP', '573__F_Right_middle_finger.BMP', '573__F_Right_ring_finger.BMP', '573__F_Right_thumb_finger.BMP', '574__F_Left_index_finger.BMP', '574__F_Left_little_finger.BMP', '574__F_Left_middle_finger.BMP', '574__F_Left_ring_finger.BMP', '574__F_Left_thumb_finger.BMP', '574__F_Right_index_finger.BMP', '574__F_Right_little_finger.BMP', '574__F_Right_middle_finger.BMP', '574__F_Right_ring_finger.BMP', '574__F_Right_thumb_finger.BMP', '575__M_Left_index_finger.BMP', '575__M_Left_little_finger.BMP', '575__M_Left_middle_finger.BMP', '575__M_Left_ring_finger.BMP', '575__M_Left_thumb_finger.BMP', '575__M_Right_index_finger.BMP', '575__M_Right_little_finger.BMP', '575__M_Right_middle_finger.BMP', '575__M_Right_ring_finger.BMP', '575__M_Right_thumb_finger.BMP', '576__M_Left_index_finger.BMP', '576__M_Left_little_finger.BMP', '576__M_Left_middle_finger.BMP', '576__M_Left_ring_finger.BMP', '576__M_Left_thumb_finger.BMP', '576__M_Right_index_finger.BMP', '576__M_Right_little_finger.BMP', '576__M_Right_middle_finger.BMP', '576__M_Right_ring_finger.BMP', '576__M_Right_thumb_finger.BMP', '577__M_Left_index_finger.BMP', '577__M_Left_little_finger.BMP', '577__M_Left_middle_finger.BMP', '577__M_Left_ring_finger.BMP', '577__M_Left_thumb_finger.BMP', '577__M_Right_index_finger.BMP', '577__M_Right_little_finger.BMP', '577__M_Right_middle_finger.BMP', '577__M_Right_ring_finger.BMP', '577__M_Right_thumb_finger.BMP', '578__M_Left_index_finger.BMP', '578__M_Left_little_finger.BMP', '578__M_Left_middle_finger.BMP', '578__M_Left_ring_finger.BMP', '578__M_Left_thumb_finger.BMP', '578__M_Right_index_finger.BMP', '578__M_Right_little_finger.BMP', '578__M_Right_middle_finger.BMP', '578__M_Right_ring_finger.BMP', '578__M_Right_thumb_finger.BMP', '579__M_Left_index_finger.BMP', '579__M_Left_little_finger.BMP', '579__M_Left_middle_finger.BMP', '579__M_Left_ring_finger.BMP', '579__M_Left_thumb_finger.BMP', '579__M_Right_index_finger.BMP', '579__M_Right_little_finger.BMP', '579__M_Right_middle_finger.BMP', '579__M_Right_ring_finger.BMP', '579__M_Right_thumb_finger.BMP', '57__M_Left_index_finger.BMP', '57__M_Left_little_finger.BMP', '57__M_Left_middle_finger.BMP', '57__M_Left_ring_finger.BMP', '57__M_Left_thumb_finger.BMP', '57__M_Right_index_finger.BMP', '57__M_Right_little_finger.BMP', '57__M_Right_middle_finger.BMP', '57__M_Right_ring_finger.BMP', '57__M_Right_thumb_finger.BMP', '580__M_Left_index_finger.BMP', '580__M_Left_little_finger.BMP', '580__M_Left_middle_finger.BMP', '580__M_Left_ring_finger.BMP', '580__M_Left_thumb_finger.BMP', '580__M_Right_index_finger.BMP', '580__M_Right_little_finger.BMP', '580__M_Right_middle_finger.BMP', '580__M_Right_ring_finger.BMP', '580__M_Right_thumb_finger.BMP', '581__F_Left_index_finger.BMP', '581__F_Left_little_finger.BMP', '581__F_Left_middle_finger.BMP', '581__F_Left_ring_finger.BMP', '581__F_Left_thumb_finger.BMP', '581__F_Right_index_finger.BMP', '581__F_Right_little_finger.BMP', '581__F_Right_middle_finger.BMP', '581__F_Right_ring_finger.BMP', '581__F_Right_thumb_finger.BMP', '582__M_Left_index_finger.BMP', '582__M_Left_little_finger.BMP', '582__M_Left_middle_finger.BMP', '582__M_Left_ring_finger.BMP', '582__M_Left_thumb_finger.BMP', '582__M_Right_index_finger.BMP', '582__M_Right_little_finger.BMP', '582__M_Right_middle_finger.BMP', '582__M_Right_ring_finger.BMP', '582__M_Right_thumb_finger.BMP', '583__M_Left_index_finger.BMP', '583__M_Left_little_finger.BMP', '583__M_Left_middle_finger.BMP', '583__M_Left_ring_finger.BMP', '583__M_Left_thumb_finger.BMP', '583__M_Right_index_finger.BMP', '583__M_Right_little_finger.BMP', '583__M_Right_middle_finger.BMP', '583__M_Right_ring_finger.BMP', '583__M_Right_thumb_finger.BMP', '584__M_Left_index_finger.BMP', '584__M_Left_little_finger.BMP', '584__M_Left_middle_finger.BMP', '584__M_Left_ring_finger.BMP', '584__M_Left_thumb_finger.BMP', '584__M_Right_index_finger.BMP', '584__M_Right_little_finger.BMP', '584__M_Right_middle_finger.BMP', '584__M_Right_ring_finger.BMP', '584__M_Right_thumb_finger.BMP', '585__M_Left_index_finger.BMP', '585__M_Left_little_finger.BMP', '585__M_Left_middle_finger.BMP', '585__M_Left_ring_finger.BMP', '585__M_Left_thumb_finger.BMP', '585__M_Right_index_finger.BMP', '585__M_Right_little_finger.BMP', '585__M_Right_middle_finger.BMP', '585__M_Right_ring_finger.BMP', '585__M_Right_thumb_finger.BMP', '586__M_Left_index_finger.BMP', '586__M_Left_little_finger.BMP', '586__M_Left_middle_finger.BMP', '586__M_Left_ring_finger.BMP', '586__M_Left_thumb_finger.BMP', '586__M_Right_index_finger.BMP', '586__M_Right_little_finger.BMP', '586__M_Right_middle_finger.BMP', '586__M_Right_ring_finger.BMP', '586__M_Right_thumb_finger.BMP', '587__M_Left_index_finger.BMP', '587__M_Left_little_finger.BMP', '587__M_Left_middle_finger.BMP', '587__M_Left_ring_finger.BMP', '587__M_Left_thumb_finger.BMP', '587__M_Right_index_finger.BMP', '587__M_Right_little_finger.BMP', '587__M_Right_middle_finger.BMP', '587__M_Right_ring_finger.BMP', '587__M_Right_thumb_finger.BMP', '588__M_Left_index_finger.BMP', '588__M_Left_little_finger.BMP', '588__M_Left_middle_finger.BMP', '588__M_Left_ring_finger.BMP', '588__M_Left_thumb_finger.BMP', '588__M_Right_index_finger.BMP', '588__M_Right_little_finger.BMP', '588__M_Right_middle_finger.BMP', '588__M_Right_ring_finger.BMP', '588__M_Right_thumb_finger.BMP', '589__M_Left_index_finger.BMP', '589__M_Left_little_finger.BMP', '589__M_Left_middle_finger.BMP', '589__M_Left_ring_finger.BMP', '589__M_Left_thumb_finger.BMP', '589__M_Right_index_finger.BMP', '589__M_Right_little_finger.BMP', '589__M_Right_middle_finger.BMP', '589__M_Right_ring_finger.BMP', '589__M_Right_thumb_finger.BMP', '58__M_Left_index_finger.BMP', '58__M_Left_little_finger.BMP', '58__M_Left_middle_finger.BMP', '58__M_Left_ring_finger.BMP', '58__M_Left_thumb_finger.BMP', '58__M_Right_index_finger.BMP', '58__M_Right_little_finger.BMP', '58__M_Right_middle_finger.BMP', '58__M_Right_ring_finger.BMP', '58__M_Right_thumb_finger.BMP', '590__M_Left_index_finger.BMP', '590__M_Left_little_finger.BMP', '590__M_Left_middle_finger.BMP', '590__M_Left_ring_finger.BMP', '590__M_Left_thumb_finger.BMP', '590__M_Right_index_finger.BMP', '590__M_Right_little_finger.BMP', '590__M_Right_middle_finger.BMP', '590__M_Right_ring_finger.BMP', '590__M_Right_thumb_finger.BMP', '591__M_Left_index_finger.BMP', '591__M_Left_little_finger.BMP', '591__M_Left_middle_finger.BMP', '591__M_Left_ring_finger.BMP', '591__M_Left_thumb_finger.BMP', '591__M_Right_index_finger.BMP', '591__M_Right_little_finger.BMP', '591__M_Right_middle_finger.BMP', '591__M_Right_ring_finger.BMP', '591__M_Right_thumb_finger.BMP', '592__M_Left_index_finger.BMP', '592__M_Left_little_finger.BMP', '592__M_Left_middle_finger.BMP', '592__M_Left_ring_finger.BMP', '592__M_Left_thumb_finger.BMP', '592__M_Right_index_finger.BMP', '592__M_Right_little_finger.BMP', '592__M_Right_middle_finger.BMP', '592__M_Right_ring_finger.BMP', '592__M_Right_thumb_finger.BMP', '593__M_Left_index_finger.BMP', '593__M_Left_little_finger.BMP', '593__M_Left_middle_finger.BMP', '593__M_Left_ring_finger.BMP', '593__M_Left_thumb_finger.BMP', '593__M_Right_index_finger.BMP', '593__M_Right_little_finger.BMP', '593__M_Right_middle_finger.BMP', '593__M_Right_ring_finger.BMP', '593__M_Right_thumb_finger.BMP', '594__M_Left_index_finger.BMP', '594__M_Left_little_finger.BMP', '594__M_Left_middle_finger.BMP', '594__M_Left_ring_finger.BMP', '594__M_Left_thumb_finger.BMP', '594__M_Right_index_finger.BMP', '594__M_Right_little_finger.BMP', '594__M_Right_middle_finger.BMP', '594__M_Right_ring_finger.BMP', '594__M_Right_thumb_finger.BMP', '595__M_Left_index_finger.BMP', '595__M_Left_little_finger.BMP', '595__M_Left_middle_finger.BMP', '595__M_Left_ring_finger.BMP', '595__M_Left_thumb_finger.BMP', '595__M_Right_index_finger.BMP', '595__M_Right_little_finger.BMP', '595__M_Right_middle_finger.BMP', '595__M_Right_ring_finger.BMP', '595__M_Right_thumb_finger.BMP', '596__M_Left_index_finger.BMP', '596__M_Left_little_finger.BMP', '596__M_Left_middle_finger.BMP', '596__M_Left_ring_finger.BMP', '596__M_Left_thumb_finger.BMP', '596__M_Right_index_finger.BMP', '596__M_Right_little_finger.BMP', '596__M_Right_middle_finger.BMP', '596__M_Right_ring_finger.BMP', '596__M_Right_thumb_finger.BMP', '597__M_Left_index_finger.BMP', '597__M_Left_little_finger.BMP', '597__M_Left_middle_finger.BMP', '597__M_Left_ring_finger.BMP', '597__M_Left_thumb_finger.BMP', '597__M_Right_index_finger.BMP', '597__M_Right_little_finger.BMP', '597__M_Right_middle_finger.BMP', '597__M_Right_ring_finger.BMP', '597__M_Right_thumb_finger.BMP', '598__M_Left_index_finger.BMP', '598__M_Left_little_finger.BMP', '598__M_Left_middle_finger.BMP', '598__M_Left_ring_finger.BMP', '598__M_Left_thumb_finger.BMP', '598__M_Right_index_finger.BMP', '598__M_Right_little_finger.BMP', '598__M_Right_middle_finger.BMP', '598__M_Right_ring_finger.BMP', '598__M_Right_thumb_finger.BMP', '599__M_Left_index_finger.BMP', '599__M_Left_little_finger.BMP', '599__M_Left_middle_finger.BMP', '599__M_Left_ring_finger.BMP', '599__M_Left_thumb_finger.BMP', '599__M_Right_index_finger.BMP', '599__M_Right_little_finger.BMP', '599__M_Right_middle_finger.BMP', '599__M_Right_ring_finger.BMP', '599__M_Right_thumb_finger.BMP', '59__F_Left_index_finger.BMP', '59__F_Left_little_finger.BMP', '59__F_Left_middle_finger.BMP', '59__F_Left_ring_finger.BMP', '59__F_Left_thumb_finger.BMP', '59__F_Right_index_finger.BMP', '59__F_Right_little_finger.BMP', '59__F_Right_middle_finger.BMP', '59__F_Right_ring_finger.BMP', '59__F_Right_thumb_finger.BMP', '5__M_Left_index_finger.BMP', '5__M_Left_little_finger.BMP', '5__M_Left_middle_finger.BMP', '5__M_Left_ring_finger.BMP', '5__M_Left_thumb_finger.BMP', '5__M_Right_index_finger.BMP', '5__M_Right_little_finger.BMP', '5__M_Right_middle_finger.BMP', '5__M_Right_ring_finger.BMP', '5__M_Right_thumb_finger.BMP', '600__M_Left_index_finger.BMP', '600__M_Left_little_finger.BMP', '600__M_Left_middle_finger.BMP', '600__M_Left_ring_finger.BMP', '600__M_Left_thumb_finger.BMP', '600__M_Right_index_finger.BMP', '600__M_Right_little_finger.BMP', '600__M_Right_middle_finger.BMP', '600__M_Right_ring_finger.BMP', '600__M_Right_thumb_finger.BMP', '60__M_Left_index_finger.BMP', '60__M_Left_little_finger.BMP', '60__M_Left_middle_finger.BMP', '60__M_Left_ring_finger.BMP', '60__M_Left_thumb_finger.BMP', '60__M_Right_index_finger.BMP', '60__M_Right_little_finger.BMP', '60__M_Right_middle_finger.BMP', '60__M_Right_ring_finger.BMP', '60__M_Right_thumb_finger.BMP', '61__M_Left_index_finger.BMP', '61__M_Left_little_finger.BMP', '61__M_Left_middle_finger.BMP', '61__M_Left_ring_finger.BMP', '61__M_Left_thumb_finger.BMP', '61__M_Right_index_finger.BMP', '61__M_Right_little_finger.BMP', '61__M_Right_middle_finger.BMP', '61__M_Right_ring_finger.BMP', '61__M_Right_thumb_finger.BMP', '62__M_Left_index_finger.BMP', '62__M_Left_little_finger.BMP', '62__M_Left_middle_finger.BMP', '62__M_Left_ring_finger.BMP', '62__M_Left_thumb_finger.BMP', '62__M_Right_index_finger.BMP', '62__M_Right_little_finger.BMP', '62__M_Right_middle_finger.BMP', '62__M_Right_ring_finger.BMP', '62__M_Right_thumb_finger.BMP', '63__M_Left_index_finger.BMP', '63__M_Left_little_finger.BMP', '63__M_Left_middle_finger.BMP', '63__M_Left_ring_finger.BMP', '63__M_Left_thumb_finger.BMP', '63__M_Right_index_finger.BMP', '63__M_Right_little_finger.BMP', '63__M_Right_middle_finger.BMP', '63__M_Right_ring_finger.BMP', '63__M_Right_thumb_finger.BMP', '64__M_Left_index_finger.BMP', '64__M_Left_little_finger.BMP', '64__M_Left_middle_finger.BMP', '64__M_Left_ring_finger.BMP', '64__M_Left_thumb_finger.BMP', '64__M_Right_index_finger.BMP', '64__M_Right_little_finger.BMP', '64__M_Right_middle_finger.BMP', '64__M_Right_ring_finger.BMP', '64__M_Right_thumb_finger.BMP', '65__M_Left_index_finger.BMP', '65__M_Left_little_finger.BMP', '65__M_Left_middle_finger.BMP', '65__M_Left_ring_finger.BMP', '65__M_Left_thumb_finger.BMP', '65__M_Right_index_finger.BMP', '65__M_Right_little_finger.BMP', '65__M_Right_middle_finger.BMP', '65__M_Right_ring_finger.BMP', '65__M_Right_thumb_finger.BMP', '66__F_Left_index_finger.BMP', '66__F_Left_little_finger.BMP', '66__F_Left_middle_finger.BMP', '66__F_Left_ring_finger.BMP', '66__F_Left_thumb_finger.BMP', '66__F_Right_index_finger.BMP', '66__F_Right_little_finger.BMP', '66__F_Right_middle_finger.BMP', '66__F_Right_ring_finger.BMP', '66__F_Right_thumb_finger.BMP', '67__F_Left_index_finger.BMP', '67__F_Left_little_finger.BMP', '67__F_Left_middle_finger.BMP', '67__F_Left_ring_finger.BMP', '67__F_Left_thumb_finger.BMP', '67__F_Right_index_finger.BMP', '67__F_Right_little_finger.BMP', '67__F_Right_middle_finger.BMP', '67__F_Right_ring_finger.BMP', '67__F_Right_thumb_finger.BMP', '68__M_Left_index_finger.BMP', '68__M_Left_little_finger.BMP', '68__M_Left_middle_finger.BMP', '68__M_Left_ring_finger.BMP', '68__M_Left_thumb_finger.BMP', '68__M_Right_index_finger.BMP', '68__M_Right_little_finger.BMP', '68__M_Right_middle_finger.BMP', '68__M_Right_ring_finger.BMP', '68__M_Right_thumb_finger.BMP', '69__M_Left_index_finger.BMP', '69__M_Left_little_finger.BMP', '69__M_Left_middle_finger.BMP', '69__M_Left_ring_finger.BMP', '69__M_Left_thumb_finger.BMP', '69__M_Right_index_finger.BMP', '69__M_Right_little_finger.BMP', '69__M_Right_middle_finger.BMP', '69__M_Right_ring_finger.BMP', '69__M_Right_thumb_finger.BMP', '6__M_Left_index_finger.BMP', '6__M_Left_little_finger.BMP', '6__M_Left_middle_finger.BMP', '6__M_Left_ring_finger.BMP', '6__M_Left_thumb_finger.BMP', '6__M_Right_index_finger.BMP', '6__M_Right_little_finger.BMP', '6__M_Right_middle_finger.BMP', '6__M_Right_ring_finger.BMP', '6__M_Right_thumb_finger.BMP', '70__M_Left_index_finger.BMP', '70__M_Left_little_finger.BMP', '70__M_Left_middle_finger.BMP', '70__M_Left_ring_finger.BMP', '70__M_Left_thumb_finger.BMP', '70__M_Right_index_finger.BMP', '70__M_Right_little_finger.BMP', '70__M_Right_middle_finger.BMP', '70__M_Right_ring_finger.BMP', '70__M_Right_thumb_finger.BMP', '71__M_Left_index_finger.BMP', '71__M_Left_little_finger.BMP', '71__M_Left_middle_finger.BMP', '71__M_Left_ring_finger.BMP', '71__M_Left_thumb_finger.BMP', '71__M_Right_index_finger.BMP', '71__M_Right_little_finger.BMP', '71__M_Right_middle_finger.BMP', '71__M_Right_ring_finger.BMP', '71__M_Right_thumb_finger.BMP', '72__M_Left_index_finger.BMP', '72__M_Left_little_finger.BMP', '72__M_Left_middle_finger.BMP', '72__M_Left_ring_finger.BMP', '72__M_Left_thumb_finger.BMP', '72__M_Right_index_finger.BMP', '72__M_Right_little_finger.BMP', '72__M_Right_middle_finger.BMP', '72__M_Right_ring_finger.BMP', '72__M_Right_thumb_finger.BMP', '73__M_Left_index_finger.BMP', '73__M_Left_little_finger.BMP', '73__M_Left_middle_finger.BMP', '73__M_Left_ring_finger.BMP', '73__M_Left_thumb_finger.BMP', '73__M_Right_index_finger.BMP', '73__M_Right_little_finger.BMP', '73__M_Right_middle_finger.BMP', '73__M_Right_ring_finger.BMP', '73__M_Right_thumb_finger.BMP', '74__F_Left_index_finger.BMP', '74__F_Left_little_finger.BMP', '74__F_Left_middle_finger.BMP', '74__F_Left_ring_finger.BMP', '74__F_Left_thumb_finger.BMP', '74__F_Right_index_finger.BMP', '74__F_Right_little_finger.BMP', '74__F_Right_middle_finger.BMP', '74__F_Right_ring_finger.BMP', '74__F_Right_thumb_finger.BMP', '75__F_Left_index_finger.BMP', '75__F_Left_little_finger.BMP', '75__F_Left_middle_finger.BMP', '75__F_Left_ring_finger.BMP', '75__F_Left_thumb_finger.BMP', '75__F_Right_index_finger.BMP', '75__F_Right_little_finger.BMP', '75__F_Right_middle_finger.BMP', '75__F_Right_ring_finger.BMP', '75__F_Right_thumb_finger.BMP', '76__F_Left_index_finger.BMP', '76__F_Left_little_finger.BMP', '76__F_Left_middle_finger.BMP', '76__F_Left_ring_finger.BMP', '76__F_Left_thumb_finger.BMP', '76__F_Right_index_finger.BMP', '76__F_Right_little_finger.BMP', '76__F_Right_middle_finger.BMP', '76__F_Right_ring_finger.BMP', '76__F_Right_thumb_finger.BMP', '77__M_Left_index_finger.BMP', '77__M_Left_little_finger.BMP', '77__M_Left_middle_finger.BMP', '77__M_Left_ring_finger.BMP', '77__M_Left_thumb_finger.BMP', '77__M_Right_index_finger.BMP', '77__M_Right_little_finger.BMP', '77__M_Right_middle_finger.BMP', '77__M_Right_ring_finger.BMP', '77__M_Right_thumb_finger.BMP', '78__F_Left_index_finger.BMP', '78__F_Left_little_finger.BMP', '78__F_Left_middle_finger.BMP', '78__F_Left_ring_finger.BMP', '78__F_Left_thumb_finger.BMP', '78__F_Right_index_finger.BMP', '78__F_Right_little_finger.BMP', '78__F_Right_middle_finger.BMP', '78__F_Right_ring_finger.BMP', '78__F_Right_thumb_finger.BMP', '79__M_Left_index_finger.BMP', '79__M_Left_little_finger.BMP', '79__M_Left_middle_finger.BMP', '79__M_Left_ring_finger.BMP', '79__M_Left_thumb_finger.BMP', '79__M_Right_index_finger.BMP', '79__M_Right_little_finger.BMP', '79__M_Right_middle_finger.BMP', '79__M_Right_ring_finger.BMP', '79__M_Right_thumb_finger.BMP', '7__M_Left_index_finger.BMP', '7__M_Left_little_finger.BMP', '7__M_Left_middle_finger.BMP', '7__M_Left_ring_finger.BMP', '7__M_Left_thumb_finger.BMP', '7__M_Right_index_finger.BMP', '7__M_Right_little_finger.BMP', '7__M_Right_middle_finger.BMP', '7__M_Right_ring_finger.BMP', '7__M_Right_thumb_finger.BMP', '80__M_Left_index_finger.BMP', '80__M_Left_little_finger.BMP', '80__M_Left_middle_finger.BMP', '80__M_Left_ring_finger.BMP', '80__M_Left_thumb_finger.BMP', '80__M_Right_index_finger.BMP', '80__M_Right_little_finger.BMP', '80__M_Right_middle_finger.BMP', '80__M_Right_ring_finger.BMP', '80__M_Right_thumb_finger.BMP', '81__F_Left_index_finger.BMP', '81__F_Left_little_finger.BMP', '81__F_Left_middle_finger.BMP', '81__F_Left_ring_finger.BMP', '81__F_Left_thumb_finger.BMP', '81__F_Right_index_finger.BMP', '81__F_Right_little_finger.BMP', '81__F_Right_middle_finger.BMP', '81__F_Right_ring_finger.BMP', '81__F_Right_thumb_finger.BMP', '82__M_Left_index_finger.BMP', '82__M_Left_little_finger.BMP', '82__M_Left_middle_finger.BMP', '82__M_Left_ring_finger.BMP', '82__M_Left_thumb_finger.BMP', '82__M_Right_index_finger.BMP', '82__M_Right_little_finger.BMP', '82__M_Right_middle_finger.BMP', '82__M_Right_ring_finger.BMP', '82__M_Right_thumb_finger.BMP', '83__M_Left_index_finger.BMP', '83__M_Left_little_finger.BMP', '83__M_Left_middle_finger.BMP', '83__M_Left_ring_finger.BMP', '83__M_Left_thumb_finger.BMP', '83__M_Right_index_finger.BMP', '83__M_Right_little_finger.BMP', '83__M_Right_middle_finger.BMP', '83__M_Right_ring_finger.BMP', '83__M_Right_thumb_finger.BMP', '84__M_Left_index_finger.BMP', '84__M_Left_little_finger.BMP', '84__M_Left_middle_finger.BMP', '84__M_Left_ring_finger.BMP', '84__M_Left_thumb_finger.BMP', '84__M_Right_index_finger.BMP', '84__M_Right_little_finger.BMP', '84__M_Right_middle_finger.BMP', '84__M_Right_ring_finger.BMP', '84__M_Right_thumb_finger.BMP', '85__M_Left_index_finger.BMP', '85__M_Left_little_finger.BMP', '85__M_Left_middle_finger.BMP', '85__M_Left_ring_finger.BMP', '85__M_Left_thumb_finger.BMP', '85__M_Right_index_finger.BMP', '85__M_Right_little_finger.BMP', '85__M_Right_middle_finger.BMP', '85__M_Right_ring_finger.BMP', '85__M_Right_thumb_finger.BMP', '86__M_Left_index_finger.BMP', '86__M_Left_little_finger.BMP', '86__M_Left_middle_finger.BMP', '86__M_Left_ring_finger.BMP', '86__M_Left_thumb_finger.BMP', '86__M_Right_index_finger.BMP', '86__M_Right_little_finger.BMP', '86__M_Right_middle_finger.BMP', '86__M_Right_ring_finger.BMP', '86__M_Right_thumb_finger.BMP', '87__M_Left_index_finger.BMP', '87__M_Left_little_finger.BMP', '87__M_Left_middle_finger.BMP', '87__M_Left_ring_finger.BMP', '87__M_Left_thumb_finger.BMP', '87__M_Right_index_finger.BMP', '87__M_Right_little_finger.BMP', '87__M_Right_middle_finger.BMP', '87__M_Right_ring_finger.BMP', '87__M_Right_thumb_finger.BMP', '88__F_Left_index_finger.BMP', '88__F_Left_little_finger.BMP', '88__F_Left_middle_finger.BMP', '88__F_Left_ring_finger.BMP', '88__F_Left_thumb_finger.BMP', '88__F_Right_index_finger.BMP', '88__F_Right_little_finger.BMP', '88__F_Right_middle_finger.BMP', '88__F_Right_ring_finger.BMP', '88__F_Right_thumb_finger.BMP', '89__M_Left_index_finger.BMP', '89__M_Left_little_finger.BMP', '89__M_Left_middle_finger.BMP', '89__M_Left_ring_finger.BMP', '89__M_Left_thumb_finger.BMP', '89__M_Right_index_finger.BMP', '89__M_Right_little_finger.BMP', '89__M_Right_middle_finger.BMP', '89__M_Right_ring_finger.BMP', '89__M_Right_thumb_finger.BMP', '8__M_Left_index_finger.BMP', '8__M_Left_little_finger.BMP', '8__M_Left_middle_finger.BMP', '8__M_Left_ring_finger.BMP', '8__M_Left_thumb_finger.BMP', '8__M_Right_index_finger.BMP', '8__M_Right_little_finger.BMP', '8__M_Right_middle_finger.BMP', '8__M_Right_ring_finger.BMP', '8__M_Right_thumb_finger.BMP', '90__M_Left_index_finger.BMP', '90__M_Left_little_finger.BMP', '90__M_Left_middle_finger.BMP', '90__M_Left_ring_finger.BMP', '90__M_Left_thumb_finger.BMP', '90__M_Right_index_finger.BMP', '90__M_Right_little_finger.BMP', '90__M_Right_middle_finger.BMP', '90__M_Right_ring_finger.BMP', '90__M_Right_thumb_finger.BMP', '91__F_Left_index_finger.BMP', '91__F_Left_little_finger.BMP', '91__F_Left_middle_finger.BMP', '91__F_Left_ring_finger.BMP', '91__F_Left_thumb_finger.BMP', '91__F_Right_index_finger.BMP', '91__F_Right_little_finger.BMP', '91__F_Right_middle_finger.BMP', '91__F_Right_ring_finger.BMP', '91__F_Right_thumb_finger.BMP', '92__F_Left_index_finger.BMP', '92__F_Left_little_finger.BMP', '92__F_Left_middle_finger.BMP', '92__F_Left_ring_finger.BMP', '92__F_Left_thumb_finger.BMP', '92__F_Right_index_finger.BMP', '92__F_Right_little_finger.BMP', '92__F_Right_middle_finger.BMP', '92__F_Right_ring_finger.BMP', '92__F_Right_thumb_finger.BMP', '93__M_Left_index_finger.BMP', '93__M_Left_little_finger.BMP', '93__M_Left_middle_finger.BMP', '93__M_Left_ring_finger.BMP', '93__M_Left_thumb_finger.BMP', '93__M_Right_index_finger.BMP', '93__M_Right_little_finger.BMP', '93__M_Right_middle_finger.BMP', '93__M_Right_ring_finger.BMP', '93__M_Right_thumb_finger.BMP', '94__M_Left_index_finger.BMP', '94__M_Left_little_finger.BMP', '94__M_Left_middle_finger.BMP', '94__M_Left_ring_finger.BMP', '94__M_Left_thumb_finger.BMP', '94__M_Right_index_finger.BMP', '94__M_Right_little_finger.BMP', '94__M_Right_middle_finger.BMP', '94__M_Right_ring_finger.BMP', '94__M_Right_thumb_finger.BMP', '95__M_Left_index_finger.BMP', '95__M_Left_little_finger.BMP', '95__M_Left_middle_finger.BMP', '95__M_Left_ring_finger.BMP', '95__M_Left_thumb_finger.BMP', '95__M_Right_index_finger.BMP', '95__M_Right_little_finger.BMP', '95__M_Right_middle_finger.BMP', '95__M_Right_ring_finger.BMP', '95__M_Right_thumb_finger.BMP', '96__M_Left_index_finger.BMP', '96__M_Left_little_finger.BMP', '96__M_Left_middle_finger.BMP', '96__M_Left_ring_finger.BMP', '96__M_Left_thumb_finger.BMP', '96__M_Right_index_finger.BMP', '96__M_Right_little_finger.BMP', '96__M_Right_middle_finger.BMP', '96__M_Right_ring_finger.BMP', '96__M_Right_thumb_finger.BMP', '97__M_Left_index_finger.BMP', '97__M_Left_little_finger.BMP', '97__M_Left_middle_finger.BMP', '97__M_Left_ring_finger.BMP', '97__M_Left_thumb_finger.BMP', '97__M_Right_index_finger.BMP', '97__M_Right_little_finger.BMP', '97__M_Right_middle_finger.BMP', '97__M_Right_ring_finger.BMP', '97__M_Right_thumb_finger.BMP', '98__M_Left_index_finger.BMP', '98__M_Left_little_finger.BMP', '98__M_Left_middle_finger.BMP', '98__M_Left_ring_finger.BMP', '98__M_Left_thumb_finger.BMP', '98__M_Right_index_finger.BMP', '98__M_Right_little_finger.BMP', '98__M_Right_middle_finger.BMP', '98__M_Right_ring_finger.BMP', '98__M_Right_thumb_finger.BMP', '99__M_Left_index_finger.BMP', '99__M_Left_little_finger.BMP', '99__M_Left_middle_finger.BMP', '99__M_Left_ring_finger.BMP', '99__M_Left_thumb_finger.BMP', '99__M_Right_index_finger.BMP', '99__M_Right_little_finger.BMP', '99__M_Right_middle_finger.BMP', '99__M_Right_ring_finger.BMP', '99__M_Right_thumb_finger.BMP', '9__M_Left_index_finger.BMP', '9__M_Left_little_finger.BMP', '9__M_Left_middle_finger.BMP', '9__M_Left_ring_finger.BMP', '9__M_Left_thumb_finger.BMP', '9__M_Right_index_finger.BMP', '9__M_Right_little_finger.BMP', '9__M_Right_middle_finger.BMP', '9__M_Right_ring_finger.BMP', '9__M_Right_thumb_finger.BMP']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path = r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\" Error: Path does not exist -> {data_path}\")\n",
    "else:\n",
    "    print(f\" Path exists: {data_path}\")\n",
    "\n",
    "\n",
    "print(os.listdir(data_path))\n",
    "\n",
    "\n",
    "for folder in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Checking folder: {folder}\")\n",
    "        print(os.listdir(folder_path))  # Print files inside each folder\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0838475-0f80-4972-9326-b76166430b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6000 images with 20 unique labels\n",
      "Dataset shape: (6000, 128, 128), Labels shape: (6000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_images(data_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Path does not exist: {data_path}\")\n",
    "\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.lower().endswith(\".bmp\"):  # Load only BMP images\n",
    "            img_path = os.path.join(data_path, filename)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, (128, 128))  # Resize to a fixed size\n",
    "                images.append(img)\n",
    "                labels.append(filename.split(\"__\")[1])  # Extract label (e.g., \"M_Left_index_finger\")\n",
    "\n",
    "    if len(images) == 0:\n",
    "        raise ValueError(\"No images loaded. Check file paths and folder structure.\")\n",
    "\n",
    "    print(f\" Loaded {len(images)} images with {len(set(labels))} unique labels\")\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Set your dataset path\n",
    "data_path = r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\"\n",
    "\n",
    "# Load dataset\n",
    "images, labels = load_images(data_path)\n",
    "\n",
    "# Normalize images\n",
    "images = images / 255.0\n",
    "\n",
    "print(f\"Dataset shape: {images.shape}, Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b60216cc-de2b-4ce2-88ee-88f451b83654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Encoded Labels Shape: (6000, 20)\n",
      "✅ Final Image Shape: (6000, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Reshape images for CNN (add channel dimension)\n",
    "images = images.reshape((images.shape[0], 128, 128, 1))  # (6000, 128, 128, 1)\n",
    "\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Convert to categorical (one-hot encoding)\n",
    "labels_categorical = to_categorical(encoded_labels)\n",
    "\n",
    "print(f\"✅ Encoded Labels Shape: {labels_categorical.shape}\")\n",
    "print(f\"✅ Final Image Shape: {images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d07a7e89-46ec-4023-a681-c2825be310b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 185ms/step - accuracy: 0.1220 - loss: 3.0640 - val_accuracy: 0.3008 - val_loss: 2.3849\n",
      "Epoch 2/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 173ms/step - accuracy: 0.2535 - loss: 2.3639 - val_accuracy: 0.3467 - val_loss: 2.1243\n",
      "Epoch 3/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 166ms/step - accuracy: 0.3292 - loss: 2.1207 - val_accuracy: 0.4033 - val_loss: 1.9920\n",
      "Epoch 4/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 171ms/step - accuracy: 0.3616 - loss: 1.9955 - val_accuracy: 0.4342 - val_loss: 1.8630\n",
      "Epoch 5/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 170ms/step - accuracy: 0.3983 - loss: 1.8500 - val_accuracy: 0.4267 - val_loss: 1.7963\n",
      "Epoch 6/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 169ms/step - accuracy: 0.4247 - loss: 1.7577 - val_accuracy: 0.4342 - val_loss: 1.7634\n",
      "Epoch 7/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 170ms/step - accuracy: 0.4519 - loss: 1.6515 - val_accuracy: 0.4392 - val_loss: 1.7156\n",
      "Epoch 8/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 168ms/step - accuracy: 0.5010 - loss: 1.4838 - val_accuracy: 0.4525 - val_loss: 1.7256\n",
      "Epoch 9/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 331ms/step - accuracy: 0.5302 - loss: 1.3560 - val_accuracy: 0.4333 - val_loss: 1.7664\n",
      "Epoch 10/10\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 171ms/step - accuracy: 0.5623 - loss: 1.2679 - val_accuracy: 0.4417 - val_loss: 1.7464\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - accuracy: 0.7309 - loss: 0.8499\n",
      "🧪 Test Accuracy: 69.22%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(20, activation='softmax')  # 20 unique labels\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(images, labels_categorical, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(images, labels_categorical)\n",
    "print(f\"🧪 Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c08486fb-709b-4162-beed-a25ed9eef7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 170ms/step - accuracy: 0.1129 - loss: 2.8912 - val_accuracy: 0.2302 - val_loss: 2.4483\n",
      "Epoch 2/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 196ms/step - accuracy: 0.2167 - loss: 2.5223 - val_accuracy: 0.3156 - val_loss: 2.2625\n",
      "Epoch 3/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 173ms/step - accuracy: 0.2803 - loss: 2.3264 - val_accuracy: 0.3385 - val_loss: 2.1405\n",
      "Epoch 4/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.3250 - loss: 2.1612 - val_accuracy: 0.3854 - val_loss: 2.0252\n",
      "Epoch 5/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 172ms/step - accuracy: 0.3329 - loss: 2.1219 - val_accuracy: 0.3990 - val_loss: 1.9024\n",
      "Epoch 6/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 175ms/step - accuracy: 0.3517 - loss: 2.0101 - val_accuracy: 0.4125 - val_loss: 1.8670\n",
      "Epoch 7/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 196ms/step - accuracy: 0.3725 - loss: 1.8965 - val_accuracy: 0.4521 - val_loss: 1.7556\n",
      "Epoch 8/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 174ms/step - accuracy: 0.3914 - loss: 1.8551 - val_accuracy: 0.4479 - val_loss: 1.6927\n",
      "Epoch 9/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 170ms/step - accuracy: 0.4036 - loss: 1.8249 - val_accuracy: 0.4677 - val_loss: 1.6322\n",
      "Epoch 10/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.4136 - loss: 1.7439 - val_accuracy: 0.4625 - val_loss: 1.6802\n",
      "Epoch 11/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 169ms/step - accuracy: 0.4352 - loss: 1.7228 - val_accuracy: 0.4500 - val_loss: 1.6650\n",
      "Epoch 12/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.4554 - loss: 1.6518 - val_accuracy: 0.4823 - val_loss: 1.5926\n",
      "Epoch 13/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.4701 - loss: 1.5913 - val_accuracy: 0.4500 - val_loss: 1.6276\n",
      "Epoch 14/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 169ms/step - accuracy: 0.4811 - loss: 1.5540 - val_accuracy: 0.4688 - val_loss: 1.6040\n",
      "Epoch 15/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.4854 - loss: 1.4936 - val_accuracy: 0.4615 - val_loss: 1.6088\n",
      "Epoch 16/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 172ms/step - accuracy: 0.5001 - loss: 1.4236 - val_accuracy: 0.4750 - val_loss: 1.5543\n",
      "Epoch 17/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 4s/step - accuracy: 0.5075 - loss: 1.4049 - val_accuracy: 0.4854 - val_loss: 1.5620\n",
      "Epoch 18/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 163ms/step - accuracy: 0.5145 - loss: 1.3534 - val_accuracy: 0.4781 - val_loss: 1.5923\n",
      "Epoch 19/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 159ms/step - accuracy: 0.5477 - loss: 1.2712 - val_accuracy: 0.4760 - val_loss: 1.5928\n",
      "Epoch 20/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 173ms/step - accuracy: 0.5902 - loss: 1.1639 - val_accuracy: 0.4688 - val_loss: 1.6638\n",
      "Epoch 21/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 170ms/step - accuracy: 0.5657 - loss: 1.1836 - val_accuracy: 0.4792 - val_loss: 1.7188\n",
      "Epoch 22/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 166ms/step - accuracy: 0.5889 - loss: 1.1209 - val_accuracy: 0.4823 - val_loss: 1.6508\n",
      "Epoch 23/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.6035 - loss: 1.0783 - val_accuracy: 0.4781 - val_loss: 1.7801\n",
      "Epoch 24/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 169ms/step - accuracy: 0.6077 - loss: 1.0410 - val_accuracy: 0.4802 - val_loss: 1.7124\n",
      "Epoch 25/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.6171 - loss: 1.0059 - val_accuracy: 0.4688 - val_loss: 1.8174\n",
      "Epoch 26/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 167ms/step - accuracy: 0.6412 - loss: 0.9422 - val_accuracy: 0.4583 - val_loss: 1.8493\n",
      "Epoch 27/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 167ms/step - accuracy: 0.6489 - loss: 0.9271 - val_accuracy: 0.4646 - val_loss: 1.8652\n",
      "Epoch 28/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 166ms/step - accuracy: 0.6444 - loss: 0.9166 - val_accuracy: 0.4635 - val_loss: 1.8822\n",
      "Epoch 29/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 167ms/step - accuracy: 0.6823 - loss: 0.8242 - val_accuracy: 0.4708 - val_loss: 2.1885\n",
      "Epoch 30/30\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.6775 - loss: 0.8248 - val_accuracy: 0.4781 - val_loss: 1.9108\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.4490 - loss: 2.1116\n",
      "\n",
      "🧪 Test Accuracy: 47.92%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\n🧪 Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c22f8ad-32d7-48cc-a24f-c30844b0de58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (3840, 128, 128, 1) images, (3840, 20) labels\n",
      "Validation set: (960, 128, 128, 1) images, (960, 20) labels\n",
      "Test set: (1200, 128, 128, 1) images, (1200, 20) labels\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 491ms/step - accuracy: 0.1174 - loss: 4.7805 - val_accuracy: 0.0250 - val_loss: 24.8845\n",
      "Epoch 2/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 482ms/step - accuracy: 0.1233 - loss: 2.9273 - val_accuracy: 0.0250 - val_loss: 16.3720\n",
      "Epoch 3/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 520ms/step - accuracy: 0.1309 - loss: 2.8531 - val_accuracy: 0.0542 - val_loss: 6.7034\n",
      "Epoch 4/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 518ms/step - accuracy: 0.1430 - loss: 2.8292 - val_accuracy: 0.1365 - val_loss: 2.8363\n",
      "Epoch 5/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 494ms/step - accuracy: 0.1390 - loss: 2.8144 - val_accuracy: 0.1562 - val_loss: 2.7380\n",
      "Epoch 6/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 496ms/step - accuracy: 0.1452 - loss: 2.7587 - val_accuracy: 0.1771 - val_loss: 2.6969\n",
      "Epoch 7/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 489ms/step - accuracy: 0.1459 - loss: 2.7756 - val_accuracy: 0.1677 - val_loss: 2.6660\n",
      "Epoch 8/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 489ms/step - accuracy: 0.1458 - loss: 2.7448 - val_accuracy: 0.1500 - val_loss: 2.6989\n",
      "Epoch 9/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 506ms/step - accuracy: 0.1647 - loss: 2.7062 - val_accuracy: 0.1135 - val_loss: 3.0543\n",
      "Epoch 10/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 516ms/step - accuracy: 0.1403 - loss: 2.7159 - val_accuracy: 0.1667 - val_loss: 2.6068\n",
      "Epoch 11/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 520ms/step - accuracy: 0.1573 - loss: 2.6648 - val_accuracy: 0.1833 - val_loss: 2.5982\n",
      "Epoch 12/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 508ms/step - accuracy: 0.1535 - loss: 2.6746 - val_accuracy: 0.2094 - val_loss: 2.7504\n",
      "Epoch 13/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 505ms/step - accuracy: 0.1523 - loss: 2.6641 - val_accuracy: 0.1781 - val_loss: 3.3609\n",
      "Epoch 14/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 491ms/step - accuracy: 0.1713 - loss: 2.6051 - val_accuracy: 0.2031 - val_loss: 2.5505\n",
      "Epoch 15/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 490ms/step - accuracy: 0.1708 - loss: 2.6249 - val_accuracy: 0.2031 - val_loss: 2.5031\n",
      "Epoch 16/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 491ms/step - accuracy: 0.1740 - loss: 2.5859 - val_accuracy: 0.2042 - val_loss: 2.5058\n",
      "Epoch 17/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 488ms/step - accuracy: 0.1604 - loss: 2.6120 - val_accuracy: 0.1979 - val_loss: 2.5480\n",
      "Epoch 18/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 493ms/step - accuracy: 0.1795 - loss: 2.5782 - val_accuracy: 0.2344 - val_loss: 2.4952\n",
      "Epoch 19/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 475ms/step - accuracy: 0.1808 - loss: 2.5642 - val_accuracy: 0.1698 - val_loss: 3.6007\n",
      "Epoch 20/20\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 486ms/step - accuracy: 0.1882 - loss: 2.5547 - val_accuracy: 0.2406 - val_loss: 2.4448\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.2336 - loss: 2.4238\n",
      "🧪 Test Accuracy: 23.67%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, RandomFlip, RandomRotation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Path to the dataset\n",
    "data_dir = r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\Real\"\n",
    "\n",
    "# Lists to store images and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through images\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".BMP\"):  # Ensure we only process image files\n",
    "        img_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        # Read the image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "        img = cv2.resize(img, (128, 128))  # Resize to 128x128\n",
    "        \n",
    "        # Extract label (e.g., \"M_Left_index_finger\" from \"100__M_Left_index_finger.BMP\")\n",
    "        label = file.split(\"__\")[1]  # Extract \"M_Left_index_finger\"\n",
    "\n",
    "        # Store data\n",
    "        X.append(img)\n",
    "        y.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X).astype(\"float32\") / 255.0  # Normalize images\n",
    "X = np.expand_dims(X, axis=-1)  # Add channel dimension\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # Convert string labels to numbers\n",
    "y = to_categorical(y)  # Convert to one-hot encoding\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check dataset shape\n",
    "print(f\"Training set: {X_train.shape} images, {y_train.shape} labels\")\n",
    "print(f\"Validation set: {X_val.shape} images, {y_val.shape} labels\")\n",
    "print(f\"Test set: {X_test.shape} images, {y_test.shape} labels\")\n",
    "\n",
    "# Define Enhanced CNN Model\n",
    "model = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.1),\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(20, activation='softmax')  # 20 unique labels\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\U0001F9EA Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b5d7cc5-513f-490c-8752-64678b3413b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New training set size after augmentation: (7530, 128, 128, 1)\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 445ms/step - accuracy: 0.0747 - loss: 3.7492 - val_accuracy: 0.0708 - val_loss: 18.6592\n",
      "Epoch 2/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 484ms/step - accuracy: 0.0748 - loss: 2.8835 - val_accuracy: 0.0396 - val_loss: 3.5733\n",
      "Epoch 3/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 516ms/step - accuracy: 0.0888 - loss: 2.8309 - val_accuracy: 0.0312 - val_loss: 2.8983\n",
      "Epoch 4/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 475ms/step - accuracy: 0.0928 - loss: 2.7710 - val_accuracy: 0.0875 - val_loss: 2.8803\n",
      "Epoch 5/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 445ms/step - accuracy: 0.0942 - loss: 2.7617 - val_accuracy: 0.0792 - val_loss: 2.8028\n",
      "Epoch 6/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 447ms/step - accuracy: 0.1027 - loss: 2.7505 - val_accuracy: 0.1365 - val_loss: 2.7884\n",
      "Epoch 7/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 446ms/step - accuracy: 0.1114 - loss: 2.6932 - val_accuracy: 0.1260 - val_loss: 2.7496\n",
      "Epoch 8/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 446ms/step - accuracy: 0.1132 - loss: 2.6632 - val_accuracy: 0.1365 - val_loss: 2.7891\n",
      "Epoch 9/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 3s/step - accuracy: 0.1267 - loss: 2.6686 - val_accuracy: 0.1052 - val_loss: 2.8479\n",
      "Epoch 10/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 451ms/step - accuracy: 0.1243 - loss: 2.6421 - val_accuracy: 0.1448 - val_loss: 2.6930\n",
      "Epoch 11/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 1s/step - accuracy: 0.1295 - loss: 2.6365 - val_accuracy: 0.1594 - val_loss: 2.7366\n",
      "Epoch 12/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 438ms/step - accuracy: 0.1333 - loss: 2.6070 - val_accuracy: 0.2000 - val_loss: 2.6103\n",
      "Epoch 13/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2261s\u001b[0m 10s/step - accuracy: 0.1362 - loss: 2.5836 - val_accuracy: 0.1865 - val_loss: 2.6149\n",
      "Epoch 14/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 444ms/step - accuracy: 0.1436 - loss: 2.5532 - val_accuracy: 0.1937 - val_loss: 2.5942\n",
      "Epoch 15/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 833ms/step - accuracy: 0.1504 - loss: 2.5401 - val_accuracy: 0.1500 - val_loss: 5.6726\n",
      "Epoch 16/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 447ms/step - accuracy: 0.1552 - loss: 2.5073 - val_accuracy: 0.1979 - val_loss: 2.7457\n",
      "Epoch 17/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2184s\u001b[0m 9s/step - accuracy: 0.1481 - loss: 2.4981 - val_accuracy: 0.1635 - val_loss: 2.6446\n",
      "Epoch 18/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 454ms/step - accuracy: 0.1564 - loss: 2.4985 - val_accuracy: 0.1792 - val_loss: 2.9859\n",
      "Epoch 19/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1210s\u001b[0m 5s/step - accuracy: 0.1598 - loss: 2.4939 - val_accuracy: 0.1781 - val_loss: 2.8016\n",
      "Epoch 20/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 448ms/step - accuracy: 0.1645 - loss: 2.4819 - val_accuracy: 0.2177 - val_loss: 2.6415\n",
      "Epoch 21/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 469ms/step - accuracy: 0.1708 - loss: 2.4907 - val_accuracy: 0.1656 - val_loss: 2.6844\n",
      "Epoch 22/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 450ms/step - accuracy: 0.1773 - loss: 2.4540 - val_accuracy: 0.1740 - val_loss: 2.6939\n",
      "Epoch 23/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 446ms/step - accuracy: 0.1691 - loss: 2.4581 - val_accuracy: 0.1688 - val_loss: 2.5965\n",
      "Epoch 24/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 448ms/step - accuracy: 0.1800 - loss: 2.4297 - val_accuracy: 0.1844 - val_loss: 2.9069\n",
      "Epoch 25/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 450ms/step - accuracy: 0.1906 - loss: 2.4143 - val_accuracy: 0.1823 - val_loss: 2.5855\n",
      "Epoch 26/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 447ms/step - accuracy: 0.1825 - loss: 2.4271 - val_accuracy: 0.1635 - val_loss: 3.3054\n",
      "Epoch 27/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 445ms/step - accuracy: 0.1870 - loss: 2.3948 - val_accuracy: 0.2271 - val_loss: 2.5074\n",
      "Epoch 28/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 551ms/step - accuracy: 0.1900 - loss: 2.3887 - val_accuracy: 0.2354 - val_loss: 2.4404\n",
      "Epoch 29/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 450ms/step - accuracy: 0.1942 - loss: 2.3859 - val_accuracy: 0.2167 - val_loss: 2.5100\n",
      "Epoch 30/30\n",
      "\u001b[1m236/236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 448ms/step - accuracy: 0.1979 - loss: 2.3936 - val_accuracy: 0.1615 - val_loss: 2.7122\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.1600 - loss: 2.6623\n",
      "🧪 Test Accuracy: 16.25%\n",
      "Training set: (7530, 128, 128, 1) images, (7530, 20) labels\n",
      "Validation set: (960, 128, 128, 1) images, (960, 20) labels\n",
      "Test set: (1200, 128, 128, 1) images, (1200, 20) labels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Path to the dataset\n",
    "data_dir = r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\Real\"\n",
    "\n",
    "# Lists to store images and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Loop through images\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".BMP\"):  # Ensure we only process image files\n",
    "        img_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        # Read the image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "        img = cv2.resize(img, (128, 128))  # Resize to 128x128\n",
    "        \n",
    "        # Extract label (e.g., \"M_Left_index_finger\" from \"100__M_Left_index_finger.BMP\")\n",
    "        label = file.split(\"__\")[1]  # Extract \"M_Left_index_finger\"\n",
    "\n",
    "        # Store data\n",
    "        X.append(img)\n",
    "        y.append(label)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X).astype(\"float32\") / 255.0  # Normalize images\n",
    "X = np.expand_dims(X, axis=-1)  # Add channel dimension\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # Convert string labels to numbers\n",
    "y = to_categorical(y)  # Convert to one-hot encoding\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify minority class samples (Female fingerprints)\n",
    "y_raw = label_encoder.inverse_transform(np.argmax(y, axis=1))  # Convert one-hot back to labels\n",
    "minority_class_indices = [i for i, label in enumerate(y_raw) if label.startswith(\"F_\")]  # Indices of female samples\n",
    "\n",
    "# Extract minority class images\n",
    "X_minority = np.array([X[i] for i in minority_class_indices])\n",
    "y_minority = np.array([y[i] for i in minority_class_indices])\n",
    "\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "# Apply augmentation\n",
    "for i in range(len(X_minority)):\n",
    "    for _ in range(3):  # Create 3 augmented copies per image\n",
    "        img = X_minority[i].reshape((1, 128, 128, 1))\n",
    "        augmented = next(datagen.flow(img, batch_size=1))[0]\n",
    "        augmented_images.append(augmented)\n",
    "        augmented_labels.append(y_minority[i])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "augmented_images = np.array(augmented_images)\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Add augmented data to training set\n",
    "X_train = np.concatenate([X_train, augmented_images])\n",
    "y_train = np.concatenate([y_train, augmented_labels])\n",
    "\n",
    "print(f\"✅ New training set size after augmentation: {X_train.shape}\")\n",
    "\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "# Define Enhanced CNN Model\n",
    "model = Sequential([\n",
    "    Input(shape=(128, 128, 1)),\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(20, activation='softmax')  # 20 unique labels\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "model.compile(optimizer=AdamW(learning_rate=0.0005, weight_decay=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=30)\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"🧪 Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Check dataset shape\n",
    "print(f\"Training set: {X_train.shape} images, {y_train.shape} labels\")\n",
    "print(f\"Validation set: {X_val.shape} images, {y_val.shape} labels\")\n",
    "print(f\"Test set: {X_test.shape} images, {y_test.shape} labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20271430-4a51-46ed-9a16-ef465e137275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'M_Left_index_finger.BMP': 477, 'M_Left_little_finger.BMP': 477, 'M_Left_middle_finger.BMP': 477, 'M_Left_ring_finger.BMP': 477, 'M_Left_thumb_finger.BMP': 477, 'M_Right_index_finger.BMP': 477, 'M_Right_little_finger.BMP': 477, 'M_Right_middle_finger.BMP': 477, 'M_Right_ring_finger.BMP': 477, 'M_Right_thumb_finger.BMP': 477, 'F_Left_index_finger.BMP': 123, 'F_Left_little_finger.BMP': 123, 'F_Left_middle_finger.BMP': 123, 'F_Left_ring_finger.BMP': 123, 'F_Left_thumb_finger.BMP': 123, 'F_Right_index_finger.BMP': 123, 'F_Right_little_finger.BMP': 123, 'F_Right_middle_finger.BMP': 123, 'F_Right_ring_finger.BMP': 123, 'F_Right_thumb_finger.BMP': 123})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Print class distribution before encoding\n",
    "y_raw = [file.split(\"__\")[1] for file in os.listdir(data_dir) if file.endswith(\".BMP\")]\n",
    "print(Counter(y_raw))  # Now it will correctly show label counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e4204b-ba83-4c82-b32d-462bef6dafe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3277417328.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    git init\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2415ff49-33fe-424b-9c6e-c342e9b2fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# U-Net Architecture for Segmentation\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.encoder1 = conv_block(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = conv_block(128, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool1(e1))\n",
    "        b = self.bottleneck(self.pool2(e2))\n",
    "        d2 = self.upconv2(b)\n",
    "        d2 = self.decoder2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = self.decoder1(torch.cat([d1, e1], dim=1))\n",
    "        return torch.sigmoid(self.output_layer(d1))\n",
    "\n",
    "\n",
    "# ResNet-Based Classifier for Feature Extraction and Recognition\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Grayscale\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "# Example Preprocessing Transform (For Grayscale Input)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26af876b-8c30-42c6-9424-c7be7111eda1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2160567290.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install torch torchvision matplotlib scikit-learn opencv-python tqdm\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision matplotlib scikit-learn opencv-python tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc4c670-009b-44aa-9978-9c54c899ec8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.1)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yogen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torchvision-0.21.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.6 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.6 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.6 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "   ---------------------------------------- 0.0/39.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/39.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/39.5 MB 1.3 MB/s eta 0:00:31\n",
      "    --------------------------------------- 0.8/39.5 MB 1.3 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 1.0/39.5 MB 1.3 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 1.3/39.5 MB 1.3 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 1.6/39.5 MB 1.3 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 1.8/39.5 MB 1.3 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 2.1/39.5 MB 1.3 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 2.4/39.5 MB 1.3 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 2.6/39.5 MB 1.3 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 2.9/39.5 MB 1.3 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 3.1/39.5 MB 1.3 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 3.4/39.5 MB 1.3 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 3.7/39.5 MB 1.3 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 3.9/39.5 MB 1.3 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 4.2/39.5 MB 1.3 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 4.5/39.5 MB 1.3 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 4.7/39.5 MB 1.3 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 5.0/39.5 MB 1.3 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 5.2/39.5 MB 1.3 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 5.5/39.5 MB 1.3 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 5.8/39.5 MB 1.3 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 6.0/39.5 MB 1.3 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 6.3/39.5 MB 1.3 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 6.6/39.5 MB 1.3 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 6.8/39.5 MB 1.3 MB/s eta 0:00:26\n",
      "   ------- -------------------------------- 7.1/39.5 MB 1.3 MB/s eta 0:00:26\n",
      "   ------- -------------------------------- 7.3/39.5 MB 1.3 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 7.6/39.5 MB 1.3 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 7.9/39.5 MB 1.3 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 8.1/39.5 MB 1.3 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 8.4/39.5 MB 1.3 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 8.7/39.5 MB 1.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 8.9/39.5 MB 1.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 9.2/39.5 MB 1.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 9.4/39.5 MB 1.3 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 9.7/39.5 MB 1.3 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 10.0/39.5 MB 1.3 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 10.2/39.5 MB 1.3 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 10.5/39.5 MB 1.3 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 10.7/39.5 MB 1.3 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 11.0/39.5 MB 1.3 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 11.3/39.5 MB 1.3 MB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 11.5/39.5 MB 1.3 MB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 11.8/39.5 MB 1.3 MB/s eta 0:00:22\n",
      "   ------------ --------------------------- 12.1/39.5 MB 1.3 MB/s eta 0:00:22\n",
      "   ------------ --------------------------- 12.3/39.5 MB 1.3 MB/s eta 0:00:22\n",
      "   ------------ --------------------------- 12.6/39.5 MB 1.3 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 12.8/39.5 MB 1.3 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 13.1/39.5 MB 1.3 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 13.4/39.5 MB 1.3 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 13.6/39.5 MB 1.3 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 13.9/39.5 MB 1.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 14.2/39.5 MB 1.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 14.4/39.5 MB 1.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 14.7/39.5 MB 1.3 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 14.9/39.5 MB 1.3 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 15.2/39.5 MB 1.3 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 15.5/39.5 MB 1.3 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 15.7/39.5 MB 1.3 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 16.0/39.5 MB 1.3 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 16.3/39.5 MB 1.3 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 16.5/39.5 MB 1.3 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 16.8/39.5 MB 1.3 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 17.0/39.5 MB 1.3 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 17.3/39.5 MB 1.3 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 17.6/39.5 MB 1.3 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 17.8/39.5 MB 1.3 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 18.1/39.5 MB 1.3 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 18.4/39.5 MB 1.3 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 18.6/39.5 MB 1.3 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 18.9/39.5 MB 1.3 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 19.1/39.5 MB 1.3 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 19.4/39.5 MB 1.3 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 19.4/39.5 MB 1.3 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 19.7/39.5 MB 1.3 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 19.9/39.5 MB 1.3 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 19.9/39.5 MB 1.3 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 20.2/39.5 MB 1.3 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 20.2/39.5 MB 1.3 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 20.4/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 20.4/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 20.7/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 20.7/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 21.0/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 21.0/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 21.2/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 21.2/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 21.5/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 21.5/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 21.8/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 21.8/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 22.0/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 22.0/39.5 MB 1.2 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 22.3/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 22.3/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 22.5/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 22.5/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 22.8/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 22.8/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 23.1/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 23.1/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 23.1/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 23.3/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 23.3/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 23.3/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 23.6/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 23.6/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 23.9/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 23.9/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 23.9/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 24.1/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 24.1/39.5 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 24.4/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 24.4/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 24.4/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 24.6/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 24.6/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 24.6/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 24.9/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 24.9/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 24.9/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.2/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.2/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.2/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.2/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.2/39.5 MB 1.0 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   ------------------------- -------------- 25.4/39.5 MB 965.6 kB/s eta 0:00:15\n",
      "   -------------------------- ------------- 25.7/39.5 MB 857.9 kB/s eta 0:00:17\n",
      "   -------------------------- ------------- 25.7/39.5 MB 857.9 kB/s eta 0:00:17\n",
      "   -------------------------- ------------- 25.7/39.5 MB 857.9 kB/s eta 0:00:17\n",
      "   -------------------------- ------------- 25.7/39.5 MB 857.9 kB/s eta 0:00:17\n",
      "   -------------------------- ------------- 25.7/39.5 MB 857.9 kB/s eta 0:00:17\n",
      "   -------------------------- ------------- 26.0/39.5 MB 825.2 kB/s eta 0:00:17\n",
      "   -------------------------- ------------- 26.2/39.5 MB 823.9 kB/s eta 0:00:17\n",
      "   -------------------------- ------------- 26.5/39.5 MB 823.9 kB/s eta 0:00:16\n",
      "   --------------------------- ------------ 26.7/39.5 MB 823.6 kB/s eta 0:00:16\n",
      "   --------------------------- ------------ 27.0/39.5 MB 825.3 kB/s eta 0:00:16\n",
      "   --------------------------- ------------ 27.3/39.5 MB 824.8 kB/s eta 0:00:15\n",
      "   --------------------------- ------------ 27.5/39.5 MB 824.0 kB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 27.8/39.5 MB 825.3 kB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 28.0/39.5 MB 825.7 kB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 28.3/39.5 MB 824.8 kB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 28.6/39.5 MB 824.8 kB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 28.8/39.5 MB 823.1 kB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 29.1/39.5 MB 825.3 kB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 29.4/39.5 MB 825.3 kB/s eta 0:00:13\n",
      "   ------------------------------ --------- 29.6/39.5 MB 825.3 kB/s eta 0:00:12\n",
      "   ------------------------------ --------- 29.9/39.5 MB 825.3 kB/s eta 0:00:12\n",
      "   ------------------------------ --------- 30.1/39.5 MB 824.8 kB/s eta 0:00:12\n",
      "   ------------------------------ --------- 30.4/39.5 MB 824.8 kB/s eta 0:00:12\n",
      "   ------------------------------- -------- 30.7/39.5 MB 824.4 kB/s eta 0:00:11\n",
      "   ------------------------------- -------- 30.9/39.5 MB 824.4 kB/s eta 0:00:11\n",
      "   ------------------------------- -------- 31.2/39.5 MB 824.4 kB/s eta 0:00:11\n",
      "   ------------------------------- -------- 31.5/39.5 MB 824.4 kB/s eta 0:00:10\n",
      "   -------------------------------- ------- 31.7/39.5 MB 824.4 kB/s eta 0:00:10\n",
      "   -------------------------------- ------- 32.0/39.5 MB 824.4 kB/s eta 0:00:10\n",
      "   -------------------------------- ------- 32.2/39.5 MB 824.4 kB/s eta 0:00:09\n",
      "   -------------------------------- ------- 32.5/39.5 MB 824.4 kB/s eta 0:00:09\n",
      "   -------------------------------- ------- 32.5/39.5 MB 824.4 kB/s eta 0:00:09\n",
      "   -------------------------------- ------- 32.5/39.5 MB 824.4 kB/s eta 0:00:09\n",
      "   --------------------------------- ------ 32.8/39.5 MB 807.7 kB/s eta 0:00:09\n",
      "   --------------------------------- ------ 33.0/39.5 MB 808.5 kB/s eta 0:00:08\n",
      "   --------------------------------- ------ 33.3/39.5 MB 808.5 kB/s eta 0:00:08\n",
      "   --------------------------------- ------ 33.6/39.5 MB 808.5 kB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 33.8/39.5 MB 808.5 kB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 34.3/39.5 MB 808.5 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 34.6/39.5 MB 808.5 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 34.6/39.5 MB 808.5 kB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 34.9/39.5 MB 808.1 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 35.1/39.5 MB 807.3 kB/s eta 0:00:06\n",
      "   ------------------------------------ --- 35.7/39.5 MB 809.0 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 35.9/39.5 MB 812.6 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 36.2/39.5 MB 808.5 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 36.2/39.5 MB 808.5 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 36.4/39.5 MB 806.4 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 36.7/39.5 MB 800.2 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 37.0/39.5 MB 805.6 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 37.0/39.5 MB 805.6 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 37.2/39.5 MB 798.9 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 37.5/39.5 MB 796.0 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 37.5/39.5 MB 796.0 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 37.7/39.5 MB 787.7 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 37.7/39.5 MB 787.7 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 38.0/39.5 MB 781.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 38.3/39.5 MB 781.4 kB/s eta 0:00:02\n",
      "   ---------------------------------------  38.5/39.5 MB 780.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------  38.8/39.5 MB 780.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  39.1/39.5 MB 780.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  39.3/39.5 MB 780.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.5/39.5 MB 776.9 kB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python, torchvision\n",
      "Successfully installed opencv-python-4.11.0.86 torchvision-0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib scikit-learn opencv-python tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f0e3f6-0b67-46f7-9d6e-69c01a9fc368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in C:\\Users\\yogen\\Downloads\\SOCOFing\\real.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 94\u001b[0m\n\u001b[0;32m     87\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Replace this with your custom fingerprint dataset\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# dataset = YourFingerprintDataset(transform=transform)\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# For demonstration, using dummy CIFAR10 dataset with grayscale\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43myogen\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSOCOFing\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mreal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Initialize Model\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\datasets\\folder.py:43\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     41\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in C:\\Users\\yogen\\Downloads\\SOCOFing\\real."
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# U-Net Architecture for Segmentation\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.encoder1 = conv_block(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = conv_block(128, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "\n",
    "        self.output_layer = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool1(e1))\n",
    "        b = self.bottleneck(self.pool2(e2))\n",
    "        d2 = self.upconv2(b)\n",
    "        d2 = self.decoder2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = self.decoder1(torch.cat([d1, e1], dim=1))\n",
    "        return torch.sigmoid(self.output_layer(d1))\n",
    "\n",
    "\n",
    "# ResNet-Based Classifier for Feature Extraction and Recognition\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Grayscale\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "# Example Preprocessing Transform (For Grayscale Input)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "# Example Usage (Pseudo-code)\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Replace this with your custom fingerprint dataset\n",
    "    # dataset = YourFingerprintDataset(transform=transform)\n",
    "    # dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # For demonstration, using dummy CIFAR10 dataset with grayscale\n",
    "    dataset = ImageFolder(root=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\", transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Initialize Model\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)  # Adjust class count as needed\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, device, num_epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505cede-a77c-4d05-8cc8-09c673c32a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85736cd0-7464-4641-9656-9c3874bc1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Assuming your image files are named with a pattern that includes labels (e.g., \"person1_img1.bmp\", \"person2_img2.bmp\")\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if filename.endswith(\"_finger\"):  # Include BMP files\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                # Extract the label from the filename (this example assumes labels are part of the filename)\n",
    "                label = filename.split(\"_\")[0]  # Adjust this as needed for your dataset\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = list(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        # Debugging: Print paths to verify\n",
    "        print(f\"Found {len(self.image_paths)} images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Convert to RGB if not grayscale\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf93790-75e2-4fe2-96f3-1b7b2d1778e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:53\u001b[1;36m\u001b[0m\n\u001b[1;33m    image = Image.open(img_path).convert(\"RGB\")  # Convert to RGB if not grayscale\u001b[0m\n\u001b[1;37m                                                                                  ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ResNet-Based Classifier for Feature Extraction and Recognition\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Grayscale\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Assuming your image files are named with a pattern that includes labels (e.g., \"person1_img1.bmp\", \"person2_img2.bmp\")\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if filename.endswith(\"_finger\"):  # Include BMP files\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                # Extract the label from the filename (this example assumes labels are part of the filename)\n",
    "                label = filename.split(\"_\")[0]  # Adjust this as needed for your dataset\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = list(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        # Debugging: Print paths to verify\n",
    "        print(f\"Found {len(self.image_paths)} images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Convert to RGB if not grayscale\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "        # Example Preprocessing Transform (For Grayscale Input)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = CustomFingerprintDataset(image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\", transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Initialize Model\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.class_names)).to(device)  # Adjust class count as needed\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train Model\n",
    "    train_model(model, dataloader, criterion, optimizer, device, num_epochs=5)\n",
    "\n",
    "    # Evaluate Model\n",
    "    evaluate_model(model, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9e6010-e8b0-47c6-8caa-68265499cbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 valid fingerprint images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:21<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 1.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [20:22<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Loss: 1.1629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [07:29<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Loss: 0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [05:49<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Loss: 0.9023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [05:48<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Loss: 0.8159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [05:47<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Loss: 0.7396\n",
      "Accuracy: 78.33%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing Transform (ResNet expects 3-channel 224x224 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom Dataset for Unstructured Fingerprint Images\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Collect valid image files with '_finger' in name\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])  # e.g., Right_thumb_finger\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ResNet-Based Classifier\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 3 channels for RGB\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, device, num_epochs=6\n",
    "               )\n",
    "    evaluate_model(model, dataloader, device)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0efa08f-e92f-4c5f-a4e4-03e1d462b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 valid fingerprint images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████| 300/300 [06:34<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████| 300/300 [06:41<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.7407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████| 300/300 [44:35<00:00,  8.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 1.6174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████| 300/300 [05:20<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 1.3892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████| 300/300 [04:39<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 1.1965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████| 300/300 [04:44<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 1.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████| 300/300 [04:40<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 1.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████████████████████████████████████| 300/300 [04:39<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████| 300/300 [04:39<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.9132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████| 300/300 [04:45<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.8810\n",
      "Accuracy: 53.58%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Model\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    full_dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    val_size = int(0.2 * len(full_dataset))\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(full_dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, train_loader, criterion, optimizer, device, num_epochs=5)\n",
    "    evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "597b23b6-6404-413a-bf14-6e8431c7f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [04:38<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9733\n",
      "Val Loss: 1.7466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [05:47<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7383\n",
      "Val Loss: 1.6388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:28<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6603\n",
      "Val Loss: 1.5578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:28<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5421\n",
      "Val Loss: 1.6098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:29<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3923\n",
      "Val Loss: 1.3961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:34<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2303\n",
      "Val Loss: 1.0665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:28<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1220\n",
      "Val Loss: 1.0983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:36<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0725\n",
      "Val Loss: 0.9686\n",
      "Accuracy: 62.75%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Transforms with Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),        # Augmentation\n",
    "    transforms.RandomRotation(10),            # Augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])  # e.g., Right_thumb_finger\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Modified ResNet with Dropout\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function with Validation and Early Stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=8, patience=2):\n",
    "    best_loss = float('inf')\n",
    "    trigger_times = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            trigger_times = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Train/Val split (80/20)\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=8)\n",
    "    \n",
    "    # Load best model and evaluate\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    evaluate_model(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f997f142-4617-45f6-ba4c-c7aa42d8bb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/9: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:44<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.8621 | Val Acc: 31.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/9: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:44<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 1.6561 | Val Acc: 36.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/9: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:46<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 1.4896 | Val Acc: 44.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/9: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:41<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 1.1412 | Val Acc: 59.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/9: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:45<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 1.0310 | Val Acc: 64.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/9: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:45<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 0.9726 | Val Acc: 64.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/9: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [14:45<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 0.8064 | Val Acc: 69.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/9: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [04:59<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 0.7549 | Val Acc: 70.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/9: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [05:03<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 0.7267 | Val Acc: 71.00%\n",
      "Best Validation Accuracy: 71.00%\n",
      "Accuracy: 72.33%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Transform (added augmentation + ImageNet normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         [0.229, 0.224, 0.225])  # ImageNet std\n",
    "])\n",
    "\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=7):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        val_acc = evaluate_model(model, val_loader, device, silent=True)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {running_loss/len(train_loader):.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "def evaluate_model(model, dataloader, device, silent=False):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    if not silent:\n",
    "        print(f\"Accuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = CustomFingerprintDataset(r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\", transform=transform)\n",
    "\n",
    "    # Split: 80% Train, 20% Validation\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)  # LR decay\n",
    "\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=9)\n",
    "    evaluate_model(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c11ea5-a334-40c7-bd2c-62605cc30349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [03:26<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Loss: 1.2846\n",
      "Val Accuracy: 64.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [03:25<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Loss: 0.8588\n",
      "Val Accuracy: 69.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [03:24<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Loss: 0.6941\n",
      "Val Accuracy: 71.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [03:24<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Loss: 0.4979\n",
      "Val Accuracy: 70.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [03:23<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Loss: 0.3867\n",
      "Val Accuracy: 72.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [03:23<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Loss: 0.2856\n",
      "Val Accuracy: 72.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [03:21<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Loss: 0.2201\n",
      "Val Accuracy: 67.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [03:24<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Loss: 0.1860\n",
      "Val Accuracy: 71.58%\n",
      "Final Test Accuracy: 71.58%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5f6ee8-69c5-4f9e-88d2-7b1cb6fa8cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 valid fingerprint images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\yogen/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [03:29<00:00, 490kB/s]\n",
      "Epoch 1/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [30:04<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 1.5736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [50:05<00:00,  8.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Loss: 1.2278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [17:22<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Loss: 1.0660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [16:37<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Loss: 0.9642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [22:02<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Loss: 0.8859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [17:55<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Loss: 0.8192\n",
      "Accuracy: 52.10%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ResNet50-Based Classifier\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, device, num_epochs=6)\n",
    "    evaluate_model(model, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5fc5ba9-68bd-496d-9d14-b8cbe52ae431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 valid fingerprint images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [07:29<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 1.8695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:57<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Loss: 1.6670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [13:48<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Loss: 1.4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [12:01<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Loss: 1.2444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:17<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Loss: 1.1203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [09:28<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Loss: 1.0294\n",
      "Accuracy: 67.08%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ✅ Preprocessing Transform with Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ResNet-Based Classifier\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, device, num_epochs=6)\n",
    "    evaluate_model(model, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e47557e-8954-44f3-85ae-d0146be51bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 valid fingerprint images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:21<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 1.8657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:23<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Loss: 1.6843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:08<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Loss: 1.5815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:07<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Loss: 1.4214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:16<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Loss: 1.1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:15<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Loss: 0.9110\n",
      "Accuracy: 73.30%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing Transform (ResNet expects 3-channel 224x224 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom Dataset for Unstructured Fingerprint Images\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Collect valid image files with '_finger' in name\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])  # e.g., Right_thumb_finger\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ResNet-Based Classifier\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 3 channels for RGB\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, scheduler, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, scheduler, device, num_epochs=6)\n",
    "    evaluate_model(model, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da85e63-4808-4e89-aa85-7fea029393e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 valid fingerprint images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:11<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.9605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [07:34<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 1.8173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [06:21<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 1.7459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [06:22<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 1.5612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [06:16<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 1.4041\n",
      "Accuracy: 60.43%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing Transform (ResNet expects 3-channel 224x224 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom Dataset for Unstructured Fingerprint Images\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Collect valid image files with '_finger' in name\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])  # e.g., Right_thumb_finger\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ResNet-Based Classifier\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 3 channels for RGB\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, scheduler, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, scheduler, device, num_epochs=5)\n",
    "    evaluate_model(model, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6575d6f-d4a4-4811-9868-47167e992abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 valid fingerprint images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/3: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [24:53<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 1.4938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [12:04<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Loss: 1.1458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [05:44<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Loss: 0.9705\n",
      "Accuracy: 58.42%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing Transform (ResNet expects 3-channel 224x224 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom Dataset for Unstructured Fingerprint Images\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Collect valid image files with '_finger' in name\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])  # e.g., Right_thumb_finger\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ResNet-Based Classifier\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 3 channels for RGB\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, device, num_epochs=3)\n",
    "    evaluate_model(model, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8689a9a-d5c0-4ff7-a28c-31100628f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [02:03<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 1.8608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [02:03<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Loss: 1.5646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [02:57<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Loss: 1.4580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [02:09<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Loss: 1.4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [02:08<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Loss: 1.3924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [02:09<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Loss: 1.3669\n",
      "Accuracy: 58.20%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing Transform (ResNet expects 3-channel 224x224 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Custom Dataset for Unstructured Fingerprint Images\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ResNet-Based Classifier (with freezing)\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Freeze all layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze only the final fully connected layer\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, device, num_epochs=6)\n",
    "    evaluate_model(model, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9117dd0e-6e9b-40d7-a3b7-34208dd3b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 valid fingerprint images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [11:10<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 1.4520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [09:20<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6], Loss: 1.0893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [09:28<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Loss: 0.9819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:12<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Loss: 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:11<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Loss: 0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|█████████████████████████████████████████████████████████████████████| 375/375 [08:12<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Loss: 0.6946\n",
      "Accuracy: 78.10%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing Transform (ResNet expects 3-channel 224x224 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3), \n",
    "    transforms.ToTensor(),\n",
    "   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Custom Dataset for Unstructured Fingerprint Images\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Collect valid image files with '_finger' in name\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])  # e.g., Right_thumb_finger\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ResNet-Based Classifier\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 3 channels for RGB\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, device, num_epochs=6\n",
    "               )\n",
    "    evaluate_model(model, dataloader, device)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048a1332-28c4-4644-9698-cc2fad0e148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 6000 valid fingerprint images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yogen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|█████████████████████████████████████████████████████████████████████| 300/300 [06:37<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 1.8302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6:  48%|█████████████████████████████████                                    | 144/300 [03:17<03:33,  1.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 173\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating on validation set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 102\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m     99\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 102\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    104\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 91\u001b[0m, in \u001b[0;36mResNetFingerprintClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m--> 276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2  # Added for fingerprint enhancement\n",
    "\n",
    "# Preprocessing Transform (ResNet expects 3-channel 224x224 images)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Fingerprint enhancement function\n",
    "def enhance_fingerprint(img):\n",
    "    # Convert to grayscale if not already\n",
    "    if img.mode != 'L':\n",
    "        img = img.convert('L')\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(img_array.astype(np.uint8))\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)\n",
    "    \n",
    "    # Apply threshold to make ridge patterns more prominent\n",
    "    _, threshold = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Convert back to PIL Image and to RGB for ResNet\n",
    "    enhanced_img = Image.fromarray(threshold).convert('RGB')\n",
    "    return enhanced_img\n",
    "\n",
    "# Custom Dataset for Unstructured Fingerprint Images\n",
    "class CustomFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None, apply_enhancement=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.apply_enhancement = apply_enhancement\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Collect valid image files with '_finger' in name\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])  # e.g., Right_thumb_finger\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        # Apply fingerprint enhancement\n",
    "        if self.apply_enhancement:\n",
    "            image = enhance_fingerprint(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ResNet-Based Classifier\n",
    "class ResNetFingerprintClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetFingerprintClassifier, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 3 channels for RGB\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Split dataset into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "def get_train_val_loaders(dataset, batch_size=16, val_split=0.2):\n",
    "    # Create indices for the split\n",
    "    indices = list(range(len(dataset)))\n",
    "    # Get class labels for stratification\n",
    "    labels = [dataset.labels[i] for i in indices]\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(\n",
    "        indices, test_size=val_split, random_state=42, stratify=labels)\n",
    "    \n",
    "    # Create samplers\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create dataset with enhancement\n",
    "    dataset = CustomFingerprintDataset(\n",
    "        image_dir=r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\",\n",
    "        transform=transform,\n",
    "        apply_enhancement=True  # Enable fingerprint enhancement\n",
    "    )\n",
    "    \n",
    "    # Create train and validation loaders\n",
    "    train_loader, val_loader = get_train_val_loaders(dataset, batch_size=16)\n",
    "\n",
    "    # Initialize model\n",
    "    model = ResNetFingerprintClassifier(num_classes=len(dataset.classes)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, criterion, optimizer, device, num_epochs=6)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"Evaluating on validation set...\")\n",
    "    val_accuracy = evaluate_model(model, val_loader, device)\n",
    "    \n",
    "    # Final evaluation on full dataset\n",
    "    full_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    print(\"Evaluating on full dataset...\")\n",
    "    full_accuracy = evaluate_model(model, full_loader, device)\n",
    "    \n",
    "    print(f\"Final Results:\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    print(f\"Full Dataset Accuracy: {full_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"fingerprint_enhanced_model.pth\")\n",
    "    print(\"Model saved as 'fingerprint_enhanced_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687d960-1ed5-4d41-b4ce-e7ca5d1cbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Advanced fingerprint enhancement function\n",
    "def enhance_fingerprint(img):\n",
    "    # Convert to grayscale if not already\n",
    "    if img.mode != 'L':\n",
    "        img = img.convert('L')\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(img_array.astype(np.uint8))\n",
    "    \n",
    "    # Apply image normalization\n",
    "    normalized = cv2.normalize(enhanced, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Apply bilateral filter to preserve edges while reducing noise\n",
    "    filtered = cv2.bilateralFilter(normalized, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    \n",
    "    # Apply adaptive threshold to improve ridge detection\n",
    "    binary = cv2.adaptiveThreshold(filtered, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    \n",
    "    # Morphological operations to enhance ridges\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    \n",
    "    # Convert back to PIL Image and to RGB for ResNet\n",
    "    enhanced_img = Image.fromarray(opening).convert('RGB')\n",
    "    return enhanced_img\n",
    "\n",
    "# Strong data augmentation for fingerprints\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((250, 250)),  # Slightly larger than needed\n",
    "    transforms.RandomCrop(224),     # Random crop to final size\n",
    "    transforms.RandomRotation(degrees=(-5, 5)),  # Slight rotation\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Validation transform - just resize and normalize\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom Dataset with advanced enhancement\n",
    "class EnhancedFingerprintDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None, apply_enhancement=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.apply_enhancement = apply_enhancement\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Collect valid image files with '_finger' in name\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if \"_finger\" in filename.lower() and os.path.splitext(filename)[1].lower() in ['.bmp', '.jpg', '.jpeg', '.png']:\n",
    "                self.image_paths.append(os.path.join(image_dir, filename))\n",
    "                label = \"_\".join(filename.split(\"_\")[-3:])  # e.g., Right_thumb_finger\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} valid fingerprint images.\")\n",
    "        print(f\"Number of unique classes: {len(self.classes)}\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"Warning: No valid image files found in the directory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        # Apply fingerprint enhancement\n",
    "        if self.apply_enhancement:\n",
    "            image = enhance_fingerprint(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Enhanced ResNet model with deeper classification head\n",
    "class EnhancedResNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super(EnhancedResNetClassifier, self).__init__()\n",
    "        # Use pretrained ResNet18 as base\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Extract all layers except the final fully connected\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # Add dropout and a more complex classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate/2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for better convergence\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Focal Loss for handling hard examples\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce_loss(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Function to create weighted sampler for imbalanced classes\n",
    "def create_weighted_sampler(dataset):\n",
    "    # Count samples per class\n",
    "    class_counts = {}\n",
    "    for _, label_idx in dataset:\n",
    "        if label_idx not in class_counts:\n",
    "            class_counts[label_idx] = 0\n",
    "        class_counts[label_idx] += 1\n",
    "    \n",
    "    # Compute weights\n",
    "    weights = []\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label_idx = dataset[idx]\n",
    "        weights.append(1.0 / class_counts[label_idx])\n",
    "    \n",
    "    # Create and return sampler\n",
    "    return WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "# Advanced training function with validation and early stopping\n",
    "def train_with_validation(model, train_loader, val_loader, criterion, optimizer, \n",
    "                        scheduler, device, num_epochs, patience=5):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # For early stopping\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss and accuracy for this epoch\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        val_acc = evaluate_model(model, val_loader, device)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"✓ New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"! Validation accuracy didn't improve. Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.savefig('training_curves.png')\n",
    "    \n",
    "    return best_val_acc\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Test-time augmentation for improved inference\n",
    "def tta_predict(model, image, device, num_augmentations=5):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Base prediction with just normalization\n",
    "    base_transform = val_transform\n",
    "    img_tensor = base_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        predictions.append(output)\n",
    "    \n",
    "    # Augmented predictions\n",
    "    aug_transforms = [\n",
    "        transforms.Compose([\n",
    "            transforms.RandomRotation(degrees=(-5, 5)),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]) for _ in range(num_augmentations)\n",
    "    ]\n",
    "    \n",
    "    for transform_fn in aug_transforms:\n",
    "        img_tensor = transform_fn(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "            predictions.append(output)\n",
    "    \n",
    "    # Average predictions\n",
    "    final_output = torch.mean(torch.stack(predictions), dim=0)\n",
    "    _, predicted = torch.max(final_output, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create dataset with separate train and validation transforms\n",
    "    dataset_path = r\"C:\\Users\\yogen\\Downloads\\SOCOFing\\real\"\n",
    "    \n",
    "    # Create train and validation datasets\n",
    "    train_dataset = EnhancedFingerprintDataset(\n",
    "        image_dir=dataset_path,\n",
    "        transform=train_transform,\n",
    "        apply_enhancement=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = EnhancedFingerprintDataset(\n",
    "        image_dir=dataset_path,\n",
    "        transform=val_transform,\n",
    "        apply_enhancement=True\n",
    "    )\n",
    "    \n",
    "    # Split indices for train and validation\n",
    "    indices = list(range(len(train_dataset)))\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        indices, test_size=0.2, random_state=42, stratify=[train_dataset.labels[i] for i in indices]\n",
    "    )\n",
    "    \n",
    "    # Create samplers\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    # Create weighted sampler for training to handle imbalance\n",
    "    # weighted_sampler = create_weighted_sampler(train_dataset)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=32,  # Larger batch size\n",
    "        sampler=train_sampler,\n",
    "        num_workers=4,  # Parallel data loading\n",
    "        pin_memory=True  # Faster data transfer to GPU\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=32,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create enhanced model\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    model = EnhancedResNetClassifier(num_classes=num_classes, dropout_rate=0.3).to(device)\n",
    "    \n",
    "    # Use Focal Loss for handling hard examples\n",
    "    criterion = FocalLoss(gamma=2.0)\n",
    "    \n",
    "    # Optimizer with weight decay for regularization\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "    \n",
    "    # Cosine annealing scheduler with warm restarts\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=5,  # Restart every 5 epochs\n",
    "        T_mult=1,  # Don't increase restart period\n",
    "        eta_min=1e-6  # Minimum learning rate\n",
    "    )\n",
    "    \n",
    "    # Train with validation and early stopping\n",
    "    print(\"Starting training with validation...\")\n",
    "    best_val_acc = train_with_validation(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, \n",
    "        device, num_epochs=30, patience=7\n",
    "    )\n",
    "    \n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    # Save the final model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_accuracy': best_val_acc,\n",
    "        'classes': train_dataset.classes,\n",
    "        'class_to_idx': train_dataset.class_to_idx\n",
    "    }, \"enhanced_fingerprint_model.pth\")\n",
    "    \n",
    "    print(\"Model saved as 'enhanced_fingerprint_model.pth'\")\n",
    "    \n",
    "    # Final evaluation with test-time augmentation\n",
    "    print(\"\\nFinal evaluation with test-time augmentation:\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Create a separate test dataset\n",
    "    test_dataset = EnhancedFingerprintDataset(\n",
    "        image_dir=dataset_path,\n",
    "        transform=None,  # No transform since we'll use TTA\n",
    "        apply_enhancement=True\n",
    "    )\n",
    "    \n",
    "    # Use validation indices for final evaluation\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,  # Process one at a time for TTA\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    )\n",
    "    \n",
    "    for image, label in tqdm(test_loader, desc=\"TTA Evaluation\"):\n",
    "        prediction = tta_predict(model, image[0], device, num_augmentations=5)\n",
    "        if prediction == label.item():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    final_accuracy = 100 * correct / total\n",
    "    print(f\"Final Test Accuracy with TTA: {final_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de254da7-05ed-434e-be13-a01e8bae0adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
